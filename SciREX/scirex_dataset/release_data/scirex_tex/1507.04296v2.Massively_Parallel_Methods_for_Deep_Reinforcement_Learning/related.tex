%!TEX root = gorila_paper.tex
\section{Related Work}

There have been several previous approaches to parallel or distributed RL. A significant part of this work has focused on distributed multi-agent systems \cite{weiss:distributedRL,lauer:distributedRL}. In this approach, there are many agents taking actions within a single shared environment, working cooperatively to achieve a common objective. While computation is distributed in the sense of decentralized control, these algorithms focus on effective teamwork and emergent group behaviors. Another paradigm which has been explored is concurrent reinforcement learning \cite{silver:concurrentRL}, in which an agent can interact in parallel with an inherently distributed environment, e.g. to optimize interactions with multiple users on the internet. Our goal is quite different to both these distributed and concurrent RL paradigms: we simply seek to solve a single-agent problem more efficiently by exploiting parallel computation. 

The MapReduce framework has been applied to standard MDP solution methods such as policy evaluation, policy iteration and value iteration, by distributing the computation involved in large matrix multiplications \cite{li:mapreduce}. However, this work is narrowly focused on batch methods for linear function approximation, and is not immediately applicable to non-linear representations using online reinforcement learning in environments with unknown dynamics.

Perhaps the closest prior work to our own is a parallelization of the canonical \emph{Sarsa} algorithm over multiple machines. Each machine has its own instance of the agent and environment \cite{grounds:parallelRL}, running a simple reinforcement learning algorithm (linear Sarsa, in this case). The changes to the parameters of the linear function approximator are periodically communicated using a peer-to-peer mechanism, focusing especially on those parameters that have changed most. In contrast, our architecture allows for client-server communication and a separation between acting, learning and parameter updates; furthermore we exploit much richer function approximators using a distributed framework for deep learning.
