%!TEX root = gorila_paper.tex
\section{Introduction}
\label{submission}

Deep learning methods have recently achieved state-of-the-art results in vision and speech domains~\cite{krizhevsky-imagenet,simonyan-deep,szegedy-deep,graves2013speech,dahl2012context}, mainly due to their ability to automatically learn high-level features from a supervised signal. Recent advances in reinforcement learning (RL) have successfully combined deep learning with value function approximation, by using a deep convolutional neural network to represent the action-value (Q) function~\cite{mnih2013atari}. Specifically, a new method for training such deep Q-networks, known as DQN, has enabled RL to learn control policies in complex environments with high dimensional images as inputs \cite{mnih-dqn-2015}. This method outperformed a human professional in many games on the Atari 2600 platform, using the same network architecture and hyper-parameters. However, DQN has only previously been applied to single-machine architectures, in practice leading to long training times. For example, it took 12-14 days on a GPU to train the DQN algorithm on a single Atari game \cite{mnih-dqn-2015}. In this work, our goal is to build a distributed architecture that enables us to scale up deep reinforcement learning algorithms such as DQN by exploiting massive computational resources.

One of the main advantages of deep learning is that computation can be easily parallelized. In order to exploit this scalability, deep learning algorithms have made extensive use of hardware advances such as GPUs. However, recent approaches have focused on massively distributed architectures that can learn from more data in parallel and therefore outperform training on a single machine~\cite{coates2013deep,dean2012distbelief}. For example, the DistBelief framework \cite{dean2012distbelief} distributes the neural network parameters across many machines, and parallelizes the training by using  asynchronous stochastic gradient descent (ASGD). DistBelief has been used to achieve state-of-the-art results in several domains \cite{szegedy-deep} and has been shown to be much faster than single GPU training~\cite{dean2012distbelief}.

Existing work on distributed deep learning has focused exclusively on supervised and unsupervised learning. In this paper we develop a new architecture for the reinforcement learning paradigm. This architecture consists of four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed experience replay memory. 

A unique property of RL is that an agent influences the training data distribution by interacting with its environment. In order to generate more data, we deploy multiple agents running in parallel that interact with multiple instances of the same environment. Each such \emph{actor} can store its own record of past experience, effectively providing a distributed \emph{experience replay memory} with vastly increased capacity compared to a single machine implementation. Alternatively this experience can be explicitly aggregated into a distributed database. In addition to generating more data, distributed actors can explore the state space more effectively, as each actor behaves according to a slightly different policy. 

A conceptually distinct set of distributed \emph{learners} reads samples of stored experience from the experience replay memory, and updates the value function or policy according to a given RL algorithm. Specifically, we focus in this paper on a variant of the DQN algorithm, which applies ASGD updates to the parameters of the Q-network. As in DistBelief, the parameters of the Q-network may also be distributed over many machines.

We applied our distributed framework for RL, known as \emph{Gorila} (General Reinforcement Learning Architecture), to create a massively distributed version of the DQN algorithm. We applied Gorila DQN to 49 games on the Atari 2600 platform. We outperformed single GPU DQN on 41 games and outperformed human professional on 25 games. Gorila DQN also trained much faster than the non-distributed version in terms of wall-time, reaching the performance of single GPU DQN roughly ten times faster for most games.


