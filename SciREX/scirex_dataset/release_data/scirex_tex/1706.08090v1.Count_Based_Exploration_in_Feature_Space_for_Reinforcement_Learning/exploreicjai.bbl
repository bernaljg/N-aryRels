\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Bellemare \bgroup \em et al.\egroup
  }{2013}]{BNVB:2013ale}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem[\protect\citeauthoryear{Bellemare \bgroup \em et al.\egroup
  }{2016}]{Bellemare2016}
Marc~G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David
  Saxton, and R{\'{e}}mi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock {\em CoRR}, abs/1606.01868, 2016.

\bibitem[\protect\citeauthoryear{Kakade}{2003}]{Kakade:2003}
Sham~Machandranath Kakade.
\newblock {\em On the Sample Complexity of Reinforcement Learning}.
\newblock PhD thesis, University College London, 2003.

\bibitem[\protect\citeauthoryear{Kolter and Ng}{2009}]{Kolter2009}
J~Zico Kolter and Andrew~Y Ng.
\newblock {Near-Bayesian exploration in polynomial time}.
\newblock {\em International Conference on Machine Learning}, pages 513--520,
  2009.

\bibitem[\protect\citeauthoryear{Lai and Robbins}{1985}]{LR:1985bandits}
Tze~Leung Lai and Herbert Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in Applied Mathematics}, 6(1):4--22, 1985.

\bibitem[\protect\citeauthoryear{Liang \bgroup \em et al.\egroup
  }{2016}]{LMTB:2015shallow}
Yitao Liang, Marlos~C Machado, Erik Talvitie, and Michael Bowling.
\newblock State of the art control of {A}tari games using shallow reinforcement
  learning.
\newblock In {\em Autonomous Agents and Multi-Agent Systems}, 2016.

\bibitem[\protect\citeauthoryear{Mnih \bgroup \em et al.\egroup
  }{2015}]{Mnih2015}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~a Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,
  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.
\newblock {Human-level control through deep reinforcement learning}.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem[\protect\citeauthoryear{Mnih \bgroup \em et al.\egroup
  }{2016}]{MBMG+2016DQN}
Volodymyr Mnih, Adri{\`{a}}~Puigdom{\`{e}}nech Badia, Mehdi Mirza, Alex Graves,
  Timothy~P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2016.

\bibitem[\protect\citeauthoryear{Nair \bgroup \em et al.\egroup
  }{2015}]{DBLP:journals/corr/NairSBAFMPSBPLM15}
Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon,
  Alessandro~De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles
  Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, and
  David Silver.
\newblock Massively parallel methods for deep reinforcement learning.
\newblock {\em CoRR}, abs/1507.04296, 2015.

\bibitem[\protect\citeauthoryear{Osband \bgroup \em et al.\egroup
  }{2016a}]{DBLP:journals/corr/OsbandBPR16}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin~Van Roy.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock {\em CoRR}, abs/1602.04621, 2016.

\bibitem[\protect\citeauthoryear{Osband \bgroup \em et al.\egroup
  }{2016b}]{Osband2016a}
Ian Osband, Benjamin {Van Roy}, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock {\em International Conference on Machine Learning}, pages 1--26,
  2016.

\bibitem[\protect\citeauthoryear{Ostrovski \bgroup \em et al.\egroup
  }{2017}]{DBLP:journals/corr/OstrovskiBOM17}
Georg Ostrovski, Marc~G. Bellemare, A{\"{a}}ron van~den Oord, and R{\'{e}}mi
  Munos.
\newblock Count-based exploration with neural density models.
\newblock {\em CoRR}, abs/1703.01310, 2017.

\bibitem[\protect\citeauthoryear{Schulman \bgroup \em et al.\egroup
  }{2015}]{DBLP:journals/corr/SchulmanLMJA15}
John Schulman, Sergey Levine, Philipp Moritz, Michael~I. Jordan, and Pieter
  Abbeel.
\newblock Trust region policy optimization.
\newblock {\em CoRR}, abs/1502.05477, 2015.

\bibitem[\protect\citeauthoryear{Stadie \bgroup \em et al.\egroup
  }{2015}]{DBLP:journals/corr/StadieLA15}
Bradly~C. Stadie, Sergey Levine, and Pieter Abbeel.
\newblock Incentivizing exploration in reinforcement learning with deep
  predictive models.
\newblock {\em CoRR}, abs/1507.00814, 2015.

\bibitem[\protect\citeauthoryear{Strehl and Littman}{2004}]{1374179}
A.~L. Strehl and M.~L. Littman.
\newblock An empirical evaluation of interval estimation for {M}arkov decision
  processes.
\newblock In {\em 16th IEEE International Conference on Tools with Artificial
  Intelligence}, pages 128--135, 2004.

\bibitem[\protect\citeauthoryear{Strehl and Littman}{2008}]{Strehl2008}
Alexander~L Strehl and Michael~L Littman.
\newblock An analysis of model-based interval estimation for {M}arkov decision
  processes.
\newblock {\em Journal of Computer and System Sciences}, 74(8):1309--1331,
  2008.

\bibitem[\protect\citeauthoryear{Strehl \bgroup \em et al.\egroup
  }{2009}]{Strehl2009}
Alexander~L Strehl, Lihong Li, and Michael~L Littman.
\newblock {Reinforcement Learning in Finite MDPs : PAC Analysis}.
\newblock {\em Journal of Machine Learning Research}, 10:2413--2444, 2009.

\bibitem[\protect\citeauthoryear{Sutton and Barto}{1998}]{Sutton1998}
R.S. Sutton and A.G. Barto.
\newblock {Reinforcement Learning: An Introduction}.
\newblock {\em IEEE Transactions on Neural Networks}, 9(5):1054--1054, 1998.

\bibitem[\protect\citeauthoryear{Sutton}{1990}]{Sutton:1990:IAL:101883.102055}
Richard~S. Sutton.
\newblock Integrated architecture for learning, planning, and reacting based on
  approximating dynamic programming.
\newblock In {\em International Conference on Machine Learning}, pages
  216--224, 1990.

\bibitem[\protect\citeauthoryear{Tang \bgroup \em et al.\egroup
  }{2016}]{Tang2016exploration}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi~Chen, Yan Duan, John
  Schulman, Filip~De Turck, and Pieter Abbeel.
\newblock {\#}{E}xploration: {A} study of count-based exploration for deep
  reinforcement learning.
\newblock {\em CoRR}, abs/1611.04717, 2016.

\bibitem[\protect\citeauthoryear{van Hasselt \bgroup \em et al.\egroup
  }{2016a}]{DBLP:journals/corr/HasseltGHS16}
Hado van Hasselt, Arthur Guez, Matteo Hessel, and David Silver.
\newblock Learning values across many orders of magnitude.
\newblock {\em CoRR}, abs/1602.07714, 2016.

\bibitem[\protect\citeauthoryear{van Hasselt \bgroup \em et al.\egroup
  }{2016b}]{HGS:2016doubleQ}
Hado van Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock In {\em AAAI}, 2016.

\bibitem[\protect\citeauthoryear{Veness \bgroup \em et al.\egroup
  }{2012}]{veness2012context}
Joel Veness, Kee~Siong Ng, Marcus Hutter, and Michael Bowling.
\newblock Context tree switching.
\newblock In {\em IEEE Data Compression Conference}, pages 327--336, 2012.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup
  }{2016}]{WFL:2015DQN}
Ziyu Wang, Nando de~Freitas, Tom Schaul, Matteo Hessel, Hado van Hasselt, and
  Marc Lanctot.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2016.

\end{thebibliography}
