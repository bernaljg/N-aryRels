In this section we conduct a thorough investigation of WSDDN and its components on weakly supervised detection and image classification.

\subsection{Benchmark data.}
We evaluate our method on the PASCAL VOC 2007 and 2010 datasets \cite{Everingham10}, as they are the most widely-used benchmark in weakly supervised object detection. While the VOC 2007 dataset consists of 2501 training, 2510 validation, and 5011 test images containing bounding box annotations for 20 object categories, VOC 2010 dataset contains 4998 training, 5105 validation, and 9637 test images for the same number of categories. We use the suggested training and validation splits and report results evaluated on \emph{test} split. We report performance of our method on both the object detection and the image classification tasks of PASCAL VOC. 

For detection, we use two performance measures. The first one follows the standard PASCAL VOC protocol and reports average precision (AP) at  $50\%$ intersection-over-union (IoU) of the detected boxes with the ground truth ones. We also report CorLoc, a commonly-used weakly supervised detection measure~\cite{Deselaers12}. CorLoc is the percentage of images that contain at least one instance of the target object class for which the most confident detected bounding box overlaps by at least $50\%$ with one of these instances. Differently from AP, which is measured on the PASCAL test set, CorLoc is evaluated on the union of the training and validation subset of PASCAL. For classification, we use the standard PASCAL VOC protocol and report AP.

\subsection{Experimental setup.}\label{subsec:expsetup}
We comprehensively evaluate our method with three pre-trained CNN models in our experiments as in~\cite{Girshick15}. The first network is the VGG-CNN-F~\cite{Chatfield14} which is similar to AlexNet~\cite{Krizhevsky12} but has reduced number of convolutional filters. We refer to this network as \textbf{S}, for small. The second one is VGG-CNN-M-1024 which has the same depth as \textbf{S} but has smaller stride in the first convolutional layer. We name this network \textbf{M} for medium. The last network is the deep VGG-VD16 model~\cite{Simonyan15} and we call this network \textbf{L} for large. These models, which are pre-trained on the ImageNet ILSVRC 2012 challenge data \cite{Russakovsky15}, attain  $18.8 \%$, $16.1 \%$ and $9.9 \%$ top-5 accuracy respectively (using a single centre-crop) on ILSVRC (importantly no bounding box information is provided during pre-training). As explained in \cref{s:pretrained}, we apply the following modifications to the network. First, we replace the last pooling layer \textit{pool5} with a SPP layer~\cite{He14} which is configured to be compatible with the network's first fully connected layer. Second, we add a parallel detection branch to the classification one that contains a fully-connected layer followed by a soft-max layer. Third, we combine the classification and detection streams by element-wise product followed by summing scores across regions, and feed the latter to a binary log-loss layer. Note that this layer assesses the classification performance for the 20 classes together, but each of them is treated as a different binary classification problem; the reason is that classes can co-occur in the PASCAL VOC, such that the softmax log loss used in AlexNet is not appropriate.

The WSDDNs are trained on the PASCAL VOC training and validation data by using fine-tuning on all layers, a widely-adopted technique to improve the performance of a CNN on a target domain~\cite{Chatfield14}. Here, however, fine tuning performs the essential function of learning the classification and detection streams, effectively causing the network to learn to detect objects, but using only weak image-level supervision. The experiments are run for $20$ epochs and all the layers are fine-tuned with the learning rate $10^{-5}$ for the first ten epochs and $10^{-6}$ for the last ten epochs. Each minibatch contains all region proposals from a single image.

In order to generate candidate regions to use with our networks, we evaluate two proposal methods, Selective Search Windows (SSW)~\cite{Sande11} using its \textit{fast} setting, and EdgeBoxes (EB)~\cite{Zitnick14}. In addition to region proposals, EB provides an objectness score for each region based on the number of contours wholly encloses. We exploit this additional information by multiplying the feature map $\phi_{\text{SPP}}$ proportional to its score via a scaling layer in WSDDN and denote this setting as \emph{Box Sc}. Since we use a SPP layer to aggregate descriptors for each region, images do not need to be resized to a particular size as in the original pre-trained model. Instead, we keep the original aspect ratio of images fixed and resize them to five different scales (setting their maximum of width or height to $\lbrace 480,576,688,864,1200\rbrace$ respectively) as in \cite{He14}. During training, we apply random horizontal flips to the images and select a scale at random as a form of jittering or data augmentation. At test time we average the outputs of 10 images (\ie the 5 scales and their flips). We use the publicly available CNN toolbox MatConvNet~\cite{Vedaldi15} to conduct our experiments and share our code, models and data~\footnote{\url{https://github.com/hbilen/WSDDN}}.

When evaluated on an image, WSDDN produces, for each target class $c$ and image $\bx$, a score $\bx^\mathcal{R}_r = S_c(\bx;r)$ for each region $r$ and an aggregated score $y_c=S_c(\bx)$ for each image. Non-maxima suppression (with $40$ \% IoU threshold) is applied to the regions and then the scored regions and images are pooled together to compute detection AP and CorLoc.

\input{table-voc2007-map-baselines}
\input{table-voc2007-map}
\input{table-voc2007-corloc}
\input{table-voc2007-classif}
\input{table-voc2010-map}
\input{table-voc2010-corloc}

% --------------------------------------------------------------------------------------
\subsection{Detection results}\label{s:detres}
% --------------------------------------------------------------------------------------

\paragraph{Baseline method.} First we design a single stream classification-detection network as an alternative baseline to WSDDN. Part of the construction is similar to WSDDN, as we replace \textit{pool5} layer of VGG-CNN-F model with an SPP. However, we do not branch off two streams, but simply append to the last fully connected layer ($\phi_\text{fc8c}$) the following loss layer \[
\frac{1}{nC}
\sum_{i=1}^n
\sum_{k=1}^C
\max\{0, 1 - y_{ki} \log \sum_{r=1}^{|\mathcal{R}|} \exp(x^{\mathcal{R}}_{cr})\}.
\]
The term $\log \sum_{r=1}^{|\mathcal{R}|} \exp(x^{\mathcal{R}}_{cr}$) is a soft approximation of the max operator $\max_r x_{cr}^\mathcal{R}$ and was found to yield better performance than using the max scoring region. This observation is also reported in \cite{Bilen14}. Note that the non-linearity is necessary as otherwise aggregating region-based scores would sum over the scores of a majority of regions that are uninformative. The loss function is once more a sum of $C$ binary hinge-losses, one for each class. This baseline obtains $21.6 \%$ mAP detection score on the PASCAL VOC test set, which is well below the state-of-the-art ($31.6 \%$ in~\cite{Wang14a}).

\paragraph{Pre-trained CNN architectures.} We evaluate our method with the models \textbf{S}, \textbf{M} and \textbf{L} and also report the results for the ensemble of these models by simply averaging their scores. Table~\ref{tab:voc2007base} shows that WSDDN with individual models \textbf{S} and \textbf{M} are already on par with the state-of-the-art method~\cite{Wang14a} and the ensemble outperforms the best previous score in the VOC 2007 dataset. Differently from supervised detection methods (\eg \cite{Girshick15}), detection performance of WSDDN does not improve with use of wider or deeper networks. In contrast, model \textbf{L} performs significantly worse than models \textbf{S} and \textbf{M} (see~\cref{tab:voc2007base}). This can be explained with the fact that model~\textbf{L} frequently focuses on parts of objects, instead of whole instances, and is still able to associate these parts with object categories due to its smaller convolution strides, higher resolution and deeper architecture.

\paragraph{Object proposals.} Next, we compare the detection performances with two popular object proposal methods, SSW~\cite{Sande11} and EB~\cite{Zitnick14}. While both the region proposals provides comparable quality region proposals, using box scores of EB (denoted as \emph{Box Sc} in~\cref{tab:voc2007base}) leads to a $~2\%$ improvement for models \textbf{S} and \textbf{M} and boosts the detection performance of model \textbf{L} $~5\%$.

\paragraph{Spatial regulariser.} We denote the setting where WSDDN is trained with the additional spatial regularisation term (denoted as \emph{Sp. Reg.} in \cref{tab:voc2007base}). Finally the introduction of the regularisation improves the detection performance $1$, $2$ and $4$ mAP points for models \textbf{S}, \textbf{M} and \textbf{L} respectively. The improvements show that larger network benefits more from introduction of the spatial invariance around high confidence regions.


\paragraph{Comparison with the state of the art.}
After evaluating the design decisions, we follow the best setting (last row in \cref{tab:voc2007base}) and compare WSDDN to the state of the art in weakly supervised detection literature in \cref{tab:voc2007ap} and \cref{tab:voc2007corloc} for the VOC 2007 dataset and in \cref{tab:voc2010ap} and \cref{tab:voc2010corloc} for the VOC 2010 dataset. The results show that our method already achieves overall significantly better performance than these alternatives with a single model and ensemble models further boost the performance. The majority of previous work \cite{Song14,Song14a,Bilen14,Wang14,Bilen15} use the Caffe reference CNN model \cite{Jia13}, which is comparable to model~\textbf{S} in this paper, as a black box to extract features over SSW proposals. In addition to CNN features, Cinbis \etal \cite{Cinbis15} use Fisher Vectors~\cite{Perronnin10a} and EB objectness measure of Zitnick and Dollar \cite{Zitnick14} as well. Differently from the previous work, WSDDN is based on a simple modification of the original CNN architecture fine-tuned on the target data using back-propagation.

Next, we investigate the results in more detail. While our method significantly outperforms the alternatives in majority of categories, is not as strong in chair, person and pottedplant categories. Failure and success case are illustrated in \cref{fig:detexamples}. It can be noted that, by far, the most important failure modality for our system is that an object part (e.g. person face) is detected instead as the object as a whole. This can be explained by the fact that parts such as ``face'' are very often much more discriminative and with a less variable appearance than the rest of the object. Note that the root cause for this failure modality is that we, as many other authors, define objects as image regions that are most predictive for a given object class, and these may not include the object as a whole. Addressing this issue will therefore require incorporating additional cue in the model to try to learn the ``whole object''.

The output of our model could also be used as input to one of the existing methods for weakly-supervised detection that use a CNN as a black-box for feature extraction. Investigating this option is left to future work.

\input{figure-samples}

% --------------------------------------------------------------------------------------
\subsection{Classification Results}
% --------------------------------------------------------------------------------------
While WSDDN is primarily designed for weakly-supervised object detection, ultimately it is trained to perform image classification. Hence, it is interesting to evaluate its performance on this task as well. To this end, we use the PASCAL VOC 2007 benchmark and contrast it to standard fine-tuning techniques that are often used in combination with CNNs and show the results in \cref{tab:voc2010corloc}. These techniques have been thoroughly investigated in~\cite{Chatfield14,He14,Oquab14}. Chatfield \etal~\cite{Chatfield14}, in particular, analyse many variants of fine-tuning, including extensive data augmentation, on the PASCAL VOC. They experiment with three architectures, VGG-F, VGG-M, and VGG-S. While VGG-F is their fastest model, the other two networks are slower but more accurate. As explained in \ref{subsec:expsetup}, we initialise WSDDN \textbf{S} and \textbf{M} with the pre-trained VGG-F and VGG-M-1024 respectively and thus they should be considered as right baselines. WSDDN~\textbf{S} and \textbf{M} improves $~8$ and $~7$ points over VGG-F and VGG-M-1024 respectively. 


We also compare WSDDN to the SPP-net \cite{He14} which uses the Overfeat-7 \cite{Sermanet13} with a 4-level spatial pyramid pooling layer $\{6\times 6, 3\times 3, 2\times 2, 1\times 1\}$ for supervised object detection. While they do not perform fine-tuning, they include a spatial pooling layer. Applied to image classification, their best performance on the PASCAL VOC 2007 is $82.4\%$. Finally we compare WSDDN~\textbf{L} to the competitive VGG-VD16~\cite{Simonyan15}. Interestingly, this method also exploits coarse local information by aggregating the activations of the last fully connected layer over multiple locations and scales. WSDDN~\textbf{L} outperforms this very competitive baseline with a margin of $0.4$ point.

