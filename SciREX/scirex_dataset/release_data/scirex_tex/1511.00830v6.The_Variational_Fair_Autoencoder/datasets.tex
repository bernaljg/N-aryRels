%\subsubsection{Fairness Datasets}
For the fairness task we experimented with three datasets that were previously used by~\citet{zemel2013learning}. The German dataset is the smallest one with $1000$ data points and the objective is to predict whether a person has a good or bad credit rating. The sensitive variable is the gender of the individual. The Adult income dataset contains $45,222$ entries and describes whether an account holder has over $\$50,000$ dollars in their account. The sensitive variable is age. Both of these are obtained from the UCI machine learning repository~\citep{UCI}. The health dataset is derived from the Heritage Health Prize\footnote{\url{www.heritagehealthprize.com}}. It is the largest of the three datasets with $147,473$ entries. The task is to predict whether a patient will spend any days in the hospital in the next year and the sensitive variable is the age of the individual. We use the same train/test/validation splits as~\citet{zemel2013learning} for our experiments. Finally we also binarized the data and used a multivariate Bernoulli distribution for $p_\theta(\*x_n|{\*z_1}_n,\*s_n) = \text{Bern}(\*x_n | \!\pi_n = \sigma(f_\theta({\*z_1}_n, \*s_n)))$, where $\sigma(\cdot)$ is the sigmoid function \footnote{$\sigma(t) = \frac{1}{1 + e^{-t}}$}.

%\subsection{Amazon Reviews Dataset}
For the domain adaptation task we used the Amazon reviews dataset (with similar preprocessing) that was also employed by~\citet{chen2012marginalized} and~\citet{2015arXiv150507818G}. It is composed from text reviews about particular products, where each product belongs to one out of four different domains: ``books'', ``dvd'', ``electronics'' and ``kitchen''. As a result we performed twelve domain adaptation tasks. The labels $\*y$ correspond to the sentiment of each review, i.e. either positive or negative. Since each feature vector $\*x$ is composed from counts of unigrams and bigrams we used a Poisson distribution for $p_\theta(\*x_n|{\*z_1}_n, \*s_n) = \text{Poisson}(\*x_n | \!\lambda_n = e^{f_\theta({\*z_1}_n, \*s_n)})$. It is also worthwhile to mention that we can fully exploit the semi-supervised nature of our model in this dataset, and thus for training we only use the source domain labels and consider the labels of the target domain as ``missing''.

%\subsection{Extended Yale B Dataset}
For the general task of learning invariant representations we used the Extended Yale B dataset, which was also employed in a similar fashion by~\citet{li2014learning}. It is composed from face images of 38 people under different lighting conditions (directions of the light source). Similarly to~\citet{li2014learning}, we created 5 states for the nuisance variable $\*s$: light source in upper right, lower right, lower left, upper left and the front. The labels $\*y$ correspond to the identity of the person. Following~\citet{li2014learning}, we used the same training, test set and no validation set. For the $p(\*x_n|{\*z_1}_n, \*s_n)$ distribution we used a Gaussian with means constrained in the 0-1 range (since we have intensity images) by a sigmoid, i.e. $p_\theta(\*x_n|{\*z_1}_n, \*s_n) = \mathcal{N}(\*x_n| \!\mu_n = \sigma(f_\theta({\*z_1}_n, \*s_n)), \!\sigma_n = e^{f_\theta({\*z_1}_n, \*s_n)})$.
