% Related work
Most related to our ``fair'' representations view is the work from~\cite{zemel2013learning}. They proposed a neural network based semi-supervised clustering model for learning fair representations. The idea is to learn a localised representation that maps each datapoint to a cluster in such a way that each cluster gets assigned roughly equal proportions of data from each group in $s$. Although their approach was successfully applied on several datasets, the restriction to clustering means that it cannot leverage the representational power of a distributed representation. Furthermore, this penalty does not account for higher order moments in the latent distribution. For example, if $p(z_k=1 | x_i, s=0)$ always returns $1$ or $0$, while $p(z_k=1 | x_i, s=1)$ returns values between values $0$ and $1$, then the penalty could still be satisfied, but information could still leak through. We addressed both of these issues in this paper.

Domain adaptation can also be cast as learning representations that are ``invariant'' with respect to a discrete variable $\*s$, the domain. Most similar to our work are neural network approaches which try to match the feature distributions between the domains. This was performed in an unsupervised way with mSDA~\citep{chen2012marginalized} by training denoising autoencoders jointly on all domains, thus implicitly obtaining a representation general enough to explain both the domain and the data. This is in contrast to our approach where we instead try to learn representations that explicitly remove domain information during the learning process. For the latter we find more similarities with ``domain-regularized'' supervised approaches that simultaneously try to predict the label for a data point and remove domain specific information. This is done with either MMD~\citep{long2015learning, DBLP:journals/corr/TzengHZSD14} or adversarial~\citep{2015arXiv150507818G} penalties at the hidden layers of the network. In our model however the main ``domain-regularizer'' stems from the independence properties of the prior over the domain and latent representations. We also employ MMD on our model but from a different perspective since we consider a slightly more difficult case where the domain $\*s$ and label $\*y$ are correlated; we need to ensure that we remain as ``invariant'' as possible since $q_\phi(\*y|\*z_1)$ might `leak' information about $\*s$. 