\section{Conclusions}

We presented a simple and yet powerful model architecture
that produces state-of-the-art results for POS tagging,
dependency parsing and sentence compression.
Our model combines the flexibility of transition-based algorithms and
the modeling power of neural networks.
Our results demonstrate that feed-forward network without
recurrence can outperform recurrent models such as LSTMs
when they are trained with global normalization.
We further support our empirical findings 
with a proof showing that global normalization
helps the model overcome the label bias problem
from which locally normalized models suffer.
