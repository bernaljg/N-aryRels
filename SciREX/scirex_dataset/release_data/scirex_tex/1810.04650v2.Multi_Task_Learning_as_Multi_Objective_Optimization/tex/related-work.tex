% !TEX root = ../multi_task.tex

\noindent \textbf{Multi-task learning.}
We summarize the work most closely related to ours and refer the interested reader to reviews by \citet{Ruder2017} and \citet{zhou2011malsar} for additional background.
%As the review of the Multi-Task learning (MTL) literature is beyond the scope of this paper, we recommend the literature review by \citet{Ruder2017} for an interested reader while we only summarise the work closely related to us here.
Multi-task learning (MTL) is typically conducted via hard or soft parameter sharing. In hard parameter sharing, a subset of the parameters is shared between tasks while other parameters are task-specific. In soft parameter sharing, all parameters are task-specific but they are jointly constrained via Bayesian priors \citep{Xue2007, Bakker2003} or a joint dictionary \citep{Argyriou2007, Long2015, Yang2017, Ruder2017}.  We focus on hard parameter sharing with gradient-based optimization, following the success of deep MTL in computer vision \citep{Bilen2016, Misra2016, Rudd2016, Yang2017, Kokkinos2016, Zamir2018}, natural language processing \citep{Collobert2008, Dong2015, Liu2015, Luong2015, Hashimoto2016}, speech processing \citep{Huang2013,Seltzer2013,Huang2015}, and even seemingly unrelated domains over multiple modalities \citep{Kaiser2017}.

\citet{Baxter2000} theoretically analyze the MTL problem as interaction between individual learners and a meta-algorithm. Each learner is responsible for one task and a meta-algorithm decides how the shared parameters are updated. All aforementioned MTL algorithms use weighted summation as the meta-algorithm. Meta-algorithms that go beyond weighted summation have also been explored. \citet{Cong2014} consider the case where each individual learner is based on kernel learning and utilize multi-objective optimization. \citet{Zhang2010} consider the case where each learner is a linear model and use a task affinity matrix. \citet{Zhou2011} and \citet{Bagherjeiran2005} use the assumption that tasks share a dictionary and develop an expectation-maximization-like meta-algorithm. \citet{Miranda2012} and \citet{ZhouDi2017} use swarm optimization. None of these methods apply to gradient-based learning of high-capacity models such as modern deep networks. \citet{Kendall2018} and \citet{Chen2018} propose heuristics based on uncertainty and gradient magnitudes, respectively, and apply their methods to convolutional neural networks. Another recent work uses multi-agent reinforcement learning \citep{Rosenbaum2017}.
%In comparison, we derive a principled algorithm using gradient-based multi-objective optimization.

\noindent \textbf{Multi-objective optimization.}
Multi-objective optimization addresses the problem of optimizing a set of possibly contrasting objectives. We recommend \citet{Miettinen1999} and \citet{Ehrgott2005} for surveys of this field. Of particular relevance to our work is gradient-based multi-objective optimization, as developed by \citet{Fliege2000}, \citet{Schaffler2002}, and \citet{Desideri2012}. These methods use multi-objective Karush-Kuhn-Tucker (KKT) conditions \citep{Kuhn1951} and find a descent direction that decreases all objectives. This approach was extended to stochastic gradient descent by \citet{Peitz2017} and \citet{Poirion2017}. In machine learning, these methods have been applied to multi-agent learning \citep{Ghish2013, Pirotta2016, Parisi2014}, kernel learning \citep{Cong2014}, sequential decision making \citep{Whiteson2018}, and Bayesian optimization \citep{Shah2016, Lobato2016}. Our work applies gradient-based multi-objective optimization to multi-task learning.
