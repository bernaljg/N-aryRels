\label{sec:related}
\myparaly{3D Shape Segmentation}
An important application of our framework is to obtain semantic part segmentation of 3D shapes in a supervised fashion. Along this track, most previous methods \cite{kalogerakis2010learning,xie20143d,makadia2014learning,guo20153d} employ traditional machine learning techniques and construct classifiers based on geometric features.
%\cite{guo20153d} proposes to use deep neural network for 3D triangle mesh labeling. Each triangle is treated as an individual entity and a neural network is learned to map its geometric features to an segment label. This framework does not leverage the power of neural network for multi-scale feature learning, which has been proved to be very important in image domain and is one of our focus.
In the domain of unsupervised shape segmentation, there is one family of methods \cite{liu2004segmentation,liu2007mesh} emphasizes the effectiveness of spectral analysis for 3D shape segmentation. Inspired by this, our framework aims to marry the powerfulness of deep neural network and spectral analysis for 3D shapes segmentation.
\iffalse
\todo{
  \begin{itemize}
    \item brief review existing works on 3d shape segmentation
    \item emphasize the family of spectral methods based shape segmentation papers. make it clear that the intrinsic property is extremely important for 3d shape segmenentation, in contrast to 2d segmentation. 
  \end{itemize}
}
\fi

\mypara{Spectral analysis on graphs}
We model a 3D shape $\shape$ as a graph $\graph=(\vertex,\edge)$, whose vertices $\vertex$ are points in $\R^3$ and edges $\edge$ connect nearby points. At each vertex of the graph, we can assign a vector. In this way, we define a vector-valued vertex function on $\graph$. For example, a segment on a shape can be represented as an indicator vertex function. this is the functional view of segmentation, as introduced in \cite{wang2013image,wang2014unsupervised}. The space of functions $\mathcal{F}$ defined on $\graph$ can be represented under different bases, i.e., $f=\sum_i \alpha_i \myvec{b}_i$ for $f\in\mathcal{F}$. One way to construct bases of $\mathcal{F}$ is through spectral analysis -- for each shape graph, the eigen vectors of its graph laplacian $L$ form an orthogonal bases $\myvec{B} = \{\myvec{b}_i\}$. One type of graph laplacian can be constructed as $L=I-D^{-1/2}WD^{-1/2}$, where $I$ is identity matrix, $D$ is the degree matrix and $W$ is the adjacency weight matrix of $\graph$. Under this construction, the eigenvalues $\myvec{\lambda}=\{\lambda_i\}$ corresponding to $\myvec{B}$ satisfy $0\le \lambda_i\le2$.

As is in Fourier analysis, the spectral decomposition also introduces the concept of frequency. For each basis $\myvec{b_i}$, the eigenvalue $\lambda_i$ in the decomposition defines its frequency, depicting its smoothness. By projecting $f$ on each basis $\myvec{b}_i$, the coefficient $\alpha_i$ can be obtained. $\myvec{\alpha} = \{\alpha_i\}$ is the spectral representation of $f$, in analogy to the Fourier transform. The convolution theorem of Fourier analysis can be extended to the laplacian spectrum: the convolution between a kernel and a function on the shape graph is equivalent to the point wise multiplication of their spectral representations \cite{bruna2013spectral,shuman2016vertex}.% Similar to Fourier analysis, a smooth function in the spatial domain corresponds to a localized function in the low-frequency end of the spectrum. Conversely, a localized function in the spatial domain corresponds to a smooth function in the spectral domain.
\iffalse
\todo{
  \begin{itemize}
    \item we model a 3d shape $\shape$ as a graph $\graph$, whose vertices are points in $\R^3$ and edges connect nearby points. % this graph can be viewed as an approximation to the underlying shape surface.
    \item at each vertex of the graph, we can assign a vector. In this way, we define a vector-valued vertex function on $\graph$.
    \item for example, a segment on a shape can be represented as an indicator vertex function. this is the functional view of segmentation, as introduced in Fan's work [CITATION].
    \item the space of functions $\mathcal{F}$ defined on $\graph$ can be represented under different bases, i.e., $f=\sum_i \alpha_i \myvec{b}_i$ for $f\in\mathcal{F}$. One way to construct bases of $\mathcal{F}$ is through spectral analysis -- for each shape graph, the eigen vectors of its graph laplacian form an orthogonal bases $\myvec{B} = \{\myvec{b}_i\}$. 
    \item the spectral decomposition also introduces the concept of frequency, as is in Fourier analysis. for each basis, the eigenvalue in the decomposition defines its frequency, depicting its smoothness. by projecting $f$ on each basis $\myvec{b}_i$, the coefficient $\alpha_i$ can be obtained. $\{\alpha_i\}$ is the spectral representation of $f$, in analogy to the Fourier transform.
    \item the convolution theorem of Fourier analysis can be extended to the laplacian spectrum: the convolution between a kernel and a function on the shape graph is equivalent to the point wise multiplication of their spectral representations.
    \item similar to Fourier analysis, a smooth function in the spatial domain corresponds to a localized function in the low-frequency end of the spectrum. Conversely, a localized function in the spatial domain corresponds to a smooth function in the spectral domain.
  \end{itemize}  
}
\fi

\mypara{Functional map}
Different shapes define shape graphs with varied bases and spectral domains, which results in incomparable graph vertex function. Inspired by the recent work on synchronization \cite{singer2011angular,wang2013image,wang2014unsupervised}, we propose to align these different spectral domains using functional map \cite{ovsjanikov2012functional}. Functional map is initially introduced for this purpose on shapes. Specifically, given a pair of shape graph $\graph_i$ and $\graph_j$, a functional map from $\mathcal{F}_i$ to $\mathcal{F}_j$ is given by a matrix $X_{ij}$, which maps a function $f\in \mathcal{F}_i$ with coefficient vector $\myvec \alpha$ to the function $f'\in \mathcal{F}_j$  with coefficient vector $\myvec \alpha' = X_{ij} \myvec \alpha$. $\myvec \alpha$ and $\myvec \alpha'$ are computed according to a pair of bases. We refer the reader to \cite{ovsjanikov2012functional} for detailed introduction and intuition.  
\iffalse
\todo{
  \begin{itemize}
    \item each shape defines a different shape graph, thus different graph laplacian. 
    \item consequently, they generate individual bases, thus independent spectral domain.
    \item Therefore, functions defined on different shapes are not directly comparable, because the spectral domain they live in are not aligned.
    \item functional map is introduced to align the spectral domain of different shapes. specifically, given a pair of shape graph $\graph_i$ and $\graph_j$, a functional map from $\mathcal{F}_i$ to $\mathcal{F}_j$ is given by a matrix $X_{ij}$, which maps a function $f\in \mathcal{F}_i$ with coefficient vector $\myvec \alpha$ to the function $f'\in \mathcal{F}_j$  with coefficient vector $\myvec \alpha' = X_{ij} \myvec \alpha$. $\myvec \alpha$ and $\myvec \alpha'$ are computed according to a pair of bases. We refer the reader to CITATION for a more detailed introduction and intuition. 
\end{itemize} 
}
\fi

\mypara{CNN on Graphs}
We call such CNNs as ``graph CNNs''. Graph CNNs takes a graph with vertex function as input. Conventional image CNN can be viewed as a graph CNN on 2D regular grids of pixels, with RGB values as the vertex function. There have been some previous work studying graph CNN on more general graphs instead of 2D regular grids \cite{bruna2013spectral,duvenaud2015convolutional,henaff2015deep,defferrard2016convolutional}, and \cite{masci2015geodesic,boscaini2015learning,boscaini2016learning} have a special focus on near-isometric 3D shape graphs like human bodies. To generalize image CNN, These work usually tries to tackle the following three challenges: defining translation structures on graphs to allow parameter sharing; designing compactly supported filters on graphs; aggregating multi-scale information. Their constructions of deep neural network usually fall into two types: spatial construction and spectral construction. The approach we propose belongs to the family of spectral construction but with two key differences: we explicitly design an effective multi-scale information aggregation scheme; we synchronize different spectral domains to allow parameter sharing among very different shape graphs thus increasing generalizability of our SyncSpecCNN.
%We call such CNNs as ``graph CNNs''. Graph CNNs takes a graph with vertex function as input. Conventional image CNN can be viewed as a graph CNN on 2D regular grids of pixels, with RGB values as the vertex function. There have been some previous work studying graph CNN on more general graphs instead of 2D regular grids \cite{bruna2013spectral,duvenaud2015convolutional,henaff2015deep,defferrard2016convolutional}, and \cite{masci2015geodesic,boscaini2015learning,boscaini2016learning} have a special focus on near-isometric 3D shape graphs like human bodies. To generalize image CNN, These work usually tries to tackle the following three challenges: defining translation structures on graphs to allow parameter sharing; designing compactly supported filters on graphs; aggregating multi-scale information. Their constructions of deep neural network usually fall into two types: spatial construction and spectral construction. Among spatial construction, \cite{bruna2013spectral} uses graph weight to define locally supported convolution filters, adopts hierarchical graph clustering for multiresolution analysis but cannot induce parameter sharing. \cite{masci2015geodesic} proposes to use  local geodesic polar coordinates for filter design and paramter sharing with the limitation of constrained filter size and sensitivity to data corruption. Spectral construction leverages the convolution theorem in Fourier analysis and convert convolution on graphs into pointwise multiplication in spectral domain. \cite{bruna2013spectral} does not design multiresolution analysis scheme in their spectral construction and considers running CNN on a fixed graph structure, thus lacking of generalizability to different graphs. \cite{boscaini2015learning,boscaini2016learning} considers the special case of designing CNN on near-isometric shape graphs, with limited generalizability to a collection of very different shapes, as what we consider. Besides, there is no explicit multi-scale information aggregation scheme designed in their settings. The approach we propose belongs to the family of spectral construction but with two key differences: we explicitly design an effective multi-scale information aggregation scheme; we synchronize different spectral domains to allow parameter sharing among very different shape graphs thus increasing generalizability of our SyncSpecCNN.

\iffalse
\todo{ 
\begin{itemize}
\item{Spatial Construction: \\GCNN: requires a triangular mesh, could only use small kernels for convolution, sensitive to incomplete input data; \\Deep Locally Connected Networks: no parameter sharing among convolution kernels, needs sophisticated hierarchical graph clustering for multi-resolution processing;}
\item{Spectral Construction: \\SCNN: lack of generalizability, cannot be used across different graph domains; \\LSCNN: uses localized kernel function for convolution, has better generalizability but not good at capturing global context efficiently; \\ACNN: considers oriented pattern while constructing the convolution kernels but still not tackles the generalizability problem.}
\end{itemize}
}
\fi

