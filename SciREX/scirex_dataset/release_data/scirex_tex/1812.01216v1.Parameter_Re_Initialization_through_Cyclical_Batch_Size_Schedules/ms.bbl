\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning]{snli:emnlp2015}
Samuel~R. Bowman, Gabor Angeli, Christopher Potts, and Christopher~D. Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}. Association for Computational
  Linguistics, 2015.

\bibitem[Chen et~al.(2017)Chen, Zhu, Ling, Wei, Jiang, and
  Inkpen]{chen2017enhanced}
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si~Wei, Hui Jiang, and Diana Inkpen.
\newblock Enhanced lstm for natural language inference.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages
  1657--1668, 2017.

\bibitem[Devarakonda et~al.(2017)Devarakonda, Naumov, and
  Garland]{devarakonda2017adabatch}
Aditya Devarakonda, Maxim Naumov, and Michael Garland.
\newblock Adabatch: Adaptive batch sizes for training deep neural networks.
\newblock \emph{arXiv preprint arXiv:1712.02029}, 2017.

\bibitem[Gholami et~al.(2018)Gholami, Azad, Jin, Keutzer, and
  Buluc]{gholami2017integrated}
Amir Gholami, Ariful Azad, Peter Jin, Kurt Keutzer, and Aydin Buluc.
\newblock Integrated model, batch and domain parallelism in training neural
  networks.
\newblock \emph{ACM Symposium on Parallelism in Algorithms and
  Architectures(SPAA'18)}, 2018.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 249--256, 2010.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow6572explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples (2014).
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hu et~al.(2017)Hu, Yao, and Li]{hu2017adaptive}
Zixi Hu, Zhewei Yao, and Jinglai Li.
\newblock On an adaptive preconditioned crank--nicolson mcmc algorithm for
  infinite dimensional bayesian inference.
\newblock \emph{Journal of Computational Physics}, 332:\penalty0 492--503,
  2017.

\bibitem[Huang et~al.(2017)Huang, Li, Pleiss, Liu, Hopcroft, and
  Weinberger]{huang2017snapshot}
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John~E Hopcroft, and Kilian~Q
  Weinberger.
\newblock Snapshot ensembles: Train 1, get m for free.
\newblock \emph{arXiv preprint arXiv:1704.00109}, 2017.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2017three}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Krizhevsky(2014)]{krizhevsky2014one}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1404.5997}, 2014.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Martin and Mahoney(2017)]{martin2017rethinking}
Charles~H Martin and Michael~W Mahoney.
\newblock Rethinking generalization requires revisiting old ideas: statistical
  mechanics approaches and complex learning behavior.
\newblock \emph{arXiv preprint arXiv:1710.09553}, 2017.

\bibitem[Martin and Mahoney(2018)]{martin2018implicit}
Charles~H Martin and Michael~W Mahoney.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock \emph{arXiv preprint arXiv:1810.01075}, 2018.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Mishkin and Matas(2015)]{mishkin2015all}
Dmytro Mishkin and Jiri Matas.
\newblock All you need is a good init.
\newblock \emph{arXiv preprint arXiv:1511.06422}, 2015.

\bibitem[Roberts and Rosenthal(2009)]{roberts2009examples}
Gareth~O Roberts and Jeffrey~S Rosenthal.
\newblock Examples of adaptive mcmc.
\newblock \emph{Journal of Computational and Graphical Statistics}, 18\penalty0
  (2):\penalty0 349--367, 2009.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Smith(2017)]{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{Applications of Computer Vision (WACV), 2017 IEEE Winter
  Conference on}, pages 464--472. IEEE, 2017.

\bibitem[Smith and Le(2018)]{smith2018bayesian}
Samuel~L Smith and Quoc~V Le.
\newblock A {Bayesian} perspective on generalization and {Stochastic Gradient
  Descent}.
\newblock \emph{arXiv preprint arXiv:1710.06451}, 2018.

\bibitem[Smith et~al.(2017)Smith, Kindermans, and Le]{smith2017don}
Samuel~L Smith, Pieter-Jan Kindermans, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{N18-1101}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122. Association for
  Computational Linguistics, 2018.
\newblock URL \url{http://aclweb.org/anthology/N18-1101}.

\bibitem[Xu et~al.(2017)Xu, Roosta-Khorasan, and Mahoney]{xu2017second}
Peng Xu, Farbod Roosta-Khorasan, and Michael~W Mahoney.
\newblock Second-order optimization for non-convex machine learning: An
  empirical study.
\newblock \emph{arXiv preprint arXiv:1708.07827}, 2017.

\bibitem[Yao et~al.(2018{\natexlab{a}})Yao, Gholami, Keutzer, and
  Mahoney]{yao2018large}
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael Mahoney.
\newblock Large batch size training of neural networks with adversarial
  training and second-order information.
\newblock \emph{arXiv preprint arXiv:1810.01021}, 2018{\natexlab{a}}.

\bibitem[Yao et~al.(2018{\natexlab{b}})Yao, Gholami, Lei, Keutzer, and
  Mahoney]{yao2018hessian}
Zhewei Yao, Amir Gholami, Qi~Lei, Kurt Keutzer, and Michael~W Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock \emph{arXiv preprint arXiv:1802.08241}, 2018{\natexlab{b}}.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\end{thebibliography}
