\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anschel et~al.(2017)Anschel, Baram, and Shimkin]{anschel2017averaged}
Oron Anschel, Nir Baram, and Nahum Shimkin.
\newblock Averaged-dqn: Variance reduction and stabilization for deep
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  176--185, 2017.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{J. Artif. Intell. Res.(JAIR)}, 47:\penalty0 253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Marc~G Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.06887}, 2017.

\bibitem[Fortunato et~al.(2017)Fortunato, Azar, Piot, Menick, Osband, Graves,
  Mnih, Munos, Hassabis, Pietquin, et~al.]{fortunato2017noisy}
Meire Fortunato, Mohammad~Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian
  Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin,
  et~al.
\newblock Noisy networks for exploration.
\newblock \emph{arXiv preprint arXiv:1706.10295}, 2017.

\bibitem[Gu et~al.(2017)Gu, Lillicrap, Ghahramani, Turner, and Levine]{gu2016q}
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard~E Turner, and Sergey
  Levine.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[He et~al.(2017)He, Liu, Schwing, and Peng]{he2016learning}
Frank~S He, Yang Liu, Alexander~G Schwing, and Jian Peng.
\newblock Learning to play in a day: Faster deep reinforcement learning by
  optimality tightening.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Hessel et~al.(2017)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1710.02298}, 2017.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lin(1992)]{lin1992self}
Long-H Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3/4):\penalty0 69--97, 1992.

\bibitem[Mitliagkas et~al.(2016)Mitliagkas, Zhang, Hadjis, and
  R{\'e}]{mitliagkas2016asynchrony}
Ioannis Mitliagkas, Ce~Zhang, Stefan Hadjis, and Christopher R{\'e}.
\newblock Asynchrony begets momentum, with an application to deep learning.
\newblock In \emph{Communication, Control, and Computing (Allerton), 2016 54th
  Annual Allerton Conference on}, pp.\  997--1004. IEEE, 2016.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih15human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy~P
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Moore \& Atkeson(1993)Moore and Atkeson]{moore1993prioritized}
Andrew~W Moore and Christopher~G Atkeson.
\newblock Prioritized sweeping: Reinforcement learning with less data and less
  time.
\newblock \emph{Machine learning}, 13\penalty0 (1):\penalty0 103--130, 1993.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
R{\'e}mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1046--1054, 2016.

\bibitem[O'Donoghue et~al.(2017)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{o2016combining}
Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih.
\newblock Combining policy gradient and q-learning.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Doina Precup, Richard~S Sutton, and Satinder Singh.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, 2000.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup01offpolicy}
Doina Precup, Richard~S Sutton, and Sanjoy Dasgupta.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{Proceedings of the 18th International Conference on Machine
  Laerning}, pp.\  417--424, 2001.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Martin Riedmiller.
\newblock Neural fitted q iteration-first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{ECML}, volume 3720, pp.\  317--328. Springer, 2005.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul16prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML-15)}, pp.\  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{agzero}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van~den
  Driessche, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 10 2017.
\newblock URL \url{http://dx.doi.org/10.1038/nature24270}.

\bibitem[Sutton et~al.(2000)Sutton, Mcallester, Singh, and
  Mansour]{Sutton00policygradient}
Richard~S. Sutton, David Mcallester, Satinder Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{In Advances in Neural Information Processing Systems 12},
  pp.\  1057--1063. MIT Press, 2000.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, pp.\  2094--2100, 2016.

\bibitem[Velskii \& Landis(1976)Velskii and Landis]{velskii1976avl}
Adelâ€™son~G Velskii and E~Landis.
\newblock An algorithm for the organisation of information.
\newblock \emph{Dokl. Akad. Nauk SSSR}, 146:\penalty0 263--266, 1976.

\bibitem[Vezhnevets et~al.(2017)Vezhnevets, Osindero, Schaul, Heess, Jaderberg,
  Silver, and Kavukcuoglu]{vezhnevets2017feudal}
Alexander~Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max
  Jaderberg, David Silver, and Koray Kavukcuoglu.
\newblock Feudal networks for hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.01161}, 2017.

\bibitem[Wang et~al.(2015)Wang, Schaul, Hessel, van Hasselt, Lanctot, and
  de~Freitas]{wang2015dueling}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando
  de~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock \emph{International Conference on Machine Learning}, pp.\
  1995--2003, 2015.

\bibitem[Wang et~al.(2017)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{wang2017sample}
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray
  Kavukcuoglu, and Nando de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992}
C.~J. C.~H. Watkins and P.~Dayan.
\newblock Q-learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3):\penalty0 272--292, 1992.

\bibitem[Wiering(1999)]{wiering1999explorations}
Marco~A Wiering.
\newblock \emph{Explorations in efficient reinforcement learning}.
\newblock PhD thesis, University of Amsterdam, 1999.

\bibitem[Zhao et~al.(2016)Zhao, Wang, Shao, and Zhu]{zhao2016deep}
Dongbin Zhao, Haitao Wang, Kun Shao, and Yuanheng Zhu.
\newblock Deep reinforcement learning with experience replay based on sarsa.
\newblock In \emph{Computational Intelligence (SSCI), 2016 IEEE Symposium
  Series on}, pp.\  1--6. IEEE, 2016.

\end{thebibliography}
