\begin{thebibliography}{65}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Al-Rfou et~al.(2018)Al-Rfou, Choe, Constant, Guo, and
  Jones}]{al2018character}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018.
\newblock Character-level language modeling with deeper self-attention.
\newblock \emph{arXiv preprint arXiv:1808.04444}.

\bibitem[{Baevski and Auli(2018)}]{baevski2018adaptive}
Alexei Baevski and Michael Auli. 2018.
\newblock Adaptive input representations for neural language modeling.
\newblock \emph{arXiv preprint arXiv:1809.10853}.

\bibitem[{Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio}]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}.

\bibitem[{Bai et~al.(2018)Bai, Kolter, and Koltun}]{bai2018empirical}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun. 2018.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}.

\bibitem[{Bengio et~al.(2003)Bengio, Ducharme, Vincent, and
  Jauvin}]{bengio2003neural}
Yoshua Bengio, R{\'e}jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003.
\newblock A neural probabilistic language model.
\newblock \emph{Journal of machine learning research}, 3(Feb):1137--1155.

\bibitem[{Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson}]{chelba2013one}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson. 2013.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv preprint arXiv:1312.3005}.

\bibitem[{Chung et~al.(2016)Chung, Ahn, and Bengio}]{chung2016hierarchical}
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. 2016.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1609.01704}.

\bibitem[{Cooijmans et~al.(2016)Cooijmans, Ballas, Laurent, G{\"u}l{\c{c}}ehre,
  and Courville}]{cooijmans2016recurrent}
Tim Cooijmans, Nicolas Ballas, C{\'e}sar Laurent, {\c{C}}a{\u{g}}lar
  G{\"u}l{\c{c}}ehre, and Aaron Courville. 2016.
\newblock Recurrent batch normalization.
\newblock \emph{arXiv preprint arXiv:1603.09025}.

\bibitem[{Dai and Le(2015)}]{dai2015semi}
Andrew~M Dai and Quoc~V Le. 2015.
\newblock Semi-supervised sequence learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  3079--3087.

\bibitem[{Dauphin et~al.(2016)Dauphin, Fan, Auli, and
  Grangier}]{dauphin2016language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2016.
\newblock Language modeling with gated convolutional networks.
\newblock \emph{arXiv preprint arXiv:1612.08083}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and
  Toutanova}]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Dieng et~al.(2016)Dieng, Wang, Gao, and Paisley}]{dieng2016topicrnn}
Adji~B Dieng, Chong Wang, Jianfeng Gao, and John Paisley. 2016.
\newblock Topicrnn: A recurrent neural network with long-range semantic
  dependency.
\newblock \emph{arXiv preprint arXiv:1611.01702}.

\bibitem[{Gal and Ghahramani(2016)}]{gal2016theoretically}
Yarin Gal and Zoubin Ghahramani. 2016.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1019--1027.

\bibitem[{Grave et~al.(2016{\natexlab{a}})Grave, Joulin, Ciss{\'e}, Grangier,
  and J{\'e}gou}]{grave2016efficient}
Edouard Grave, Armand Joulin, Moustapha Ciss{\'e}, David Grangier, and
  Herv{\'e} J{\'e}gou. 2016{\natexlab{a}}.
\newblock Efficient softmax approximation for gpus.
\newblock \emph{arXiv preprint arXiv:1609.04309}.

\bibitem[{Grave et~al.(2016{\natexlab{b}})Grave, Joulin, and
  Usunier}]{grave2016improving}
Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016{\natexlab{b}}.
\newblock Improving neural language models with a continuous cache.
\newblock \emph{arXiv preprint arXiv:1612.04426}.

\bibitem[{Graves(2013)}]{graves2013generating}
Alex Graves. 2013.
\newblock Generating sequences with recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1308.0850}.

\bibitem[{Graves et~al.(2014)Graves, Wayne, and Danihelka}]{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}.

\bibitem[{Ha et~al.(2016)Ha, Dai, and Le}]{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le. 2016.
\newblock Hypernetworks.
\newblock \emph{arXiv preprint arXiv:1609.09106}.

\bibitem[{Hochreiter et~al.(2001)Hochreiter, Bengio, Frasconi, Schmidhuber
  et~al.}]{hochreiter2001gradient}
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J{\"u}rgen Schmidhuber, et~al.
  2001.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies.

\bibitem[{Hochreiter and Schmidhuber(1997)}]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber. 1997.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9(8):1735--1780.

\bibitem[{Huang et~al.(2018)Huang, Vaswani, Uszkoreit, Shazeer, Hawthorne, Dai,
  Hoffman, and Eck}]{huang2018improved}
Cheng-Zhi~Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis
  Hawthorne, Andrew~M Dai, Matthew~D Hoffman, and Douglas Eck. 2018.
\newblock An improved relative self-attention mechanism for transformer with
  application to music generation.
\newblock \emph{arXiv preprint arXiv:1809.04281}.

\bibitem[{Inan et~al.(2016)Inan, Khosravi, and Socher}]{inan2016tying}
Hakan Inan, Khashayar Khosravi, and Richard Socher. 2016.
\newblock Tying word vectors and word classifiers: A loss framework for
  language modeling.
\newblock \emph{arXiv preprint arXiv:1611.01462}.

\bibitem[{Ji et~al.(2015)Ji, Cohn, Kong, Dyer, and Eisenstein}]{ji2015document}
Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein.
  2015.
\newblock Document context language models.
\newblock \emph{arXiv preprint arXiv:1511.03962}.

\bibitem[{Jozefowicz et~al.(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and
  Wu}]{jozefowicz2016exploring}
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
  2016.
\newblock Exploring the limits of language modeling.
\newblock \emph{arXiv preprint arXiv:1602.02410}.

\bibitem[{Kalchbrenner et~al.(2016)Kalchbrenner, Espeholt, Simonyan, Oord,
  Graves, and Kavukcuoglu}]{kalchbrenner2016neural}
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van~den Oord, Alex
  Graves, and Koray Kavukcuoglu. 2016.
\newblock Neural machine translation in linear time.
\newblock \emph{arXiv preprint arXiv:1610.10099}.

\bibitem[{Kanai et~al.(2018)Kanai, Fujiwara, Yamanaka, and
  Adachi}]{kanai2018sigsoftmax}
Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, and Shuichi Adachi. 2018.
\newblock Sigsoftmax: Reanalysis of the softmax bottleneck.
\newblock \emph{arXiv preprint arXiv:1805.10829}.

\bibitem[{Ke et~al.(2018)Ke, GOYAL, Bilaniuk, Binas, Mozer, Pal, and
  Bengio}]{ke2018sparse}
Nan~Rosemary Ke, Anirudh Goyal ALIAS~PARTH GOYAL, Olexa Bilaniuk, Jonathan
  Binas, Michael~C Mozer, Chris Pal, and Yoshua Bengio. 2018.
\newblock Sparse attentive backtracking: Temporal credit assignment through
  reminding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7650--7661.

\bibitem[{Khandelwal et~al.(2018)Khandelwal, He, Qi, and
  Jurafsky}]{khandelwal2018sharp}
Urvashi Khandelwal, He~He, Peng Qi, and Dan Jurafsky. 2018.
\newblock Sharp nearby, fuzzy far away: How neural language models use context.
\newblock \emph{arXiv preprint arXiv:1805.04623}.

\bibitem[{Knol(2017)}]{cmix}
Bryon Knol. 2017.
\newblock cmix v13.
\newblock \url{http://www.byronknoll.com/cmix.html}.

\bibitem[{Koutnik et~al.(2014)Koutnik, Greff, Gomez, and
  Schmidhuber}]{koutnik2014clockwork}
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. 2014.
\newblock A clockwork rnn.
\newblock \emph{arXiv preprint arXiv:1402.3511}.

\bibitem[{Krause et~al.(2016)Krause, Lu, Murray, and
  Renals}]{krause2016multiplicative}
Ben Krause, Liang Lu, Iain Murray, and Steve Renals. 2016.
\newblock Multiplicative lstm for sequence modelling.
\newblock \emph{arXiv preprint arXiv:1609.07959}.

\bibitem[{Kuchaiev and Ginsburg(2017)}]{kuchaiev2017factorization}
Oleksii Kuchaiev and Boris Ginsburg. 2017.
\newblock Factorization tricks for lstm networks.
\newblock \emph{arXiv preprint arXiv:1703.10722}.

\bibitem[{Le et~al.(2015)Le, Jaitly, and Hinton}]{le2015simple}
Quoc~V Le, Navdeep Jaitly, and Geoffrey~E Hinton. 2015.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv preprint arXiv:1504.00941}.

\bibitem[{Li et~al.(2018)Li, Li, Cook, Zhu, and Gao}]{li2018independently}
Shuai Li, Wanqing Li, Chris Cook, Ce~Zhu, and Yanbo Gao. 2018.
\newblock Independently recurrent neural network (indrnn): Building a longer
  and deeper rnn.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5457--5466.

\bibitem[{Liu et~al.(2018)Liu, Simonyan, and Yang}]{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018.
\newblock Darts: Differentiable architecture search.
\newblock \emph{arXiv preprint arXiv:1806.09055}.

\bibitem[{LLC(2009)}]{mahoney2011large}
MultiMedia LLC. 2009.
\newblock Large text compression benchmark.

\bibitem[{Melis et~al.(2018)Melis, Blundell, Ko{\v{c}}isk{\`y}, Hermann, Dyer,
  and Blunsom}]{melis2018pushing}
G{\'a}bor Melis, Charles Blundell, Tom{\'a}{\v{s}} Ko{\v{c}}isk{\`y},
  Karl~Moritz Hermann, Chris Dyer, and Phil Blunsom. 2018.
\newblock Pushing the bounds of dropout.
\newblock \emph{arXiv preprint arXiv:1805.09208}.

\bibitem[{Merity et~al.(2017)Merity, Keskar, and
  Socher}]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher. 2017.
\newblock Regularizing and optimizing lstm language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}.

\bibitem[{Merity et~al.(2018)Merity, Keskar, and Socher}]{merity2018analysis}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher. 2018.
\newblock An analysis of neural language modeling at multiple scales.
\newblock \emph{arXiv preprint arXiv:1803.08240}.

\bibitem[{Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher}]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}.

\bibitem[{Mikolov et~al.(2014)Mikolov, Joulin, Chopra, Mathieu, and
  Ranzato}]{mikolov2014learning}
Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc'Aurelio
  Ranzato. 2014.
\newblock Learning longer memory in recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1412.7753}.

\bibitem[{Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget,
  {\v{C}}ernock{\`y}, and Khudanpur}]{mikolov2010recurrent}
Tom{\'a}{\v{s}} Mikolov, Martin Karafi{\'a}t, Luk{\'a}{\v{s}} Burget, Jan
  {\v{C}}ernock{\`y}, and Sanjeev Khudanpur. 2010.
\newblock Recurrent neural network based language model.
\newblock In \emph{Eleventh Annual Conference of the International Speech
  Communication Association}.

\bibitem[{Mikolov and Zweig(2012)}]{mikolov2012context}
Tomas Mikolov and Geoffrey Zweig. 2012.
\newblock Context dependent recurrent neural network language model.
\newblock \emph{SLT}, 12(234-239):8.

\bibitem[{Morin and Bengio(2005)}]{morin2005hierarchical}
Frederic Morin and Yoshua Bengio. 2005.
\newblock Hierarchical probabilistic neural network language model.
\newblock In \emph{Aistats}, volume~5, pages 246--252. Citeseer.

\bibitem[{Mujika et~al.(2017)Mujika, Meier, and Steger}]{mujika2017fast}
Asier Mujika, Florian Meier, and Angelika Steger. 2017.
\newblock Fast-slow recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5915--5924.

\bibitem[{Pascanu et~al.(2012)Pascanu, Mikolov, and
  Bengio}]{pascanu2012understanding}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2012.
\newblock Understanding the exploding gradient problem.
\newblock \emph{CoRR, abs/1211.5063}.

\bibitem[{Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer}]{peters2018deep}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer. 2018.
\newblock Deep contextualized word representations.
\newblock \emph{arXiv preprint arXiv:1802.05365}.

\bibitem[{Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean}]{pham2018efficient}
Hieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean. 2018.
\newblock Efficient neural architecture search via parameter sharing.
\newblock \emph{arXiv preprint arXiv:1802.03268}.

\bibitem[{Press and Wolf(2016)}]{press2016using}
Ofir Press and Lior Wolf. 2016.
\newblock Using the output embedding to improve language models.
\newblock \emph{arXiv preprint arXiv:1608.05859}.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever}]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{URL https://s3-us-west-2. amazonaws.
  com/openai-assets/research-covers/languageunsupervised/language understanding
  paper. pdf}.

\bibitem[{Rae et~al.(2018)Rae, Dyer, Dayan, and Lillicrap}]{rae2018fast}
Jack~W Rae, Chris Dyer, Peter Dayan, and Timothy~P Lillicrap. 2018.
\newblock Fast parametric learning with activation memorization.
\newblock \emph{arXiv preprint arXiv:1803.10049}.

\bibitem[{Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani}]{shaw2018self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
\newblock Self-attention with relative position representations.
\newblock \emph{arXiv preprint arXiv:1803.02155}.

\bibitem[{Shazeer et~al.(2018)Shazeer, Cheng, Parmar, Tran, Vaswani,
  Koanantakool, Hawkins, Lee, Hong, Young et~al.}]{shazeer2018mesh}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn
  Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,
  et~al. 2018.
\newblock Mesh-tensorflow: Deep learning for supercomputers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10434--10443.

\bibitem[{Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean}]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean. 2017.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}.

\bibitem[{Shazeer et~al.(2014)Shazeer, Pelemans, and Chelba}]{shazeer2014skip}
Noam Shazeer, Joris Pelemans, and Ciprian Chelba. 2014.
\newblock Skip-gram language modeling using sparse non-negative matrix
  probability estimation.
\newblock \emph{arXiv preprint arXiv:1412.1454}.

\bibitem[{Trinh et~al.(2018)Trinh, Dai, Luong, and Le}]{trinh2018learning}
Trieu~H Trinh, Andrew~M Dai, Thang Luong, and Quoc~V Le. 2018.
\newblock Learning longer-term dependencies in rnns with auxiliary losses.
\newblock \emph{arXiv preprint arXiv:1803.00144}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008.

\bibitem[{Wang and Cho(2015)}]{wang2015larger}
Tian Wang and Kyunghyun Cho. 2015.
\newblock Larger-context language modelling.
\newblock \emph{arXiv preprint arXiv:1511.03729}.

\bibitem[{Wang et~al.(2017)Wang, Gan, Wang, Shen, Huang, Ping, Satheesh, and
  Carin}]{wang2017topic}
Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev
  Satheesh, and Lawrence Carin. 2017.
\newblock Topic compositional neural language model.
\newblock \emph{arXiv preprint arXiv:1712.09783}.

\bibitem[{Weston et~al.(2014)Weston, Chopra, and Bordes}]{weston2014memory}
Jason Weston, Sumit Chopra, and Antoine Bordes. 2014.
\newblock Memory networks.
\newblock \emph{arXiv preprint arXiv:1410.3916}.

\bibitem[{Wu et~al.(2016)Wu, Zhang, Zhang, Bengio, and
  Salakhutdinov}]{wu2016multiplicative}
Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan~R
  Salakhutdinov. 2016.
\newblock On multiplicative integration with recurrent neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  2856--2864.

\bibitem[{Yang et~al.(2017)Yang, Dai, Salakhutdinov, and
  Cohen}]{yang2017breaking}
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W Cohen. 2017.
\newblock Breaking the softmax bottleneck: A high-rank rnn language model.
\newblock \emph{arXiv preprint arXiv:1711.03953}.

\bibitem[{Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals}]{zaremba2014recurrent}
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}.

\bibitem[{Zilly et~al.(2016)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber}]{zilly2016recurrent}
Julian~Georg Zilly, Rupesh~Kumar Srivastava, Jan Koutn{\'\i}k, and J{\"u}rgen
  Schmidhuber. 2016.
\newblock Recurrent highway networks.
\newblock \emph{arXiv preprint arXiv:1607.03474}.

\bibitem[{Zoph and Le(2016)}]{zoph2016neural}
Barret Zoph and Quoc~V Le. 2016.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.01578}.

\end{thebibliography}
