\begin{abstract}

Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling.
% As a solution,
We propose a novel neural architecture \textit{Transformer-XL} that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.
Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning).
When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens.
Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch\footnote{\url{https://github.com/kimiyoung/transformer-xl}}.

% Additionally, we improve the state-of-the-art (SoTA) results of bpc/perplexity from 1.06 to 0.99 on enwiki8, from 1.13 to 1.08 on text8, from 20.5 to 18.3 on WikiText-103, from 23.7 to 21.8 on One Billion Word, and from 55.3 to 54.5 on Penn Treebank (without finetuning).





% We propose a novel neural architecture, \textit{Transformer-XL}, to fully exploit the power of self-attention in language modeling.
% Different from previous works that rely on fixed-length contexts to perform training, we introduce a notion of recurrence by enabling Transformer to reuse the historical representations without disrupting the temporal coherence.
% The recurrence not only offers the ability of capturing longer-term dependency, but also resolves the problem of context fragmentation.
% Empirically, we significantly advance state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including from 1.06 to 0.99 in bpc on enwiki8, from 1.13 to 1.08 in bpc on text8, from 29.9 to 18.3 in perplexity on WikiText-103, and from 28.0 to 21.7 in perplexity on One Billion Word.
% On datasets requiring longer-term dependency, the performance of Transformer-XL improves when the attention length increases during evaluation, with best models attending to up to 1,600 words and 3,800 characters.
% We quantify the effective context length with a newly devised metric, and show that Transformer-XL manages to model dependency that is about 80\% longer than recurrent networks and 450\% longer than standard Transformer.
% Moreover, for One Billions Word with only short-range dependency, Transformer-XL retains a performance advantage.


%We propose a novel neural architecture, \textit{Transformer-XL}, for better modeling longer-term dependency. To address the limitation of fixed-length contexts, we introduce a notion of recurrence by reusing the representations from the history. Empirically, we show state-of-the-art (SoTA) results on both word-level and character-level language modeling datasets, including WikiText-103, One Billion Word, Penn Treebank, text8 and enwik8.
%Notably, we improve the SoTA results from 1.06 to 0.99 in bpc on enwiki8, from 1.13 to 1.08 in bpc on text8, from 33.0 to 18.3 in perplexity on WikiText-103, and from 28.0 to 21.7 in perplexity on One Billion Word. Performance improves when the attention length increases during evaluation, and our best model attends to up to 1,600 words and 3,800 characters. To quantify the effective length of dependency, we devise a new metric and show that on WikiText-103 Transformer-XL manages to model dependency that is about 80\% longer than recurrent networks and 450\% longer than Transformer. Moreover, Transformer-XL is up to 1,800+ times faster than vanilla Transformer during evaluation.
\end{abstract}