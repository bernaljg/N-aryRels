\section{Conclusions}

% We propose a novel architecture, Transformer-XL, for language modeling with self-attention architectures beyond a fixed-length context. 
Transformer-XL obtains strong perplexity results, models longer-term dependency than RNNs and Transformer, achieves substantial speedup during evaluation, and is able to generate coherent text articles. We envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling.

