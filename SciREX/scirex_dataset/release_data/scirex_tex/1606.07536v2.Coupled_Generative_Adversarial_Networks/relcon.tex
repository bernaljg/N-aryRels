\section{Related Work}\label{sec::rel}

Neural generative models has recently received an increasing amount of attention. Several approaches, including generative adversarial networks\cite{goodfellow2014generative}, variational autoencoders (VAE)\cite{kingma2013auto}, attention models\cite{gregor2015draw}, moment matching\cite{li2015generative}, stochastic back-propagation\cite{rezende2014stochastic}, and diffusion processes\cite{sohl2015deep}, have shown that a deep network can learn an image distribution from samples. The learned networks can be used to generate novel images. Our work was built on~\cite{goodfellow2014generative}. However, we studied a different problem, the problem of learning a {\it joint} distribution of multi-domain images. We were interested in whether a joint distribution of images in different domains can be learned from samples drawn separately from its marginal distributions of the individual domains. We showed its achievable via the proposed CoGAN framework. Note that our work is different to the Attribute2Image work\cite{yan2015attribute2image}, which is based on a conditional VAE model~\cite{kingma2014semi}. The conditional model can be used to generate images of different styles, but they are unsuitable for generating images in two different domains such as color and depth image domains.

Following~\cite{goodfellow2014generative}, several works improved the image generation quality of GAN, including a Laplacian pyramid implementation\cite{denton2015deep}, a deeper architecture\cite{radford2015unsupervised}, and conditional models\cite{mirza2014conditional}. Our work extended GAN to dealing with joint distributions of images.

Our work is related to the prior works in multi-modal learning, including joint embedding space learning~\cite{kiros2014unifying} and multi-modal Boltzmann machines~\cite{srivastava2012multimodal,ngiam2011multimodal}. These approaches can be used for generating corresponding samples in different domains only when correspondence annotations are given during training. The same limitation is also applied to dictionary learning-based approaches~\cite{wang2012semi,yang2010image}. Our work is also related to the prior works in cross-domain image generation~\cite{yim2015rotating,reed2015deep,dosovitskiy2015learning}, which studied transforming an image in one style to the corresponding images in another style. However, we focus on learning the joint distribution in an unsupervised fashion, while~\cite{yim2015rotating,reed2015deep,dosovitskiy2015learning} focus on learning a transformation function directly in a supervised fashion.

\section{Conclusion}\label{sec::conc}

We presented the CoGAN framework for learning a joint distribution of multi-domain images. We showed that via enforcing a simple weight-sharing constraint to the layers that are responsible for decoding abstract semantics, the CoGAN learned the joint distribution of images by just using samples drawn separately from the marginal distributions. In addition to convincing image generation results on faces and RGBD images, we also showed promising results of the CoGAN framework for the image transformation and unsupervised domain adaptation tasks.
