The approach presented in Section~\ref{sec:method} is far from the first we tried. In this section we describe a number of alternative approaches that will be used as baselines in Section~\ref{sec:experiments}.

\mypara{GAN and semantic segmentation.}
Our first baseline is consistent with current trends in the research community. It combines a GAN with a semantic segmentation objective. The generator is trained to synthesize an image that fools the discriminator~\cite{Goodfellow2014}. An additional term in the loss specifies that when the synthesized image is given as input to a pretrained semantic segmentation network, it should produce a label map that is as close to the input layout $L$ as possible. The GAN setup follows the work of Radford et al.~\cite{Radford2016}. The input to the generator is the semantic layout $L$. For the semantic segmentation network, we use publicly available networks that were pretrained for the Cityscapes dataset~\cite{YuKoltun2016} and the NYU dataset~\cite{Long2015}. The training objective combines the GAN loss and the semantic segmentation (pixelwise cross-entropy) loss.

%For image synthesis conditioned on a semantic layout, one of the most natural ideas is to combine semantic segmentation and GANs. We could use GAN to regularize the synthesized image so that it looks natural and use semantic segmentation to make sure the output image conforms to the given layout. We combine the GAN architecture \cite{Radford2016} and semantic segmentation network \cite{YuKoltun2016} for this task. The training loss for the generator is a weighted sum of cross entropy loss in semantic segmentation and the discriminator loss in GAN. The discriminator tries to discriminates the synthesized images and the natural images.

\mypara{Full-resolution network.}
Our second baseline is a feedforward convolutional network that operates at full resolution. This baseline uses the same loss as the CRN described in Section~\ref{sec:method}. The only difference is the network architecture. In particular, we have experimented with variants of the multi-scale context aggregation network~\cite{YuKoltun2016}. An appealing property of this network is that it retains high resolution in the intermediate layers, which we hypothesized to be helpful for photorealistic image synthesis. The original architecture described in~\cite{YuKoltun2016} did not yield good results and is not well-suited to our problem, because the input semantic layouts are piecewise constant and the network of~\cite{YuKoltun2016} begins with a small receptive field. We obtained much better results with the inverse architecture: start with large dilation and decrease it by a factor of 2 in each layer. This can be viewed as a full-resolution counterpart to the CRN, based on dilating the filters instead of scaling the feature maps. One of the drawbacks of this approach is that all intermediate feature layers are at full image resolution and have a high memory footprint. Thus the ratio of capacity (number of parameters) to memory footprint is much lower than in the CRN. This high memory footprint of intermediate layers also constrains the resolution to which this approach can scale: with 10 layers and 256 feature maps per layer, the maximal resolution that could be trained with available GPU memory is $256\timess 512$.

%As an alternative architecture to our CRN model, we can use the context aggregation network \cite{YuKoltun2016}. We invert the original dilation module and extend it to have dilation rates in a decreasing order from 128 exponentially down to 1, which allows aggregating nonlocal information early in the network. Having dilation rates in an increasing order is not efficient because the semantic layout is piecewise constant, so that the filter responses are likely the same and redudant in the first few layers of the network.  We use 256 feature channels for this task. This network is only able to synthesize images up to 256p because all the layers in the full resolution.

\mypara{Encoder-decoder.}
Our third baseline is an encoder-decoder network, the u-net~\cite{Ronneberger2015}. This network is also trained with the same loss as the CRN. It is thus an additional baseline that evaluates the effect of using the CRN versus a different architecture, when everything else (loss, training procedure) is held fixed.% On the Cityscapes dataset, the \mbox{Ours $>$ Baseline} rate relative to the encoder-decoder baseline is 78.3\%. (Compare to corresponding results in Table 1 in the paper.) On the NYU dataset, the \mbox{Ours $>$ Baseline} rate relative to the encoder-decoder baseline is 71.2\%. (Compare to corresponding results in Table 2 in the paper.) Overall, the images synthesized by the encoder-decoder baseline are more realistic than the images synthesized by the approach of Isola et al., but less realistic than the images synthesized by the full-resolution network baseline. Images synthesized by all five baselines are less realistic than images synthesized by the CRN, on both datasets.

\mypara{Image-space loss.}
Our next baseline controls for the feature matching loss used to train the CRN. Here we use exactly the same architecture as in Section~\ref{sec:method}, but use only the first layer $\Phi_0$ (image color) in the loss:
\begin{equation}
\lL_{I,L}(\theta) = \sum_l{\lambda_l\| I-g(L;\theta)\|_1}.
\label{eq:loss-color}
\end{equation}

%As an alternative loss for the content loss, we investigate the performance of $L_1$ loss in the color space. We train the a baseline model using the same architecture of CRN but with $L_1$ color loss:
%$$
%\lL(I,L;\theta)=\sum_l{\lambda_l\| I-g(L;\theta)\|_1}.
%$$

\mypara{Image-to-image translation.}
Our last baseline is the contemporaneous approach of Isola et al., the implementation and results of which are publicly available~\cite{Isola2017}. This approach uses a conditional GAN and is representative of the dominant stream of research in image synthesis. The generator is an encoder-decoder~\cite{Ronneberger2015}. The GAN setup is derived from the work of Radford et al.~\cite{Radford2016}.
%Image-to-image translation is a general-purpose approach to map one type of image to another type. The loss function is learned automatically from the data based on a conditioned generative adversarial network. It uses a U-Net architecture for the generator. However, such a general approach fails to capture consistent structure for image synthesis, as demonstrated in the experiments.
