\section{Related work}
In language modeling, \citet{hutchinson2011low,hutchinson2012sparse} have previously considered the problem from a matrix rank perspective. However, their focus was to improve the generalization of Ngram language models via a sparse plus low-rank approximation.
By contrast, as neural language models already generalize well, we focus on a high-rank neural language model that improves expressiveness without sacrificing generalization. 
\citet{neubig2016generalizing} proposed to mix Ngram and neural language models to unify and benefit from both.
However, this mixture might not generalize well since an Ngram model, which has poor generalization, is included. Moreover, the fact that the two components are separately trained can limit its expressiveness. \citet{levy2014neural} also considered the matrix factorization perspective, but in the context of learning word embeddings.

In a general sense, Mixture of Softmaxes proposed in this work can be seen as a particular instantiation of the long-existing idea called Mixture of Experts (MoE)~\citep{jacobs1991adaptive}.
However, there are two core differences. Firstly, MoE has usually been instantiated as mixture of Gaussians to model data in continuous domains~\citep{jacobs1991adaptive,graves2013generating,bazzani2016recurrent}. More importantly, the motivation of using the mixture structure is distinct. For Gaussian mixture models, the mixture structure is employed to allow for a parameterized multi-modal distribution. By contrast, Softmax by itself can parameterize a multi-modal distribution, and MoS is introduced to break the Softmax bottleneck as discussed in Section \ref{sec:rank}. 

There has been previous work~\citep{eigen2013learning,shazeer2017outrageously} proposing architectures that can be categorized as instantiations of MoC, since the mixture structure is employed in the feature space.\footnote{Although \citet{shazeer2017outrageously} name their architecture as MoE, it is not a standard MoE~\citep{jacobs1991adaptive} and should be classified as MoC under our terminology.}
The target of \citet{eigen2013learning} is to create a more expressive feed-forward layer through the mixture structure. In comparison, \citet{shazeer2017outrageously} focuses on a sparse gating mechanism also on the feature level, which enables efficient conditional computation and allows the training of a very large neural architecture. 
%In a more general sense, MoC can be viewed as a Product of Experts (PoE)~\citep{hinton2006training} geometrically weighted by the mixture weights.
In addition to having different motivations from our work, all these MoC variants suffer from the same rank limitation problem as discussed in Section \ref{sec:rank}. 

Finally, several previous works have tried to introduce latent variables into sequence modeling~\citep{bayer2014learning,gregor2015draw,chung2015recurrent,gan2015deep,fraccaro2016sequential,chung2016hierarchical}. Except for \citep{chung2016hierarchical}, these structures all define a continuous latent variable for each step of the RNN computation, and rely on the SGVB estimator~\citep{kingma2013auto} to optimize a variational lower bound of the log-likelihood.
Since exact integration is infeasible, these models cannot estimate the likelihood (perplexity) exactly at test time. Moreover, for discrete data, the variational lower bound is usually too loose to yield a competitive approximation compared to standard auto-regressive models. As an exception, \citet{chung2016hierarchical} utilizes Bernoulli latent variables to model the hierarchical structure in language, where the Bernoulli sampling is replaced by a thresholding operation at test time to give perplexity estimation.

