\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
 % \usepackage{hyperref}
\usepackage{url}

\usepackage[T1]{fontenc}  
\usepackage[utf8]{inputenc}             
 % \usepackage[english,french]{babel}  
\usepackage[english]{babel} 

\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{enumitem}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Breaking the Softmax Bottleneck: \\ A High-Rank RNN Language Model}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Zhilin Yang\thanks{Equal contribution. Ordering determined by dice rolling.}~~, Zihang Dai\samethanks~~, Ruslan Salakhutdinov, William W. Cohen\\
School of Computer Science\\
Carnegie Mellon University\\
\texttt{\{zhiliny,dzihang,rsalakhu,wcohen\}@cs.cmu.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

% table cell
\usepackage{array}
\newcolumntype{C}{>{\centering\arraybackslash}p{0.15\linewidth}}

% table cell length
\newlength\contextlength 
\setlength\contextlength{\linewidth}

% special token
\newcommand{\blank}{\_\_?\_\_}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{coro}{Corollary}
\newtheorem{property}{Property}

\begin{document}


\maketitle

\begin{abstract}
We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a {\em Softmax bottleneck}. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.\footnote{Code is available at \url{https://github.com/zihangdai/mos}.}
\end{abstract}

\input{intro}
\input{rank}
\input{exp}
\input{related}
\input{conc}



\subsubsection*{Acknowledgments}
This work was supported by the DARPA award D17AP00001, the Google focused award, and the Nvidia NVAIL award.

%
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.

\clearpage

\bibliography{iclr2018_conference}
\bibliographystyle{iclr2018_conference}

\clearpage
\input{appendix}

\end{document}
