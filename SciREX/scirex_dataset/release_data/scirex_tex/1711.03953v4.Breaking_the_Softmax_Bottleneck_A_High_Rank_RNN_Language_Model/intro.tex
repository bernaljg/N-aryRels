\section{Introduction} \label{sec:intro}

As a fundamental task in natural language processing, statistical language modeling has gone through significant development from traditional Ngram language models to neural language models in the last decade~\citep{bengio2003neural,mnih2007three,mikolov2010recurrent}. Despite the huge variety of models, as a density estimation problem, language modeling mostly relies on a universal auto-regressive factorization of the joint probability and then models each conditional factor using different approaches.
Specifically, given a corpus of tokens $\mathbf{X} = (X_1, \dots, X_T)$, the joint probability $P(\mathbf{X})$ factorizes as
$ P(\mathbf{X}) = \prod_{t} P(X_t \mid X_{<t}) = \prod_{t} P(X_t \mid C_t), $
where $C_t = X_{<t}$ is referred to as the \textit{context} of the conditional probability hereafter. 

Based on the factorization, recurrent neural networks (RNN) based language models achieve state-of-the-art results on various benchmarks~\citep{merity2017regularizing,melis2017state,krause2017dynamic}. A standard approach is to use a recurrent network to encode the context into a fixed size vector, which is then multiplied by the word embeddings~\citep{inan2016tying,press2017using} using dot product to obtain the logits. The logits are consumed by the Softmax function to give a categorical probability distribution over the next token.
In spite of the expressiveness of RNNs as universal approximators \citep{schafer2006recurrent}, an unclear question is whether the combination of dot product and Softmax is capable of modeling the conditional probability, which can vary dramatically with the change of the context.

% However, natural language is highly context-dependent \citep{mikolov2012context}. As a result, simply using a Softmax operation to represent the probability distribution can be unrealistic.

In this work, we study the expressiveness of the aforementioned Softmax-based recurrent language models from a perspective of matrix factorization. We show that learning a Softmax-based recurrent language model with the standard formulation is essentially equivalent to solving a matrix factorization problem. More importantly, due to the fact that natural language is highly context-dependent, the matrix to be factorized can be high-rank. This further implies that standard Softmax-based language models with distributed (output) word embeddings do not have enough capacity to model natural language. We call this the {\em Softmax bottleneck}.

We propose a simple and effective method to address the Softmax bottleneck. Specifically, we introduce discrete latent variables into a recurrent language model, and formulate the next-token probability distribution as a {\em Mixture of Softmaxes} (MoS). Mixture of Softmaxes is more expressive than Softmax and other surrogates considered in prior work. Moreover, we show that MoS learns matrices that have much larger normalized singular values and thus much higher rank than Softmax and other baselines on real-world datasets.

We evaluate our proposed approach on standard language modeling benchmarks. MoS substantially improves over the current state-of-the-art results on benchmarks, by up to 3.6 points in terms of perplexity, reaching perplexities 47.69 on Penn Treebank and 40.68 on WikiText-2. We further apply MoS to a dialog dataset and show improved performance over Softmax and other baselines.

Our contribution is two-fold. First, we identify the Softmax bottleneck by formulating language modeling as a matrix factorization problem. Second, we propose a simple and effective method that substantially improves over the current state-of-the-art results.
