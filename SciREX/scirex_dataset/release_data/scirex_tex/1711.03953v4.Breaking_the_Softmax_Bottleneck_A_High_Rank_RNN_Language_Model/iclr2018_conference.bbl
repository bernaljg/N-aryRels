\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bayer \& Osendorfer(2014)Bayer and Osendorfer]{bayer2014learning}
Justin Bayer and Christian Osendorfer.
\newblock Learning stochastic recurrent networks.
\newblock \emph{arXiv preprint arXiv:1411.7610}, 2014.

\bibitem[Bazzani et~al.(2016)Bazzani, Larochelle, and
  Torresani]{bazzani2016recurrent}
Loris Bazzani, Hugo Larochelle, and Lorenzo Torresani.
\newblock Recurrent mixture density network for spatiotemporal visual
  attention.
\newblock \emph{arXiv preprint arXiv:1603.08199}, 2016.

\bibitem[Bengio et~al.(2003)Bengio, Ducharme, Vincent, and
  Jauvin]{bengio2003neural}
Yoshua Bengio, R{\'e}jean Ducharme, Pascal Vincent, and Christian Jauvin.
\newblock A neural probabilistic language model.
\newblock \emph{Journal of machine learning research}, 3\penalty0
  (Feb):\penalty0 1137--1155, 2003.

\bibitem[Blei et~al.(2003)Blei, Ng, and Jordan]{blei2003latent}
David~M Blei, Andrew~Y Ng, and Michael~I Jordan.
\newblock Latent dirichlet allocation.
\newblock \emph{Journal of machine Learning research}, 3\penalty0
  (Jan):\penalty0 993--1022, 2003.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{chelba2013one}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv preprint arXiv:1312.3005}, 2013.

\bibitem[Chung et~al.(2015)Chung, Kastner, Dinh, Goel, Courville, and
  Bengio]{chung2015recurrent}
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron~C Courville,
  and Yoshua Bengio.
\newblock A recurrent latent variable model for sequential data.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2980--2988, 2015.

\bibitem[Chung et~al.(2016)Chung, Ahn, and Bengio]{chung2016hierarchical}
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1609.01704}, 2016.

\bibitem[Eigen et~al.(2013)Eigen, Ranzato, and Sutskever]{eigen2013learning}
David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever.
\newblock Learning factored representations in a deep mixture of experts.
\newblock \emph{arXiv preprint arXiv:1312.4314}, 2013.

\bibitem[Fraccaro et~al.(2016)Fraccaro, S{\o}nderby, Paquet, and
  Winther]{fraccaro2016sequential}
Marco Fraccaro, S{\o}ren~Kaae S{\o}nderby, Ulrich Paquet, and Ole Winther.
\newblock Sequential neural models with stochastic layers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2199--2207, 2016.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016theoretically}
Yarin Gal and Zoubin Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1019--1027, 2016.

\bibitem[Gan et~al.(2015)Gan, Li, Henao, Carlson, and Carin]{gan2015deep}
Zhe Gan, Chunyuan Li, Ricardo Henao, David~E Carlson, and Lawrence Carin.
\newblock Deep temporal sigmoid belief networks for sequence modeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2467--2475, 2015.

\bibitem[Godfrey \& Holliman(1997)Godfrey and Holliman]{godfrey1997switchboard}
John~J Godfrey and Edward Holliman.
\newblock Switchboard-1 release 2.
\newblock \emph{Linguistic Data Consortium, Philadelphia}, 1997.

\bibitem[Grave et~al.(2016)Grave, Joulin, and Usunier]{grave2016improving}
Edouard Grave, Armand Joulin, and Nicolas Usunier.
\newblock Improving neural language models with a continuous cache.
\newblock \emph{arXiv preprint arXiv:1612.04426}, 2016.

\bibitem[Graves(2013)]{graves2013generating}
Alex Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1308.0850}, 2013.

\bibitem[Gregor et~al.(2015)Gregor, Danihelka, Graves, Rezende, and
  Wierstra]{gregor2015draw}
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo~Jimenez Rezende, and Daan
  Wierstra.
\newblock Draw: A recurrent neural network for image generation.
\newblock \emph{arXiv preprint arXiv:1502.04623}, 2015.

\bibitem[Hutchinson et~al.(2011)Hutchinson, Ostendorf, and
  Fazel]{hutchinson2011low}
Brian Hutchinson, Mari Ostendorf, and Maryam Fazel.
\newblock Low rank language models for small training sets.
\newblock \emph{IEEE Signal Processing Letters}, 18\penalty0 (9):\penalty0
  489--492, 2011.

\bibitem[Hutchinson et~al.(2012)Hutchinson, Ostendorf, and
  Fazel]{hutchinson2012sparse}
Brian Hutchinson, Mari Ostendorf, and Maryam Fazel.
\newblock A sparse plus low rank maximum entropy language model.
\newblock In \emph{INTERSPEECH}, pp.\  1676--1679, 2012.

\bibitem[Inan et~al.(2016)Inan, Khosravi, and Socher]{inan2016tying}
Hakan Inan, Khashayar Khosravi, and Richard Socher.
\newblock Tying word vectors and word classifiers: A loss framework for
  language modeling.
\newblock \emph{arXiv preprint arXiv:1611.01462}, 2016.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and
  Hinton]{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 79--87, 1991.

\bibitem[Kim et~al.(2016)Kim, Jernite, Sontag, and Rush]{kim2016character}
Yoon Kim, Yacine Jernite, David Sontag, and Alexander~M Rush.
\newblock Character-aware neural language models.
\newblock In \emph{AAAI}, pp.\  2741--2749, 2016.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kneser \& Ney(1995)Kneser and Ney]{kneser1995improved}
Reinhard Kneser and Hermann Ney.
\newblock Improved backing-off for m-gram language modeling.
\newblock In \emph{Acoustics, Speech, and Signal Processing, 1995. ICASSP-95.,
  1995 International Conference on}, volume~1, pp.\  181--184. IEEE, 1995.

\bibitem[Krause et~al.(2017)Krause, Kahembwe, Murray, and
  Renals]{krause2017dynamic}
Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals.
\newblock Dynamic evaluation of neural sequence models.
\newblock \emph{arXiv preprint arXiv:1709.07432}, 2017.

\bibitem[Levy \& Goldberg(2014)Levy and Goldberg]{levy2014neural}
Omer Levy and Yoav Goldberg.
\newblock Neural word embedding as implicit matrix factorization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2177--2185, 2014.

\bibitem[Mahoney(2011)]{mahoney2011large}
Matt Mahoney.
\newblock Large text compression benchmark, 2011.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017state}
G{\'a}bor Melis, Chris Dyer, and Phil Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Merity et~al.(2017)Merity, Keskar, and Socher]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing lstm language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017.

\bibitem[Mikolov \& Zweig(2012)Mikolov and Zweig]{mikolov2012context}
Tomas Mikolov and Geoffrey Zweig.
\newblock Context dependent recurrent neural network language model.
\newblock \emph{SLT}, 12:\penalty0 234--239, 2012.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and
  Khudanpur]{mikolov2010recurrent}
Tomas Mikolov, Martin Karafi{\'a}t, Lukas Burget, Jan Cernock{\`y}, and Sanjeev
  Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Interspeech}, volume~2, pp.\ ~3, 2010.

\bibitem[Mikolov et~al.(2012)Mikolov, Sutskever, Deoras, Le, Kombrink, and
  Cernocky]{mikolov2012subword}
Tom{\'a}{\v{s}} Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan
  Kombrink, and Jan Cernocky.
\newblock Subword language modeling with neural networks.
\newblock \emph{preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char.
  pdf)}, 2012.

\bibitem[Mnih \& Hinton(2007)Mnih and Hinton]{mnih2007three}
Andriy Mnih and Geoffrey Hinton.
\newblock Three new graphical models for statistical language modelling.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  641--648. ACM, 2007.

\bibitem[Neubig \& Dyer(2016)Neubig and Dyer]{neubig2016generalizing}
Graham Neubig and Chris Dyer.
\newblock Generalizing and hybridizing count-based and neural language models.
\newblock \emph{arXiv preprint arXiv:1606.00499}, 2016.

\bibitem[Pinker(1994)]{pinker1994language}
Steven Pinker.
\newblock The language instinct, 1994.

\bibitem[Press \& Wolf(2017)Press and Wolf]{press2017using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock In \emph{EACL}, 2017.

\bibitem[Press(2007)]{press2007numerical}
William~H Press.
\newblock \emph{Numerical recipes 3rd edition: The art of scientific
  computing}.
\newblock Cambridge university press, 2007.

\bibitem[Sch{\"a}fer \& Zimmermann(2006)Sch{\"a}fer and
  Zimmermann]{schafer2006recurrent}
Anton~Maximilian Sch{\"a}fer and Hans~Georg Zimmermann.
\newblock Recurrent neural networks are universal approximators.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  632--640. Springer, 2006.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3104--3112, 2014.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Cun, and
  Fergus]{wan2013regularization}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann~L Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{Proceedings of the 30th international conference on machine
  learning (ICML-13)}, pp.\  1058--1066, 2013.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\bibitem[Zhao et~al.(2017)Zhao, Zhao, and Eskenazi]{zhao2017learning}
Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
\newblock Learning discourse-level diversity for neural dialog models using
  conditional variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1703.10960}, 2017.

\bibitem[Zilly et~al.(2016)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber]{zilly2016recurrent}
Julian~Georg Zilly, Rupesh~Kumar Srivastava, Jan Koutn{\'\i}k, and J{\"u}rgen
  Schmidhuber.
\newblock Recurrent highway networks.
\newblock \emph{arXiv preprint arXiv:1607.03474}, 2016.

\bibitem[Zoph \& Le(2016)Zoph and Le]{zoph2016neural}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.01578}, 2016.

\end{thebibliography}
