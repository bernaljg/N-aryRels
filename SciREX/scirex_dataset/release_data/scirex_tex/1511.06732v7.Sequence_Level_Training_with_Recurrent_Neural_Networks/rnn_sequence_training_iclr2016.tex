\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
%\usepackage[outdir=./]{epstopdf}
\usepackage[cmex10]{amsmath}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{stfloats}
\hyphenation{net-works Boltz-mann}
\usepackage{times}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage[]{algorithm2e}
\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
%\usepackage{capt-of}
%\usepackage[normalem]{ulem}
% \usepackage[all=normal,floats=tight,paragraphs=tight,indent=tight]{savetrees}
% \usepackage[subtle]{savetrees}

% \renewcommand{\baselinestretch}{0.97}

\newcommand{\argmax}{\arg\!\max}
\definecolor{green}{rgb}{0.2,0.8,0.2}
\definecolor{blue}{rgb}{0.2,0.2,0.8}
\usepackage{xspace}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand*{\aka}{a.k.a.\@\xspace}
\makeatletter
\newcommand*{\etc}{%
    \@ifnextchar{.}%
        {etc}%
        {etc.\@\xspace}%
}


\title{Sequence Level Training with\\Recurrent Neural Networks}


\author{Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba \\
Facebook AI Research\\
\texttt{\{ranzato, spchopra, michealauli, wojciech\}@fb.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\btheta}{\mathbf{\theta}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\setr}{\mathcal{R}}
\newcommand{\mamark}[1]{\textcolor{orange}{{#1}}}
\newcommand{\spcmark}[1]{\textcolor{red}{{#1}}}
\newcommand{\macomment}[1]{\marginpar{\begin{center}\textcolor{orange}{#1}\end{center}}}
\iclrfinalcopy % Uncomment for camera-ready version



\begin{document}
\maketitle

%\baselinestretch{0.95}

\begin{abstract}
Many natural language processing applications use language models to generate text.
These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. 
However, at test time the model is expected to generate the entire sequence from scratch. 
This discrepancy makes generation brittle, as errors may accumulate along the way. 
We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE.
On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search,
 while being several times faster. 
\end{abstract}

\input{introduction.tex}
\input{model.tex}
\input{experiments.tex}
\input{conclusions.tex}

\subsubsection*{Acknowledgments}
The authors would like to thank David Grangier, Tomas Mikolov, Leon Bottou, Ronan Collobert and Laurens van der Maaten for their insightful comments. 
We also would like to thank Alexander M. Rush for his help in preparing the data set for the summarization task and Sam Gross for providing the image features. 

\bibliography{iclr2016_conference}
\bibliographystyle{iclr2016_conference}
\newpage
\input{sup_material.tex}

\end{document}