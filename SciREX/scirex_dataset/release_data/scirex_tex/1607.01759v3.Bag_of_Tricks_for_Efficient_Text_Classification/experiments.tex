\begin{table*}[t]
\centering
\small
\begin{tabular}{@{\hspace{3pt}}l@{\hspace{3pt}}ccccccccc}
\toprule
Model && AG & Sogou & DBP & Yelp P. & Yelp F. & Yah. A. & Amz. F. & Amz. P. \\
\midrule
BoW~\cite{zhang2015character}          && 88.8 & 92.9 & 96.6 & 92.2 & 58.0 & 68.9 & 54.6 & 90.4 \\
ngrams~\cite{zhang2015character}       && 92.0 & 97.1 & 98.6 & 95.6 & 56.3 & 68.5 & 54.3 & 92.0 \\
ngrams TFIDF~\cite{zhang2015character} && 92.4 & 97.2 & 98.7 & 95.4 & 54.8 & 68.5 & 52.4 & 91.5 \\
char-CNN~\cite{zhang2015text}          && 87.2 & 95.1 & 98.3 & 94.7 & 62.0 & 71.2 & 59.5 & 94.5 \\
char-CRNN~\cite{xiao2016efficient}     && 91.4 & 95.2 & 98.6 & 94.5 & 61.8 & 71.7 & 59.2 & 94.1 \\
VDCNN~\cite{conneau2016}               && 91.3 & 96.8 & 98.7 & 95.7 & 64.7 & 73.4 & 63.0 & 95.7 \\
\midrule
%Ours, $h=100$                         && 91.0 & 92.6 & 98.2 & 92.9 & 59.6 & 70.7 & 55.3 & 90.9 \\
%Ours, $h=100$, ReLU                   && 92.4 & 94.5 & 98.4 & 94.0 & 60.3 & 71.0 & 57.2 & 92.7 \\ 
%Ours, $h=100$, ReLU, bigram           && 92.8 & 96.4 & 98.7 & 95.6 & 62.6 & 71.5 & 58.2 & 94.4 \\
%Ours, $h=100$, bigram                 && 92.4 & 96.4 & 98.5 & 95.7 & 63.7 & 71.9 & 59.2 & 94.5 \\
%Ours, $h=10$, bigram, ReLU            && 92.5 & 95.9 & 98.6 & 95.3 & 63.0 & 69.2 & 60.2 & 94.5 \\
\texttt{fastText}, $h=10$             && 91.5 & 93.9 & 98.1 & 93.8 & 60.4 & 72.0 & 55.8 & 91.2 \\
\texttt{fastText}, $h=10$, bigram     && 92.5 & 96.8 & 98.6 & 95.7 & 63.9 & 72.3 & 60.2 & 94.6 \\
\bottomrule
\end{tabular}
\caption{Test accuracy [\%] on sentiment datasets.
\texttt{FastText} has been run with the same parameters for all the datasets. 
It has $10$ hidden units and we evaluate it with and without bigrams.
For char-CNN, we show the best reported numbers without data augmentation. 
}\label{tab:sent_res}
\end{table*}

\begin{table*}[t]
\centering
\small
\begin{tabular}{@{\hspace{3pt}}cc@{\hspace{3pt}}cccccccc}
\toprule
&& \multicolumn{2}{c}{\newcite{zhang2015text}} && \multicolumn{3}{c}{\newcite{conneau2016}} && \texttt{fastText} \\ %\multicolumn{2}{|c|}{Ours with ReLU+bigram} \\
\cmidrule(l){3-4}\cmidrule(l){6-8}
&& small char-CNN & big char-CNN && depth=9 & depth=17 & depth=29 && $h=10$, bigram \\ %& $h=100$ \\
\midrule
AG      && 1h & 3h && 24m  & 37m  & 51m  && 1s  \\%& 19s \\
Sogou   && - & -   && 25m  & 41m  & 56m  && 7s \\%& 2m29 \\
DBpedia && 2h & 5h && 27m  & 44m  & 1h   && 2s  \\%& 41s \\
Yelp P. && - & -   && 28m  & 43m  & 1h09 && 3s \\%& 1m9 \\
Yelp F. && - & -   && 29m  & 45m  & 1h12 && 4s \\%& 1m23 \\
Yah. A. && 8h & 1d && 1h   & 1h33 & 2h   && 5s \\%& 1m52 \\
Amz. F. && 2d & 5d && 2h45 & 4h20 & 7h   && 9s \\%& 3m18 \\
Amz. P. && 2d & 5d && 2h45 & 4h25 & 7h   && 10s \\%& 3m46 \\
\bottomrule
\end{tabular}
\caption{Training time for a single epoch on sentiment analysis datasets compared to char-CNN and VDCNN.
  }\label{tab:sent_speed}
\end{table*}

\section{Experiments}

We evaluate~\texttt{fastText} on two different tasks.  First, we compare it to
existing text classifers on the problem of sentiment analysis.  Then, we
evaluate its capacity to scale to large output space on a tag prediction
dataset.  Note that our model could be implemented with the Vowpal Wabbit
library,\footnote{Using the options \texttt{--nn}, \texttt{--ngrams} and \texttt{--log\_multi}}
but we observe in practice, that our tailored implementation is at
least~2-5$\times$ faster.

\subsection{Sentiment analysis}

\paragraph{Datasets and baselines.}
We employ the same~8 datasets and evaluation protocol of~\newcite{zhang2015character}.
We report the n-grams and~TFIDF baselines from~\newcite{zhang2015character}, as
well as the character level convolutional model~(char-CNN)
of~\newcite{zhang2015text}, the character based convolution recurrent
network~(char-CRNN) of~\cite{xiao2016efficient} and the very deep convolutional
network~(VDCNN) of~\newcite{conneau2016}.
We also compare to~\newcite{tang2015document} following their evaluation protocol.
We report their main baselines as well as their two approaches based on recurrent
networks~(Conv-GRNN and LSTM-GRNN). 

\paragraph{Results.}
We present the results in~Figure~\ref{tab:sent_res}.
We use~10 hidden units and run~\texttt{fastText} for~5 epochs with a learning rate selected
on a validation set from~$\{$0.05,~0.1,~0.25,~0.5$\}$.
On this task, adding bigram information improves the performance by~1-4$\%$. 
Overall our accuracy is slightly better than char-CNN and char-CRNN and, a bit worse than~VDCNN.
Note that we can increase the accuracy slightly by using more n-grams, for example
with trigrams, the performance on Sogou goes up to~97.1$\%$. 
Finally, Figure~\ref{tab:tang_res} shows that our method is competitive with the methods presented
in~\newcite{tang2015document}. We tune the hyper-parameters on the validation set and observe
that using n-grams up to~5 leads to the best performance.
Unlike~\newcite{tang2015document},~\texttt{fastText} does not use pre-trained word embeddings, which
can be explained the 1$\%$ difference in accuracy.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{\hspace{2pt}}l@{\hspace{2pt}}cccc}
\toprule
Model & Yelp'13 & Yelp'14 & Yelp'15 & IMDB \\
\midrule
SVM+TF & 59.8 & 61.8 & 62.4 & 40.5 \\
CNN & 59.7 & 61.0 & 61.5 & 37.5\\
Conv-GRNN & 63.7 & 65.5 & 66.0 & 42.5 \\
LSTM-GRNN & 65.1 & 67.1 & 67.6 & 45.3 \\
\midrule
\texttt{fastText}          & 64.2 & 66.2 & 66.6 & 45.2 \\
%y13 : 634 (standard) lr.5 ng=5
%y14 : 658 (std) ng=4
%y15: 665 (std) ng=3
% IMDB: nh = 100, lr=.5, t = 10, ngram = 4
\bottomrule
\end{tabular}
\caption{Comparision with \protect\newcite{tang2015document}. The hyper-parameters
are chosen on the validation set. We report the test accuracy.}\label{tab:tang_res}
\end{table}


\begin{table*}[t]
\centering
\small
\begin{tabular}{p{20em}clcp{15em}}
\toprule
Input &~~& Prediction &~~& Tags \\
\midrule
taiyoucon 2011 digitals: individuals digital photos from the anime convention
taiyoucon 2011 in mesa, arizona. if you know the model and/or the character,
please comment.
&&
{\#}cosplay
&&
{\#}24mm {\#}anime {\#}animeconvention {\#}arizona {\#}canon {\#}con {\#}convention {\#}cos \textbf{{\#}cosplay} {\#}costume {\#}mesa {\#}play {\#}taiyou {\#}taiyoucon
\\ 
\midrule[0.02em]
2012 twin cities pride 2012 twin cities pride parade
&&
{\#}minneapolis
&&
{\#}2012twincitiesprideparade \textbf{{\#}minneapolis} {\#}mn {\#}usa 
\\
\midrule[0.02em]
beagle enjoys the snowfall 
&&
{\#}snow
&&
{\#}2007 {\#}beagle {\#}hillsboro {\#}january {\#}maddison {\#}maddy {\#}oregon \textbf{{\#}snow} 
%\\\hline
%the vatican museum 
%&
%{\#}rome
%&
%{\#}architecture {\#}art {\#}interior {\#}italy {\#}march2009 {\#}museum \textbf{{\#}rome} {\#}vatican
\\
\midrule[0.08em]
christmas && {\#}christmas && {\#}cameraphone {\#}mobile
\\
\midrule[0.02em]
euclid avenue && {\#}newyorkcity && {\#}cleveland {\#}euclidavenue 
%\\\hline
%page 39 of contemporary german paintings contemporary german paintings
%exhibition / page 39 december 26, 1906 - january 20, 1907
%&
%{\#}art
%& {\#}academy {\#}arts {\#}buffalo {\#}catalogue {\#}exhibition {\#}fine
\\
\bottomrule
\end{tabular}
\caption{Examples from the validation set of YFCC100M dataset obtained with \texttt{fastText}
with $200$ hidden units and bigrams. We show a few correct and incorrect tag predictions.
}\label{tab:tag_ex}
\end{table*}

\paragraph{Training time.}
Both char-CNN and VDCNN are trained on a~NVIDIA Tesla~K40~GPU, while our
models are trained on a~CPU using~20 threads.
Table~\ref{tab:sent_speed} shows that methods using convolutions are several orders of
magnitude slower than~\texttt{fastText}. While it is
possible to have a 10$\times$ speed up for char-CNN by using more recent~CUDA
implementations of convolutions,~\texttt{fastText} takes less than a minute to
train on these datasets. The~GRNNs method of~\newcite{tang2015document} takes
around~12 hours per epoch on~CPU with a single thread.
Our speed-up compared to neural network based methods increases with the size
of the dataset, going up to at least a~15,000$\times$ speed-up.

\subsection{Tag prediction}

\paragraph{Dataset and baselines.}
To test scalability of our approach, further evaluation is carried on the~YFCC100M dataset~\cite{ni15} which consists
of almost~100M images with captions, titles and tags.
We focus on predicting the tags according to the title and caption~(we do not use the images).
We remove the words and tags occurring less than~100 times and split the data
into a train, validation and test set.  The train set contains~91,188,648
examples~(1.5B tokens). The validation has~930,497 examples and the test set~543,424. 
The vocabulary size is~297,141 and there are~312,116 unique tags.
We will release a script that recreates this dataset so that our numbers could be reproduced.
We report precision at~1.

We consider a frequency-based baseline which predicts the most frequent tag. We
also compare with Tagspace~\cite{weston2014tagspace}, which is a tag prediction
model similar to ours, but based on the~Wsabie model of
~\newcite{weston2011wsabie}.  While the Tagspace model is described using
convolutions, we consider the linear version, which achieves comparable
performance but is much faster.

\begin{table}[t]
\centering
\small
\begin{tabular}{@{\hspace{2pt}}l@{\hspace{2pt}}ccc}
\toprule
\multirow{2}[3]{*}{Model} & \multirow{2}[3]{*}{prec@1} & \multicolumn{2}{c}{Running time}\\
\cmidrule(l){3-4}
& &  Train & Test \\
\midrule
Freq. baseline           & 2.2  & -    & - \\
%Tagspace, $h=50$, $e=5$  & 28.8 & 47m  & 6h \\
Tagspace, $h=50$         & 30.1 & 3h8  & 6h \\
%Tagspace, $h=200$, $e=5$~~ & 32.7 & 1h23 & 15h \\
Tagspace, $h=200$        & 35.6 & 5h32 & 15h \\
\midrule
\texttt{fastText}, $h=50$          & 31.2 & 6m40 & 48s \\
\texttt{fastText}, $h=50$, bigram  & 36.7 & 7m47 & 50s \\ % loading takes for 2 minutes TODO make it binary 1m12 \\
\texttt{fastText}, $h=200$         & 41.1 & 10m34  & 1m29 \\
\texttt{fastText}, $h=200$, bigram & 46.1 & 13m38  & 1m37 \\ 
\bottomrule
\end{tabular}
\caption{Prec@1 on the test set for tag prediction on
YFCC100M. We also report the training time and test time.
Test time is reported for a single thread, while training uses 20 threads for both models.
%We report our method after $5$ epochs.
%Accuracy for Tagspace is computed after 5 epochs ($e=5$) and at convergence. %% i would keep just after convergence, else our model can be also much faster if we just run for as long as is needed to get to P@1=28.5%
}\label{tab:tag_res}
\end{table}

\paragraph{Results and training time.}
Table~\ref{tab:tag_res} presents a comparison of~\texttt{fastText} and the baselines.
We run~\texttt{fastText} for~5 epochs and compare it to~Tagspace for two sizes of the
hidden layer,~i.e.,~50 and~200.
Both models achieve a similar performance with a small hidden layer, but adding
bigrams gives us a significant boost in accuracy. At test
time, Tagspace needs to compute the scores for all the classes which makes it
relatively slow, while our fast inference gives a significant speed-up when the
number of classes is large~(more than~300K here). Overall, we are more than an order of magnitude
faster to obtain model with a better quality. The speedup of the test phase is
even more significant~(a~600$\times$ speedup). Table~\ref{tab:tag_ex} shows
some qualitative examples.
