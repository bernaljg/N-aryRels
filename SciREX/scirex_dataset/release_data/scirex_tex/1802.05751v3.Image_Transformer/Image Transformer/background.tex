There is a broad variety of types of image generation models in the literature. This work is strongly inspired by autoregressive models such as fully visible belief networks and NADE \citep{Bengio00, larochelle2011} in that we also factor the joint probability of the image pixels into conditional distributions. Following PixelRNN \citep{PixelRNN}, we also model the color channels of the output pixels as discrete values generated from a multinomial distribution, implemented using a simple softmax layer.

The current state of the art in modeling images on CIFAR-10 data set was achieved by PixelCNN++, which models the output pixel distribution with a discretized logistic mixture likelihood, conditioning on whole pixels instead of color channels and changes to the architecture \citep{PixelCNNpp}. These modifications are readily applicable to our model, which we plan to evaluate in future work.

Another, popular direction of research in image generation is training models with an adversarial loss \citep{gan}. Typically, in this regime a generator network is trained in opposition to a discriminator network trying to determine if a given image is real or generated. In contrast to the often blurry images generated by networks trained with likelihood-based losses, generative adversarial networks (GANs) have been shown to produce sharper images with realistic high-frequency detail in generation and image super-resolution tasks \citep{StackGAN, SRGAN}.

While very promising, GANs have various drawbacks.
They are notoriously unstable \citep{DCGAN}, motivating a large number of methods attempting to make their training more robust \citep{unrolled_gans, began}. Another common issue is that of mode collapse, where generated images fail to reflect the diversity in the training set \citep{unrolled_gans}.

A related problem is that GANs do not have a density in closed-form. This makes it challenging to measure the degree to which the models capture diversity. This also complicates model design. Objectively evaluating and comparing, say, different hyperparameter choices is typically much more difficult in GANs than in models with a tractable likelihood.