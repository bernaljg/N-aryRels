Recent advances in modeling the distribution of natural images with neural networks allow them to generate increasingly natural-looking images. % (PixelRNN, CNN++, cf refs from decoding with pixelcnn decoders, but also some fundamental VAEs, GANs, ...)
Some models, such as the PixelRNN and PixelCNN \cite{PixelRNN}, have a tractable likelihood. Beyond licensing the comparatively simple and stable training regime of directly maximizing log-likelihood, this enables the straightforward application of these models in problems such as image compression \cite{vandenoord14} and probabilistic planning and exploration \cite{Bellemarre16}.

The likelihood is made tractable by modeling the joint distribution of the pixels in the image as the product of conditional distributions \cite{larochelle2011, Theis2015}. Thus turning the problem into a sequence modeling problem, the state of the art approaches apply recurrent or convolutional neural networks to predict each next pixel given all previously generated pixels \cite{PixelRNN}. Training recurrent neural networks to sequentially predict each pixel of even a small image is computationally very challenging. Thus, parallelizable models that use convolutional neural networks such as the PixelCNN have recently received much more attention, and have now surpassed the PixelRNN in quality \cite{PixelCNN}.

One disadvantage of CNNs compared to RNNs is their typically fairly limited receptive field. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly \cite{PixelCNNpp}. Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging.

In this work we show that self-attention \cite{cheng2016long, decomposableAttnModel, aiayn} can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions.
%revise

We adopt similar factorizations of the joint pixel distribution as previous work. Following recent work on modeling text \cite{aiayn}, however, we propose eschewing recurrent and convolutional networks in favor of the Image Transformer, a model based entirely on a self-attention mechanism. The specific, locally restricted form of multi-head self-attention we propose can be interpreted as a sparsely parameterized form of gated convolution. By decoupling the size of the receptive field from the number of parameters, this allows us to use significantly larger receptive fields than the PixelCNN.

Despite comparatively low resource requirements for training, the Image Transformer attains a new state of the art in modeling images from the standard ImageNet data set, as measured by log-likelihood. Our experiments indicate that increasing the size of the receptive field plays a significant role in this improvement. We observe significant improvements up to effective receptive field sizes of 256 pixels, while the PixelCNN \cite{PixelCNN} with 5x5 filters used 25.

Many applications of image density models require conditioning on additional information of various kinds: from images in enhancement or reconstruction tasks such as super-resolution, in-painting and denoising to text when synthesizing images from natural language descriptions \cite{Mansimov15}. In visual planning tasks, conditional image generation models could predict future frames of video conditioned on previous frames and taken actions.

In this work we hence also evaluate two different methods of performing conditional image generation with the Image Transformer. In image-class conditional generation we condition on an embedding of one of a small number of image classes. In super-resolution with high magnification ratio (4x), we condition on a very low-resolution image, employing the Image Transformer in an encoder-decoder configuration \cite{KalchbrennerB13}. In comparison to recent work on autoregressive super-resolution \cite{PixelRecursiveSuperResolution}, a human evaluation study found images generated by our models to look convincingly natural significantly more often.

%Recent work demonstrated significant progress towards tractably modeling the distribution of natural images using deep neural networks. This was achieved by modeling the joint distribution of pixels in the image as the product of conditional distributions, thereby turning it into a sequence problem, and applying recurrent and convolutional neural networks to it.

%In this work we instead apply the Transformer, a recently proposed fully attentional network architecture to similar factorizations of the problem. We present two extensions of the architecture that allow it to scale to images and to take advantage of their two-dimensional structure.

%While conceptually simple, our generative models of two image data sets match or outperform the current state of the art at significantly lower training costs. We also present results on two super-resolution tasks with large magnification ratio in which we apply our architecture in an encoder-decoder configuration, transforming a very low resolution input image into a higher resolution output.

%Lastly, we provide examples of images generated or completed by our various models which, following previous work, we believe to look pretty cool.


%Creating images is cool because.

%PixelRNNs and CNNs are slower and slow (TFLOPS numbers!). State-of-the-art uses lots of tricks (CNN++). GANs don't talk probs and are hard to train.

%Machine learning systems have been successful in many domains, from computer vision \cite{img12} to machine translation \cite{sutskever14,bahdanau2014neural,cho2014learning}.