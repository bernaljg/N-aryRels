All code we used to develop, train, and evaluate our models is available in Tensor2Tensor \citep{tensor2tensor}.
% \url{https://github.com/tensorflow/tensor2tensor}.

For all experiments we optimize with Adam~\citep{kingma2014adam}, and vary the learning rate as specified in~\cite{aiayn}. We train our models on both p100 and k40 GPUs, with batch sizes ranging from $1$ to $8$ per GPU.

\input{cifar10_gen}

\subsection{Generative Image Modeling}
%\sidenote{trandustin: are we reporting training time too?}
Our unconditioned and class-conditioned image generation models both use 1D local attention, with $l_q=256$ and a total memory size of $512$.
On CIFAR-10 our best unconditional models achieve a perplexity of $2.90$ bits/dim on the test set using either DMOL or categorical. For categorical, we use $12$ layers with $\modeldim=512$, heads=$4$, feed-forward dimension $2048$ with a dropout of $0.3$. In DMOL, our best config uses $14$ layers, $\modeldim=256$, heads=$8$, feed-forward dimension $512$ and a dropout of $0.2$. This is a considerable improvement over two baselines: the PixelRNN ~\cite{PixelRNN} and PixelCNN++~\cite{PixelCNNpp}. Introduced after the Image Transformer, the also self-attention based PixelSNAIL model reaches a significantly lower perplexity of $2.85$ bits/dim on CIFAR-10 \cite{chen2017pixelsnail}. On the more challenging ImageNet data set, however, the Image Transformer performs significantly better than PixelSNAIL.
% argue that a discretized mixture-of-logistics is a better fit for modeling ordinal pixel intensities than the cross-entropy loss, which models them as categorical. 
% With $8$ layers, $16$ heads, and $\modeldim=1024$, we achieve $2.99$ bits/dim, indicating that increasing the number of layers can improve model performance.

We also train smaller $8$ layer CIFAR-10 models which have $\modeldim=512$, $1024$ dimensions in the feed-forward layers, $8$ attention heads and use dropout of $0.1$, and achieve $3.03$ bits/dim, matching the PixelCNN model~\cite{PixelRNN}. Our best CIFAR-10 model with DMOL has $\modeldim$ and feed-forward layer layer dimension of $256$ and perform attention in $512$ dimensions.

ImageNet is a much larger dataset, with many more categories than CIFAR-10, requiring more parameters in a generative model. Our ImageNet unconditioned generation model has $12$ self-attention and feed-forward layers, $\modeldim=512$, $8$ attention heads, $2048$ dimensions in the feed-forward layers, and dropout of $0.1$. It significantly outperforms the Gated PixelCNN and establishes a new state-of-the-art of $3.77$ bits/dim with checkpoint averaging. We trained only unconditional generative models on ImageNet, since class labels were not available in the dataset provided by~\cite{PixelRNN}.

Table~\ref{tab:generative-log-probs} shows that growing the receptive field improves perplexity significantly. We believe this to highlight a key advantage of local self-attention over CNNs: namely that the number of parameters used by local self-attention is independent of the size of the receptive field. Furthermore, while $\modeldim > \mathrm{receptive field}$, self-attention still requires fewer floating-point operations.

For experiments with the categorical distribution we evaluated both coordinate encoding schemes described in Section \ref{sec:local-self-attention} and found no difference in quality. For DMOL we only evaluated learned coordinate embeddings.

% As Table~{\ref{tab:generative-log-probs}} shows, our models improve over various previously proposed models including the PixelRNN and the gated PixelCNN. On ImageNet we establish a new state of the art of 3.78, which we can improve to 3.77 by averaging the last ten checkpoints.

% While the PixelCNN++ achieved significantly better log-likelihoods on CIFAR-10 \citep{PixelCNNpp}, we expect that many of the modifications in the PixelCNN++ carry over to the Image Transformer. We further believe our curated images for various classes to be of reasonable perceptual quality.

%celeb a figures



%\begin{table*}[ht]
%\centering
%\caption{Negative log-likelihoods on the CIFAR-10 test and ImageNet validation sets. The Image %Transformer outperforms all models but PixelCNN++, achieving a new state of the art on %ImageNet. Larger memory blocks significantly improve its performance.}
%\label{tab:nll_table}
%
%\vspace{2mm}
%\begin{tabular}{llll}
%%\toprule
%Model Type & Memory Block Size & \multicolumn{2}{c}{NLL}  \\
% & & CIFAR-10 (Test) & ImageNet (Validation) \\
%\hline
%Pixel CNN & - & $3.14$ & -\\
%Row Pixel RNN & - &  $3.00$ & $3.86$ \\
%Gated Pixel CNN & - & $3.03$ & $3.83$\\
%Pixel CNN++ & - & $\mathbf{2.92}$ & -\\
%\hline
%Image Transformer 1D loc 8l & 8 & $4.06$ & - \\
% & 16 & $3.47$ & - \\
% & 64 & $3.13$ & - \\
% & 256 & $2.99$ & - \\ %$3.78$ \\
% with checkpoint averaging & 256 & 2.98 & $\mathbf{3.77}$ \\
%best model & 256 & $2.93$ & $3.78$ \\
%ith checkpoint averaging & 256& $\mathbf{2.92}$ & $\mathbf{3.77}$
%\label{tab:generative-log-probs}
%
%\end{tabular}
%}
%\end{table*}

\begin{table}%[hb]
\centering
\caption{Bits/dim on CIFAR-10 test and ImageNet validation sets. The Image Transformer outperforms all models and matches PixelCNN++, achieving a new state-of-the-art on ImageNet. Increasing memory block size ($bsize$) significantly improves performance.}
\vspace{2mm}
% \label{tab:nll_table}

\begin{tabular}{llll}
%\toprule
Model Type & $bsize$ & \multicolumn{2}{c}{NLL}  \\
 & & CIFAR-10 & ImageNet \\
 & & (Test) & (Validation) \\
\hline
Pixel CNN & - & $3.14$ & -\\
Row Pixel RNN & - &  $3.00$ & $3.86$ \\
Gated Pixel CNN & - & $3.03$ & $3.83$\\
Pixel CNN++ & - & $2.92$ & -\\
PixelSNAIL & - & $\mathbf{2.85}$ & $3.80$\\
\hline
Ours 1D local (8l, cat) & 8 & $4.06$ & - \\
 & 16 & $3.47$ & - \\
 & 64 & $3.13$ & - \\
 & 256 & $2.99$ & - \\ %$3.78$ \\
\hline
% with checkpoint averaging & 256 & 2.98 & $\mathbf{3.77}$ \\
Ours 1D local (cat) & 256 & $2.90$ & $\mathbf{3.77}$ \\
% w/ checkpoint avg. & 256 & $2.90$ & $\mathbf{3.77}$
Ours 1D local (dmol) & 256 & $2.90$ & -
\label{tab:generative-log-probs}

\end{tabular}
%}  
\end{table}

\subsection{Conditioning on Image Class}
We represent the image classes as learned $\modeldim$-dimensional embeddings per class and simply add the respective embedding to the input representation of every input position together with the positional encodings.

We trained the class-conditioned Image Transformer on CIFAR-10, achieving very similar log-likelihoods as in unconditioned generation. The perceptual quality of generated images, however, is significantly higher than that of our unconditioned models. The samples from our $8$-layer class- conditioned models in Table~\ref{tab:conditional_images}, show that we can generate realistic looking images for some categories, such as cars and trucks.

% We present some samples in Table~\ref{tab:conditional_images}. 

% This is a table 


\subsection{Image Super-Resolution}\label{sec:super-res}

%\marginpar{It's important to mention why high magnification-ratio superres is very different from lower ratio. Have a look at the great explanation in Mohammad's paper, for instance.}

Super-resolution is the process of recovering a high resolution image from a low resolution image while generating realistic and plausible details. Following \citep{PixelRecursiveSuperResolution}, in our experimental setup we enlarge an $8 \times 8$ pixel image four-fold to $32 \times 32$, a process that is massively underspecified: the model has to generate aspects such as texture of hair, makeup, skin and sometimes even gender that cannot possibly be recovered from the source image.

Here, we use the Image Transformer in an encoder-decoder configuration, connecting the encoder and decoder through an attention mechanism \citep{aiayn}. For the encoder, we use embeddings for RGB intensities for each pixel in the $8 \times $8 image and add $2$ dimensional positional encodings for each row and width position. Since the input is small, we flatten the whole image as a $[\height \times \width \times 3, \modeldim]$ tensor, where $\modeldim$ is typically $512$. We then feed this sequence to our stack of transformer encoder layers that uses repeated self-attention and feed forward layers. In the encoder we don't require masking, but allow any input pixel to attend to any other pixel. In the decoder, we use a stack of local self-attention, encoder-decoder-attention and feed-forward layers. We found using two to three times fewer encoder than decoder layers to be ideal for this task.

%We train end-to-end, maximizing likelihood..
We perform end-to-end training of the encoder-decoder model for Super resolution using the log-likelihood objective function. Our method generates higher resolution images that look plausible and realistic across two datasets.

For both of the following data sets, we resized the image to $8 \times 8$ pixels for the input and $32\times32$ pixels for the label using TensorFlow's $\mathrm{area}$ interpolation method.

\input{human_eval_tab}

\input{celeba_superres_images}

\paragraph{CelebA}

We trained both our 1D Local and 2D Local models on the standard CelebA data set of celebrity faces with cropped boundaries. With the 1D Local, we achieve a negative log likelihood (NLL) of $\mathbf{2.68}$ bits/dim on the dev set, using $l_q=128$, memory size of $256$, $12$ self-attention and feed-forward layers, $\modeldim=512$, $8$ attention heads, $2048$ dimensions in the feed-forward layers, and a dropout of $0.1$. With the 2D Local model, we only change the query and memory to now represent a block of size $8\times32$ pixels and $16\times64$ pixels respectively. This model achieves a NLL of $\mathbf{2.61}$ bits/dim.
Existing automated metrics like pSNR, SSIM and MS-SSIM have been shown to not correlate with perceptual image quality \citep{PixelRecursiveSuperResolution}. Hence, we conducted a human evaluation study on Amazon Mechanical Turk where each worker is required to make a binary choice when shown one generated and one real image. Following the same procedure for the evaluation study as \cite{PixelRecursiveSuperResolution}, we show $50$ pairs of images, selected randomly from the validation set, to $50$ workers each. Each generated and original image is upscaled to $128\times128$ pixels using the Bilinear interpolation method. Each worker then has $1$-$2$ seconds to make a choice between these two images. In our method, workers choose images from our model up to $36.1$\% of the time, a significant improvement over previous models. Sampling temperature of $0.8$ and 2D local attention maximized perceptual quality as measured by this evaluation.

To measure how well the high resolution samples correspond to the low resolution input, we calculate Consistency, the $L2$ distance between the low resolution input and a bicubic downsampled version of the high resolution sample. We observe a Consistency score of $0.01$ which is on par with the models in \cite{PixelRecursiveSuperResolution}. 

We quantify that our models are more effective than exemplar based Super Resolution techniques like Nearest Neighbors, which perform a naive look-up of the training data to find the high resolution output. We take a bicubic down-sampled version of our high resolution sample, find the nearest low resolution input image in the training data for that sample, and calculate the MS-SSIM score between the high resolution sample and the corresponding high resolution image in the training data. On average, we get a MS-SSIM score of $44.3$, on $50$ samples from the validation set, which shows that our models don't merely learn to copy training images but generate high-quality images by adding synthesized details on the low resolution input image.

\paragraph{CIFAR-10} We also trained a super-resolution model on the CIFAR-10 data set. Our model reached a negative log-likelihood of $2.76$ using 1D local attention and $2.78$ using 2D local attention on the test set. As seen in Figure~\ref{tab:completion_and_superres}, our model commonly generates plausible looking objects even though the input images seem to barely show any discernible structure beyond coarse shapes.