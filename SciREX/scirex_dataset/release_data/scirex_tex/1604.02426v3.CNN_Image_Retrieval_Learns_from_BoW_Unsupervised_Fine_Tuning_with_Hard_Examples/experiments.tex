\section{Experiments}
%
\vspace{-5pt}
In this section we discuss implementation details of our training, evaluate different components of our method, and compare to the state of the art. 

\vspace{-5pt}
\subsection{Training setup and implementation details}
%
Our training samples are derived from the dataset used in the work of Schonberger~\etal~\cite{SRCF15}, which consists of 7.4 million images downloaded from Flickr using keywords of popular landmarks, cities and countries across the world.
The clustering procedure~\cite{CM10a} gives $19,546$ images to serve as query seeds. 
The extensive retrieval-SfM reconstruction~\cite{RSJFCM16} of the whole dataset results in $1,474$ reconstructed 3D models. 
Removing overlapping models leaves us with $713$ 3D models containing $163,671$ unique images from the initial dataset.
The initial dataset contained on purpose all images of Oxford5k and Paris6k datasets. 
In this way, we are able to exclude 98 clusters that contain any image (or their near duplicates) from these test datasets.

The largest model has $11,042$ images, while the smallest has $25$.
We randomly select $551$ models ($133,659$ images) for training and $162$ ($30,012$) for validation. 
The number of training queries per cluster is 10\% of the cluster size for clusters of 300 or less images, or 30 images for larger clusters. A total number of $5,974$ images is selected for training queries, and $1,691$ for validation queries.

Each training and validation tuple contains $1$ query, $1$ positive and $5$ negative images.
The pool of candidate positives consists of $k=100$ images with closest camera centers to the query.
In particular, for method $m_3$, the inliers overlap threshold is $t_i=0.2$, and the scale change threshold $t_s=1.5$.
Hard negatives are re-mined $3$ times per epoch, \ie roughly every $2,000$ training queries. 
Given the chosen queries and the chosen positives, we further add 20 images per cluster to serve as candidate negatives during re-mining.
This constitutes a training set of $22,156$ images and it corresponds to the case that all 3D models are included for training.

To perform the fine-tuning as described in Section~\ref{sec:network}, we initialize by the convolutional layers of AlexNet~\cite{KSH12} or VGG~\cite{SZ14}.
We use learning rate equal to $0.001$, which is divided by $5$ every $10$ epochs, momentum $0.9$, weight decay $0.0005$, parameter $\tau$ for contrastive loss $0.7$, and batch size of $5$ training tuples.
All training images are resized to a maximum $362 \times 362$ dimensionality, while keeping the original aspect ratio.
Training is done for at most $30$ epochs and the best network is selected based on performance, measured via mean Average Precision (mAP)~\cite{PCISZ07}, on validation tuples. 

\vspace{-5pt}
\subsection{Test datasets and evaluation protocol}
%
We evaluate our approach on Oxford buildings~\cite{PCISZ07}, Paris~\cite{PCISZ08} and Holidays\footnote{We use the up-right version of Holidays dataset (rotated).}~\cite{JDS08} datasets.
First two are closer to our training data, while the last differentiates by containing similar scenes and not only man made objects or buildings. 
These are also combined with 100k distractors from Oxford100k to allow for evaluation at larger scale. 
The performance is measured via mAP. 
We follow the standard evaluation protocol for Oxford and Paris and crop the query images with the provided bounding box. 
The cropped image is fed as input to the CNN.
However, to deliver a direct comparison with other methods, we also evaluate queries generated by keeping all activations that fall into this bounding box~\cite{BL15,AGTPS15} when the full query image is used as input to the network.
We refer to the cropped images approach as \cropI and the cropped activations~\cite{BL15,AGTPS15} as \cropA. 
The dimensionality of the images fed into the CNN is limited to $1024 \times 1024$ pixels.
In our experiments, no vector post-processing is applied if not otherwise stated.

%
%
\input{fig_exp_posmethods}
%
%
\input{fig_exp_clusternumber}
%
\subsection{Results on image retrieval}
%
\paragraph{Learning.}
%
We evaluate the off-the-shelf CNN and our fine-tuned ones after different number of training epochs. 
Our different methods for positive and negative selection are evaluated independently in order to decompose the benefit of each ingredient. 
Finally, we also perform a comparison with the triplet loss~\cite{AGTPS15}, trained on exactly the same training data as the ones used for our architecture with the contrastive loss. 
Results are presented in Figure~\ref{fig:posnegmethod}.
%
The results show that positive examples with larger view point variability, and negative examples with higher content variability, both acquire a consistent increase in the performance. 
The triplet loss\footnote{The margin parameter for the triplet loss is set equal to 0.1~\cite{AGTPS15}.} appears to be inferior in our context; we observe oscillation of the error in the validation set from early epochs, which implies over-fitting. 
In the rest of the paper, we adopt the $m_3,\cN_2$ approach.

\paragraph{Dataset variability.}
%
We perform fine-tuning by using a subset of the available 3D models. 
Results are presented in Figure~\ref{fig:clusternumber} with 10, 100 and 551 (all available) clusters, while keeping the amount of training data, \ie training queries, fixed.
In the case of 10 and 100 models we use the largest ones, \ie ones with the highest number of images.
It is better to train with all 3D models due to the higher variability in the training set. 
Remarkably, significant increase in performance is achieved even with 10 or 100 models. 
However, the network is able to over-fit in the case of few clusters.
All models are utilized in all other experiments.

\paragraph{Learned projections.}
%
The PCA-whitening~\cite{JC12} (\pcawhiten) is shown to be essential in some cases of CNN-based descriptors~\cite{BSCL14,BL15,TSJ16}.
On the other hand, it is shown that on some of the datasets, the performance after \pcawhiten substantially drops compared with the raw descriptors (max pooling on Oxford5k~\cite{BL15}). 
We perform comparison of this traditional way of whitening  and our learned whitening (\cpl2), described in Section~\ref{ref:projections}.
Table~\ref{tab:postproc} shows results without post-processing and with the two different methods of whitening.
%
Our experiments confirm, that \pcawhiten often reduces the performance. In contrast to that, the proposed
\cpl2 achieves the best performance in most cases and is never the worst performing method. Compared to no post-processing baseline, \cpl2 reduces the performance twice for AlexNet, but the drop is negligible compared to the drop observed for \pcawhiten. For the VGG, the proposed \cpl2 {\em always} outperforms the no post-processing baseline.

Our unsupervised learning directly optimizes MAC when extracted from full images, however, we further apply the fine-tuned networks to construct R-MAC representation~\cite{TSJ16} with regions of three different scales.
It consists of extracting MAC from multiple sub-windows and then aggregating them. 
Directly optimizing R-MAC during learning is possible and could offer extra improvements, but this is left for future work. Despite the fact that R-MAC offers improvements due to the regional representation, in our experiments it is not always better than MAC, since the latter is optimized during the end-to-end learning.
We apply \pcawhiten on R-MAC as in~\cite{TSJ16}, that is, we whiten each region vector first and then aggregate. Performance is significantly higher in this way. In the case of our \cpl2, we directly whiten the final vector after aggregation, which is also faster to compute.
%
\input{tab_exp_postproc}
%


\vspace{10pt}
\paragraph{Dimensionality reduction.}
%
We compare dimensionality reduction performed with \pcawhiten~\cite{JC12} and with our \cpl2. The performance for varying descriptor dimensionality is plotted in Figure~\ref{fig:dimreduce}. The plots suggest that \cpl2 works better in higher dimensionalities, while \pcawhiten works slightly better for the lower ones. Remarkably, MAC reduced down to 16D outperforms state-of-the-art on BoW-based 128D compact codes~\cite{RJC15} on Oxford105k ($45.5$ vs $41.4$). Further results on very short codes can be found in Table~\ref{tab:stateofart}. 

%
\input{fig_exp_dimreduction}
%

\vspace{10pt}
\paragraph{Over-fitting and generalization.}
%
In all experiments, all clusters including any image (not only query landmarks) from Oxford5k or Paris6k datasets are removed. 
To evaluate whether the network tends to over-fit to the training data or to generalize, we repeat the training, this time using all 3D reconstructions, including those of Oxford and Paris landmarks.
The same amount of training queries is used for a fair comparison.
We observe negligible difference in the performance of the network on Oxford and Paris evaluation results, \ie the difference in mAP was on average $+0.3$ over all testing datasets. 
We conclude that the network generalizes well and is relatively insensitive to over-fitting. 

%
\input{tab_exp_stateofart}
%
\paragraph{Comparison with the state of the art.}
%
We extensively compare our results with the state-of-the-art performance on compact image representations and extremely short codes. The results for MAC and R-MAC with the fine-tuned networks are summarized together with previously published results in Table~\ref{tab:stateofart}. The proposed methods outperform the state of the art on Paris and Oxford datasets, with and without distractors with all 16D, 32D, 128D, 256D, and 512D descriptors. On Holidays dataset, the Neural codes~\cite{BSCL14} win the extreme short code category, while off-the-shelf NetVlad performs the best on 256D and higher. %No results better than ours were reported for 128D on Holidays.

We additionally combine MAC and R-MAC with recent localization method for re-ranking~\cite{TSJ16} to further boost the performance. Our scores compete with state-of-the-art systems based on local features and query expansion. These have much higher memory needs and larger query times. 

Observations on the recently published NetVLAD~\cite{AGTPS15}:
(1) After fine-tuning, NetVLAD performance drops on Holidays, while our training improves off-the-shelf results on all datasets.
(2) Our 32D MAC descriptor has comparable performance to 256D NetVLAD  on Oxford5k (ours $69.2$ vs NetVLAD $63.5$), and on Paris6k (ours $69.5$ vs NetVLAD $73.5$).
