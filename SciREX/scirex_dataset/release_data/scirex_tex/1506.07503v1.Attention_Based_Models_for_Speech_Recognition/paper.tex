\documentclass{article} % For LaTeX2e

\usepackage{nips15submit_e,times}
\usepackage[colorlinks=True,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{multirow}

%TODO: comment for submission
%\usepackage{showkeys}% show equation keys in the pdf

\usepackage[pdftex]{graphicx}
\graphicspath{{img/}{extras/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage{rotating}
\newcommand{\tred}[1]{\textcolor{red}{#1}}


%\usepackage{natbib}
%\renewcommand{\bibsection}{\subsubsection*{References}}

\title{Attention-Based Models for Speech Recognition} 

\author{
Jan Chorowski \\
University of Wroc\l{}aw, Poland\\
\texttt{jan.chorowski@ii.uni.wroc.pl} \\
\And
Dzmitry Bahdanau \\
Jacobs University Bremen, Germany
\And
Dmitriy  Serdyuk \\
Universit\'{e} de Montr\'{e}al
\And
Kyunghyun Cho\\
Universit\'{e} de Montr\'{e}al
\And
Yoshua Bengio \\
Universit\'{e} de Montr\'{e}al \\
CIFAR Senior Fellow
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
% TODO: insert final correct numbers    
Recurrent sequence generators conditioned on input data through an attention
mechanism have recently shown very good performance on a range of tasks
including machine translation, handwriting synthesis
\cite{graves_generating_2013,bahdanau_neural_2014} and image caption generation
\cite{xu_show_2015}. We extend the attention-mechanism with features needed for speech
recognition. We show that while an adaptation of the model used for machine
translation in \cite{bahdanau_neural_2014} reaches a competitive 18.7\% phoneme
error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are
roughly as long as the ones it was trained on. We offer a qualitative
explanation of this failure 
and propose a novel and generic method of adding
location-awareness to the attention mechanism to alleviate this issue. The new
method yields a model that is robust to long inputs and achieves 18\% PER in single
utterances and 20\% in 10-times longer (repeated) utterances.  Finally, we
propose a change to the attention mechanism that prevents it from concentrating
too much on single frames, which further reduces PER to 17.6\% level. 
\end{abstract}

\section{Introduction}

% Paragraph 1: introduction attention mechanisms
Recently, attention-based recurrent networks have been successfully applied to a
wide variety of tasks, such as handwriting
synthesis~\cite{graves_generating_2013}, machine
translation~\cite{bahdanau_neural_2014}, image caption
generation~\cite{xu_show_2015} and  visual object
classification~\cite{mnih_2014}.\footnote{%Stop space
  An early version of this work was presented at the NIPS 2014 Deep Learning
  Workshop \cite{chorowski_2014}.
}
Such models iteratively process their input by selecting relevant content at
every step. This basic idea significantly extends the applicability range of
end-to-end training methods, for instance, making it possible to construct
networks with external memory~\cite{graves_2014,weston_2014}.

% Paragraph 3: why speech recognition with attention is an 
% an interesting machine learning problem.
% IMPORTANT, THE STORY
We introduce extensions to attention-based recurrent networks
that make them applicable to
speech recognition. Learning to recognize speech can be viewed as
learning to generate a sequence (transcription) given another sequence (speech).
From this perspective it is similar to machine translation and handwriting
synthesis tasks, for which attention-based methods have been found suitable
\cite{bahdanau_neural_2014,graves_generating_2013}.  However, compared to
machine translation, speech recognition principally differs by
requesting much longer input
sequences (thousands of frames instead of dozens of words), which introduces a
challenge of distinguishing similar speech fragments\footnote{Explained in
more detail in Sec.~\ref{subsec:framework}.} in a single utterance.
% as
% well as issues of long-term dependencies~\cite{Bengio-trnn93-small} and the
% presence of two different scales in the input and output sequences. 
% DIMA:  both these issues are revelent for machine and
% translation and handwriting recognition. Let's
% concentrate on key points.
It is
also different from handwriting synthesis, since the input sequence is much
noisier and does not have as clear structure. For these reasons speech
recognition is an interesting testbed for developing new attention-based
architectures capable of processing long and noisy inputs.


% Paragraph 2: introduction to speech recognition
Application of attention-based models to speech recognition is also an
important step toward building fully end-to-end trainable speech
recognition systems, which is an active area of research. The dominant
approach is still based
on hybrid systems consisting of a deep neural acoustic model, a triphone HMM
model and an n-gram language
model~\cite{gales_application_2007,hinton_deep_2012}. This requires dictionaries
of hand-crafted pronunciation and phoneme lexicons, and a multi-stage training
procedure to make the components work together. Excellent results by an HMM-less
recognizer have recently been reported, with the system consisting of a
CTC-trained neural network and a language model~\cite{hannun2014_deepspeech}.
Still, the language model was added only at the last stage in that work, thus
leaving open a question of how much an acoustic model can benefit from being
aware of a language model during training.


% Paragraph 4: what we actually do
In this paper, we evaluate attention-based models on a
phoneme recognition task using the widely-used TIMIT
dataset. At each time step in generating an output sequence (phonemes),
an attention mechanism selects or weighs the signals produced
by a trained feature extraction mechanism at potentially all of the time steps 
in the input sequence (speech frames). The weighted feature vector then
helps to condition the generation of the next element of the output sequence.
Since the utterances in this dataset are rather
short (mostly under 5 seconds), we measure the
ability of the considered models in recognizing much longer
utterances which were created by artificially concatenating
the existing utterances.

% Paragraph 5: first important finding is the deceptively
% good performance of the baseline
We start with a model proposed in
\cite{bahdanau_neural_2014} for the machine translation task
as the baseline. This model seems entirely vulnerable to
the issue of similar speech fragments 
but despite our
expectations it was competitive on the original test set, reaching 18.7\% phoneme
error rate (PER). However, its performance
degraded quickly with longer, concatenated utterances. We
provide evidence that this model adapted to track the absolute
location in the input sequence of the content it is
recognizing, a strategy feasible for short utterances from the
original test set but inherently unscalable.

% Paragraph 6: the main contribution
In order to circumvent this undesired behavior, in this
paper, we propose to modify the attention mechanism such
that it explicitly takes into account both (a) the location of the
focus from the previous step, as in~\cite{graves_2014} and (b)
the features of the input sequence, as in~\cite{bahdanau_neural_2014}. 
This is achieved by adding as inputs to the attention mechanism
auxiliary {\it convolutional features} which are extracted 
by convolving the attention
weights from the previous step with trainable filters.  We show that a model
with such convolutional features performs significantly
better on the considered task (18.0\% PER). More
importantly, the model with convolutional features robustly
recognized utterances many times longer than the ones
from the training set, always staying below 20\% PER.

% Paragraph 7: smoothing?? windowing?? should we add it?

Therefore, the contribution of this work is three-fold. For one, we present a
novel purely neural speech recognition architecture based on an attention
mechanism, whose performance is comparable to that of the conventional
approaches on the TIMIT dataset.  Moreover, we propose a generic method of
adding location awareness to the attention mechanism. Finally, we introduce a
modification of the attention mechanism to avoid concentrating the attention on
a single frame, and thus avoid obtaining less ``effective training examples'',
bringing the PER down to 17.6\%.

\section{Attention-Based Model for Speech Recognition}

\subsection{General Framework}
\label{subsec:framework}

An attention-based recurrent sequence generator (ARSG) is a recurrent neural
network that stochastically generates an output sequence $(y_1, \dots, y_T)$
from an input $x$.  In practice, $x$ is often processed by an {\it encoder}
which outputs a sequential input representation $h=(h_1,\ldots,h_L)$ more
suitable for the attention mechanism to work with.

In the context of this work, the output $y$ is a sequence of phonemes, and the
input $x=(x_1, \ldots, x_{L'})$ is a sequence of feature vectors. Each feature
vector is extracted from a small overlapping window of audio frames. The encoder
is implemented as a deep bidirectional recurrent network (BiRNN), to form a
sequential representation $h$ of length $L=L'$.

At the $i$-th step an ARSG generates an output $y_i$ by focusing on the relevant
elements of $h$:
\begin{align}
    \alpha_i = Attend(s_{i-1},\alpha_{i-1},h) 
    \label{eq:attention}
    \\
    g_i = \sum\limits_{j=1}^L \alpha_{i,j} h_j 
    \label{eq:glimpse}
    \\
    y_i \sim Generate(s_{i-1}, g_i),
    \label{eq:generate}
\end{align}
where $s_{i-1}$ is the $(i-1)$-th state of the recurrent neural network to which
we refer as the {\it generator}, $\alpha_i \in \mathbb{R}^L$ is a vector of the
{\it attention weights}, also often called the
alignment~\cite{bahdanau_neural_2014}. Using the terminology from
\cite{mnih_2014}, we call $g_i$ a {\it glimpse}. The step is completed by
computing a new generator state:
\begin{align}
s_i = Recurrency(s_{i-1}, g_i, y_i)
\label{eq:recurrency}
\end{align}
Long short-term memory units (LSTM, \cite{hochreiter_1997}) and gated recurrent
units (GRU, \cite{cho_2014}) are typically used as a recurrent activation, to
which we refer as a {\it recurrency}.  The process is graphically illustrated in
Fig.~\ref{fig:model}.

Inspired by \cite{graves_2014}  we distinguish between location-based,
content-based and hybrid attention mechanisms. $Attend$ in
Eq.~\eqref{eq:attention} describes the most generic, hybrid attention. If the
term $\alpha_{i-1}$ is dropped from $Attend$ arguments, i.e.,
$\alpha_i=Attend(s_{i-1},h)$,
we call it content-based (see, e.g., \cite{bahdanau_neural_2014} or
\cite{xu_show_2015}). In this case, $Attend$ is often implemented by scoring
each element in $h$ separately and normalizing the scores:
\begin{align}
  \label{eq:pure_cb_attention}
    e_{i,j}=Score(s_{i-1}, h_j), \\ 
  \label{eq:softmax_normalization}
    \alpha_{i,j} = 
        \exp(e_{i,j}) \left/
        \sum\limits_{j=1}^L \exp(e_{i,j}) \right..
\end{align}    

%
% JAN: While I understand this point, I don't like it. The net has finite
% capacity (because we assume finite float precision and fixed
% number of units). Then it must fail for sequences which can be
% arbitrarily long.
%
% OTOH, If you assume unbounded capacity of the representation, then
% how do you learn it?
%
% DIMA: Agreed, we should rephrase it later.
%
% Cho: I rephrased it a bit, but now I feel that what I wrote is too dramatic.

The main limitation of such scheme is that identical or very similar elements of
$h$ are scored equally regardless of their position in the
sequence. This is the issue of ``similar speech fragments'' raised above.
Often this issue is partially alleviated by an encoder such
as e.g. a BiRNN~\cite{bahdanau_neural_2014} or a deep convolutional
network~\cite{xu_show_2015} that encode contextual
information into every element of $h$ . However, capacity of
$h$ elements is always limited, and thus disambiguation by
context is only possible to a limited extent.

%In
%practice $h$ elements often contain a lot of contextual information, being e.g.
%states of a BiRNN or feature vectors extracted by a deep convolutional network
%\cite{xu_show_2015}, which partially alleviates the issue. However, their
%capacity is still limited, which imposes certain upper bound on the the maximum
%size of the input $x$ that can be handled.

Alternatively, a location-based attention mechanism computes the alignment from
the generator state and the previous alignment only such that
$\alpha_{i} = Attend(s_{i-1}, \alpha_{i-1})$. 
For instance, Graves \cite{graves_generating_2013} used the location-based
attention mechanism using a Gaussian mixture model in his handwriting synthesis
model.  In the case of speech recognition, this type of location-based attention
mechanism would have to predict the distance between consequent 
phonemes using $s_{i-1}$ only, which we expect to be hard due to
large variance of this quantity.

For these limitations associated with both content-based and location-based
mechanisms, we argue that a hybrid attention mechanism is a natural candidate
for speech recognition. Informally, we would like an attention model that uses
the previous alignment $\alpha_{i-1}$ to select a short list of elements from
$h$, from which the content-based attention, in
Eqs.~\eqref{eq:pure_cb_attention}--\eqref{eq:softmax_normalization}, will select
the relevant ones without confusion.

\begin{figure}
    \centering
    %with .65\textwidth the font size roughly matches text
    \hfill
    \begin{minipage}{0.6\textwidth}
        \centering
        \includegraphics[width=0.95\columnwidth]{model.pdf}
    \end{minipage}
    \begin{minipage}{0.38\textwidth}
        \caption{
            Two steps of the proposed attention-based recurrent sequence
            generator (ARSG) with a hybrid attention mechanism (computing $\alpha$), based on
            both content ($h$) and location (previous $\alpha$) information.
            The dotted lines correspond to Eq.~\eqref{eq:attention}, thick solid
            lines to Eq.~\eqref{eq:glimpse} and dashed lines to
            Eqs.~\eqref{eq:generate}--\eqref{eq:recurrency}.
        }
        \label{fig:model}
    \end{minipage}
    \hfill
    %\includegraphics[width=.5\textwidth]{model.pdf}
    % As per the style file: the extra space here is intentional
    % \vspace{-.8cm}
    %\caption{Two steps of an ARSG with a hybrid attention
    %    mechanism. The dotted lines correspond to
    %    equation \eqref{eq:attention}, thick solid lines to \eqref{eq:glimpse} and dashed lines
    %    to \eqref{eq:generate} and \eqref{eq:recurrency}.
    %}
    %\label{fig:model}

  \vspace{-4mm}
\end{figure}    

\subsection{Proposed Model: ARSG with Convolutional Features}

We start from the ARSG-based model with the content-based attention mechanism
proposed in \cite{bahdanau_neural_2014}. This model can be described by
Eqs.~\eqref{eq:pure_cb_attention}--\eqref{eq:softmax_normalization}, where 
\begin{align}
e_{i,j} = w^\top \tanh(W s_{i-1} + V h_j + b).
\label{eq:base_attention}
\end{align}
$w$ and $b$ are vectors, $W$ and $V$ are matrices. 

% %TODO: JAN what to do with this 
% We further consider the following location-based additions
% to it. Define $p_i$ the ``expected'' position of the Generator
% at step $i$:
% \begin{align}
%     p_i = \sum\limits_{j=1} \alpha_{i,j} j
% \end{align}
% One natural idea is that the positions to the left of $p_i$ or far
% to the right of $p_i$ are unlikely to be useful for
% generating $y_{i+1}$. We add a \textit{gating mechanism} $g$ to make
% the model capable of learning it:
% \begin{align}
%   \label{eq:gating}
%     g(x) = w_g \tanh(v_g x) \\
%     e_{i,j} = w^T \tanh(W s_{i-1} + V h_j) g(j - p_i)
% \end{align}
% where $w_g$ and $v_g$ are parameter vectors trainable with
% the rest of the model.

% The gating mechanism can be critized for being a shallow
% addition to the attention model, only applied at the latest
% stage. It can also be affected by non-robustness of the
% mean. For instance, if the correct location were given a
% weight of 0.9, and a wrong location 500 steps to the right
% were given a weight of 0.1, the expected position mean $p_i$
% would be 50 position wrong, which would very likely harm the
% decoding procedure.  Finally, the gater $g$ is a rather
% unconventional scalar-to-scalar network whose arguments span
% a very large range. Therefore it required a special
% initialization in order to be trained in our experiments.

We extend this content-based attention mechanism of the original model to be
location-aware by making it take into account the alignment produced at the
previous step. First, we extract $k$ vectors $f_{i,j} \in \mathbb{R}^{k}$ for
every position $j$ of the previous alignment $\alpha_{i-1}$  by convolving it
with a matrix $F \in \mathbb{R}^{k \times r}$:
\begin{align}
    \label{eq:conv_feats}
    f_i = F * \alpha_{i-1}.
\end{align}
These additional vectors $f_{i,j}$ are then used by the scoring mechanism $e_{i,j}$:
\begin{align}
    \label{eq:hybrid_score}
    e_{i,j} = w^\top \tanh(W s_{i-1} + V h_j + U f_{i,j} + b)
\end{align}

%A location-aware extension of the attention mechanism takes into
%account the alignment produced by the ARSG at the previous 
%step. We propose to extract $k$ features
%$f_{i,j} \in \mathbb{R}^{k}$ for every position $j$ from
%the previous alignment $\alpha_{i-1}$  and
%add them to Equation \eqref{eq:base_attention}:
%\begin{align}
%  \label{eq:conv_feats}
%    f_i = F * \alpha_{i-1} \\ 
%    e_{i,j} = w^T \tanh(W s_{i-1} + V h_j + U f_{i,j} + b)
%\end{align}
%The $*$ character stands for convolution, that is $L$
%subvectors $(\alpha_{i-1,j-l}, \ldots, \alpha_{i-1,j+r-1})$,
%padded when necessary with zeros, 
%are multiplied with the matrix $F$
%yielding a matrix $f_i \in \mathbb{R}^{L,k}$. The numbers
%$l$ and $r$ define a possibly asymmetric window from which
%the features are extracted.

%We say that \textit{convolutional
    %features} were added to the attention mechanism to
%distinguish this approach from the others ones. In contrast to the
%gating mechanism, the hypothetical error situation described above
%should be handled just fine by the convolutional features.
%At the next iteration the locations close to the correctly assigned
%0.9 weight would be preferred over those near the wrongly assigned 0.1.
%This thought-experiment suggests that attention with convolutional 
%features should be robust, which was confirmed in our experiments.

% My take: (Jan)
% In contrast to the
% gating mechanism, the convolutional features are \emph{local} and robust in the
% sense that they only depend on the previous alignments that are inside
% the window. Thus a single mis-selected frame far away will not be seen
% by the window and will not influence the features. Moreover, the
% convolutional features are more generic. They can easily be applied to
% non-sequential data, such as images.

\subsection{Score Normalization: Sharpening and Smoothing}
\label{sec:sharpening}

There are three potential issues with the normalization in
Eq.~\eqref{eq:softmax_normalization}. 

First, when the input sequence $h$ is long, the glimpse $g_i$ is likely to
contain noisy information from many irrelevant feature vectors $h_j$, as the
normalized scores $\alpha_{i,j}$ are all positive and sum to $1$. This makes it
difficult for the proposed ARSG to focus clearly on a few relevant frames at
each time $i$. Second, the attention mechanism is required to consider all the
$L$ frames each time it decodes a single output $y_i$ while decoding the output
of length $T$, leading to a computational complexity of $O(LT)$. This may
easily become prohibitively expensive, when input utterances are long
(and issue that is less serious for machine translation, because in that
case the input sequence is made of words, not of 20ms acoustic frames).

The other side of the coin is that the use of {\it softmax} normalization in
Eq.~\eqref{eq:softmax_normalization} prefers to mostly focus on only a single
feature vector $h_j$. This prevents the model from aggregating multiple
top-scored frames to form a glimpse $g_i$.  

\paragraph{Sharpening}

There is a straightforward way to address the first issue of a noisy glimpse by
``sharpening'' the scores $\alpha_{i,j}$. One way to sharpen the weights is to
introduce an {\it inverse temperature} $\beta > 1$ to the softmax function such
that
\[
    a_{i,j}=\exp(\beta e_{i,j})\left/ \sum_{j=1}^{L}\exp(\beta e_{i,j})\right.,
\]
or to keep only the top-$k$ frames according to the scores and re-normalize
them.  These sharpening methods, however, still requires us to compute the score
of every frame each time ($O(LT)$), and they worsen the second issue, of overly narrow 
focus.

We also propose and investigate a {\it windowing} technique.
At each time $i$, the attention mechanism considers only a subsequence
$\tilde{h} = (h_{p_i-w}, \ldots, h_{p_i+w-1})$ of the whole sequence $h$, where 
$w \ll L$ is the predefined window width and $p_i$ is the median of the alignment
$\alpha_{i-1}$. The scores for $h_j \notin \tilde{h}$ are not computed, 
resulting in a lower complexity of $O(L+T)$.
This windowing technique is similar to taking the top-$k$ frames, and similarly,
has the effect of sharpening.
%Last but not least, windowing can be
%considered a location-based addition to the attention
%mechanism.

The proposed sharpening based on windowing can be used both during training and
evaluation. Later, in the experiments, we only consider the case where it is
used during evaluation.

\paragraph{Smoothing}

We observed that the proposed sharpening methods indeed
helped with long utterances. However, all of them, and
especially selecting the frame with the highest score,
negatively affected the model's performance on the standard
development set which mostly consists of short utterances.
This observations let us hypothesize that it is helpful for
the model to aggregate selections from multiple top-scored
frames. In a sense this brings more diversity, i.e., more
effective training examples, to the output part of the model,
as more input locations are considered.
To facilitate this effect, we replace the
unbounded exponential
function of the softmax function in
Eq.~\eqref{eq:softmax_normalization} with the
bounded logistic sigmoid $\sigma$ such that 
\[
    a_{i,j}=\sigma(e_{i,j})\left/\sum_{j=1}^{L}\sigma(e_{i,j})\right..
\]
This has the effect of {\it smoothing} the focus found by the attention
mechanism.




%One potential issue with the normalization \eqref{eq:softmax_normalization} is
%that all frames are assigned a non-zero weight. Thus, the longer the input
%sequence is, the more noise is contained in the glimpse $g_i$, which limits
%scalability of the approach. This motivated us to investigate methods that
%``sharpen'' the alignment by increasing weights of frames selected with highest
%scores $e_{i,j}$. One way to sharpen the weights is by introducing a factor
%$\beta>1$ into \eqref{eq:softmax_normalization}: $a_{i,j}=exp(\beta e_{i,j}) /
%\sum_{j=1}^{L}exp(\beta e_{i,j})$. 
%% \begin{equation} a_{i,j}=\frac{exp(\beta
%% e_{i,j})}{\sum_{j=1}^{L}exp(\beta e_{i,j})}.
%% \end{equation}
%Alternatively, one can keep only the $k$ highest scored frames during
%normalization and set the weights of all other frames to zero.

% lower scores to zero for all the
% locations, whose scores are not in top $k$ at the current
% step:
% \begin{equation}
%   \label{eq:top_k}
%   a_{i,j}=\begin{cases}
% \frac{exp(e_{i,j})}{\sum_{j\in \text{top $k$ scored}}exp(e_{i,j})} &
% \text{if $j$ is in top $k$ scored frames for step $i$} \\
% 0 & \text{otherwise}.
% \end{cases}
% \end{equation}

%Another issue related to weight sharpening is that
%scoring all representation elements $h_j$ seems not efficient in
%the light of our strong expectations of relevant content
%being close to the locus of attention at the previous step. 
%For this reason we investigate a
%\textit{windowing} technique, which amounts to considering only a
%subsequence $(h_{p_i-w}, \ldots, h_{p_i+w-1})$ of the
%representation $h$ for computing $g_i$ from Equation
%\eqref{eq:glimpse}, where $w$ defines the window width and
%$p_i$ is the median of the alignment $\alpha_i$.
%This way scores for $h$ elements outside the window are not
%computed, which decreases the computational
%complexity of decoding from $O(LT)$ to $O(L + T)$. Windowing
%has also a clear sharpening effect similar to the keep-$k$ method
%described above. Last but not least, windowing can be
%considered a location-based addition to the attention
%mechanism.
%
%In this work sharpening was used only during
%evaluation, however, all of them, and especially
%windowing, could be also applied during
%training, which we plan to investigate in future work. 
%
%In our experiments, alignment sharpening indeed turned out to
%be helpful for decoding long utterances, however it
%negatively affected decoding performance on the standard
%test set. Especially
%harmful was limiting the selection made at each step to only
%the highest scored frame\footnote{A table with detailed
%    information can be found in the Supplementary Material}.
%This lets us hypothesize that
%it is helpful for the model to aggregate selections from
%multiple top-scored frames. We found that the 
%normalization \eqref{eq:softmax_normalization} has a
%tendency to focus on single frames. To remedy it we
%replace the unbounded exponential function with the
%bounded logistic sigmoid $a_{i,j}=\sigma(e_{i,j}) / \sum_{j=1}^{L}\sigma(e_{i,j})$
%%\begin{equation}\label{eq:smooth_attention}
%%    a_{i,j}=\frac{\sigma(e_{i,j})}{\sum_{j=1}^{L}\sigma(e_{i,j})}.
%%\end{equation}
%and term the resulting procedure \textit{smooth focus} attention
%mechanism.

\section{Related Work}

Speech recognizers based on the connectionist temporal classification (CTC,
\cite{graves_2006_connectionist}) and its extension, RNN
Transducer~\cite{graves_2012_sequence}, are the closest to the ARSG model
considered in this paper. They follow earlier work on end-to-end trainable deep
learning over sequences with gradient signals flowing through the alignment
process~\cite{LeCun98-small}. They have been shown to perform well on the
phoneme recognition task~\cite{graves_2013_timit}.  Furthermore, the CTC was
recently found to be able to directly transcribe text from speech without any
intermediate phonetic representation~\cite{graves_2014_towards}.

The considered ARSG is different from both the CTC and RNN Transducer in two
ways.  First, whereas the attention mechanism deterministically aligns the input
and the output sequences, the CTC and RNN Transducer treat the alignment as a
latent random variable over which MAP (maximum a posteriori) inference is
performed. This deterministic nature of the ARSG's alignment mechanism allows
beam search procedure to be simpler. Furthermore, we empirically observe that a
much smaller beam width can be used with the deterministic mechanism, which
allows faster decoding (see Sec.~\ref{sec:evaluate} and
Fig.~\ref{fig:beam_dependency}).
Second, the alignment mechanism of both the CTC and RNN Transducer is
constrained to be ``monotonic'' to keep marginalization of the alignment
tractable. On the other hand, the proposed attention mechanism can result in
non-monotonic alignment, which makes it suitable for a larger variety of tasks
other than speech recognition.

A hybrid attention model using a convolution operation was also proposed in
\cite{graves_2014} for neural Turing machines (NTM). At each time step, the NTM
computes content-based attention weights which are then convolved with a
predicted shifting distribution. Unlike the NTM's approach, the hybrid mechanism
proposed here lets learning figure out how the content-based and location-based
addressing be combined by a deep, parametric function (see
Eq.~\eqref{eq:hybrid_score}.) 
%Also, it does not need to separately predict the
%shifting distribution.

Sukhbaatar et al. \cite{sukhbaatar_2015} describes a similar hybrid attention
mechanism, where location embeddings are used as input to the attention model.
This approach has an important disadvantage that the model cannot work with an
input sequence longer than those seen during training. Our approach, on the
other hand, works well on sequences many times longer than those seen during
training (see Sec.~\ref{sec:results}.)


%The main difference of ARSG from these approaches is the
%way aligment is treated. Specifically, it is
%deterministically computed, instead of being introduced as
%a hidden variable and marginalized at both training and
%% JAN: I guess only Herbert uses exploitation in this context
%evaluation stage, as it is done in CTC and RNN
%Transducer. Both approaches are feasible, however the beam
%search procedure for ARSG ends up being simpler and faster,
%mostly because much smaller beam size is required (see
%Section \ref{sec:decoding} and Figure \ref{fig:beam_dependency}). In addition,
%alignments produced by ARSG can be non-monotonous, unlike CTC
%and RNN Transducer, whose alignments are constrained to be
%monotonous to make marginalization tractable. However, in the
%context of speech recognition this additional freedom is not
%required, since the correct alignment is naturally
%left-to-right monotonous.

%A hybrid attention model that also involves convolution can
%be found in the Neural Turing Machine (NTM)
%\cite{graves_2014}. In that work the alignment proposed by a
%content-based attention is first interpolated with the previous
%alignment and then convolved with a predicted shift
%distribution to obtain a new aligment. We note, that our
%model fuses content-based and location-based contributions at
%a deeper level and also does not involve the shift
%prediction step, which we argue against in
%\ref{subsec:framework}.

%TODO: if we get rid of the gater, reword below
% Much like the gating mechanism from this work, the attention
% from the NTM shallowly fuses contributions from the
% content-based and location-based components. For this reason
% we deem our approach with convolutional features more
% powerful, since it allows deep fusion of these two types of
% information. It is also simpler in a way that no sharpening step
% is required. 

%Another approach to hybrid attention models can be found in
%\cite{sukhbaatar_2015}, where location embeddings are
%used as inputs to the attention. A disadvantage of such
%approach is that it does not scale to inputs larger than the
%ones used at the training stage, since no embeddings are
%available for the new, previously unseen locations. Our approach
%does not have such a limitation, and in practice works well
%on sequences many times longer than those seen during training (see Section
%\ref{sec:results}).


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{beam_influence}
  % \vspace{-.8cm}
  \caption{Decoding performance w.r.t. the beam size.
      %on the TIMIT test set. 
      For rigorous comparison, if decoding failed to generate
      $\left<\text{eos}\right>$, we considered it wrongly recognized without
      retrying with a larger beams size. The models, especially with smooth focus, 
      perform well even
      with a beam width as small as 1.
      %beam size was
      %not increased even when decoding failed to generate
      %the ``end-of-sequence'' token. In such a case all
      %phonemes of the utterance were considered wrongly
      %recognized.
  }
  \label{fig:beam_dependency}

  \vspace{-4mm}
\end{figure}

\section{Experimental Setup}
\label{sec:setup}

We closely followed the procedure in \cite{graves_2013_timit}. All experiments
were performed on the TIMIT corpus \cite{timit}. We used the train-dev-test
split from the Kaldi \cite{povey_2011} TIMIT s5 recipe. We trained on the
standard 462 speaker set with all SA utterances removed and used the 50 speaker
dev set for early stopping. We tested on the 24 speaker core test set. All
networks were trained on 40 mel-scale filter-bank features together with the
energy in each frame, and first and second temporal differences, yielding in
total 123 features per frame. Each feature was rescaled to have zero mean and
unit variance over the training set. Networks were trained on the full 61-phone
set extended with an extra ``end-of-sequence'' token that was appended to each
target sequence. Similarly, we appended an all-zero frame at the end of each
input sequence to indicate the end of the utterance. Decoding was performed
using the 61+1 phoneme set, while scoring was done on the 39 phoneme set.

\subsection{Training Procedure}

One property of ARSG models is that different subsets of parameters are reused
different number of times; $L$ times for those of the encoder, $LT$ for the
attention weights and $T$ times for all the other parameters of the ARSG.  This
makes the scales of derivatives w.r.t. parameters vary significantly, and we
handle it by using an adaptive learning rate algorithm,
AdaDelta~\cite{zeiler_2012} which has two hyperparameters $\epsilon$ and $\rho$.
All the weight matrices were initialized from a normal Gaussian distribution
with its standard deviation set to $0.01$. Recurrent weights were furthermore
orthogonalized.
%DIMA: reference for orthogonal weights?


As TIMIT is a relatively small dataset, proper regularization is crucial. We used
the adaptive weight noise as a main regularizer~\cite{graves_2011}.  We first
trained our models with a column norm constraint~\cite{hinton_2012} with the
maximum norm $1$
%(all weights incoming to each neuron were clipped to norm 1) 
until the lowest development negative log-likelihood is achieved.\footnote{
    Applying the weight noise from the beginning of training caused severe
    underfitting.
    %At the beginning of training the attention mechanism is not functional.
    %Therefore too much noise is applied to its weights and in consequence it
    %fails to train.
}
During this time, $\epsilon$ and $\rho$ are set to $10^{-8}$ and $0.95$,
respectively.  At this point, we began using the adaptive weight noise, and
scaled down the model complexity cost $L_C$ by a factor of 10, while disabling
the column norm constraints. Once
the new lowest development log-likelihood was reached, we fine-tuned the model
with a smaller $\epsilon=10^{-10}$, until we did
not observe the improvement in the development phoneme error rate (PER) for 100K
weight updates. Batch size 1 was used throughout the
training.

% Cho: I remember Hinton argued for the norm constraint as an alternative to
% weight decay, meaning we probably shouldn't consider them similar.
%were lifted, since adaptive
%weight noise method includes a term which acts similarly to
%weight decay. 

%TODO: what other details are needed there: initialization?
%For more information about the training procedure, see
%Supplementary Material.

% As indicated, in some experiments, the alignments selected by
% the model were limited to a window around the median position of the
% decoder at the previous step.

\subsection{Details of Evaluated Models}
\label{sec:evaluate}

We evaluated the ARSGs with different attention mechanisms.  The encoder was a
3-layer BiRNN with 256 GRU units in each direction, and the activations of the
512 top-layer units were used as the representation $h$.  The generator had a
single recurrent layer of 256 GRU units. 
$Generate$ in Eq.~\eqref{eq:generate} had a hidden layer of 64 maxout units.
The initial states of both the encoder and generator were treated as additional
parameters.

%and were learned for both
%Encoder and Generator.



%Similarly to \cite{bahdanau_neural_2014}, an
%additional layer of 64 maxout units combined the state $s_{i-1}$ and
%the glimpse $g_i$ (c.f. function $Generate$ in
%\eqref{eq:generate}). 

Our baseline model is the one with a purely content-based attention mechanism
(See Eqs.~\eqref{eq:pure_cb_attention}--\eqref{eq:base_attention}.) The scoring
network in Eq.~\eqref{eq:base_attention} had 512 
%$\tanh$ 
hidden units.
%, i.e. matrices $W$ and $V$ in Eq.~\eqref{eq:base_attention} had 512 rows.
%TODO: when removing the gater, remove this
% When used, the gating mechanism $g$ from equation \eqref{eq:gating} 
% had 10 hidden $\tanh$ units. 
The other two models use the convolutional features in Eq.~\eqref{eq:conv_feats}
with $k=10$ and $r=201$. One of them uses the smoothing from
Sec.~\ref{sec:sharpening}.

\paragraph{Decoding Procedure}
A left-to-right beam search over phoneme sequences
was used during decoding \cite{sutskever_sequence_2014}. Beam search was stopped
when the ``end-of-sequence'' token $\left<\text{eos}\right>$ was emitted. We
started with a beam width of 10, increasing it up to 40 when the network failed
to produce $\left<\text{eos}\right>$ with the narrower beam.  As shown in
Fig.~\ref{fig:beam_dependency}, decoding with a wider beam gives little-to-none
benefit. 


\section{Results}
\label{sec:results}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{michael_baseline}
  % \vspace{-.8cm}
  \caption[Alignments produced by the baseline]{Alignments
      produced by the baseline model. The vertical bars
      indicate ground truth phone location from TIMIT. Each
      row of the upper image indicates frames selected by
      the attention mechanism to emit a phone symbol. 
      %We observe that the 
      % Cho: probably redundant since the text below is already mentioned in the
      % text..
      The network has clearly learned to produce a left-to-right alignment with
      a tendency to look slightly ahead, and does not confuse between the
      repeated ``kcl-k'' phrase. 
      Best viewed in color.
      % TODO: the reviewer can ask what is the difference
      % when smooth focus is used
  }  
  \label{fig:ali_baseline}

  \vspace{-4mm}
\end{figure}

\begin{table}[h]
    \caption{Phoneme error rates (PER). The bold-faced PER corresponds to the
    best error rate with an attention-based recurrent sequence generator (ARSG)
    incorporating convolutional attention features and a smooth focus. }
  \label{tab:results}
  % \vspace{1mm}
  \centering
\begin{tabular}{l|c|c}
  % \hline
  \multicolumn{1}{c|}{\bf Model}  &\multicolumn{1}{c|}{\bf Dev} &\multicolumn{1}{c}{\bf Test} \\ 
  \hline 
  \hline 
  %\multicolumn{3}{c}{Kaldi TIMIT s5 recipe with {\tt basic} scorer} \\
  %\hline 
  % JCH13
  Baseline Model & 15.9\% & 18.7\% \\
  % JCH14
  Baseline + Conv. Features & 16.1\% & 18.0\% \\
  Baseline + Conv. Features + Smooth Focus & 15.8\% & {\bf 17.6\%} \\
  \hline
  % Pre-Transducer
  RNN Transducer 
  \cite{graves_2013_timit} & N/A & 17.7\% \\
  \hline\hline
  HMM over Time and Frequency Convolutional Net
  \cite{toth_2014} & 13.9\% & 16.7\% 
  \end{tabular}

  \vspace{-4mm}
\end{table}

All the models achieved competitive
PERs (see Table~\ref{tab:results}). 
With the convolutional features, we see 3.7\% relative improvement over the
baseline and further 5.9\% with the smoothing. 

%, which increased to 5.4\% when we also
%switched to the smooth-focus attention. 

% Cho: Maybe unnecessary to say, unless the other results (RNN Transducer and
% HMM T-F Conv Net) reported the variance.
%Every experiment was
%repeated only once, thus variance of these results is
%unknown. 

% Cho: Maybe in the conclusion?
%We note, that we also experimented with the gating
%mechanism proposed in the preliminary version of this work
%\cite{chorowski_2014}. We found it however in all aspects
%inferior to the convolutional features and subjected to
%instability of the mean.

To our surprise
%create proper alignments 
%considerations that a content-based attention can not do so 
(see Sec.~\ref{subsec:framework}.),  
the baseline model learned to align properly.
An alignment produced by the baseline model
on a sequence with repeated phonemes (utterance FDHC0\_SX209) is presented in
Fig.~\ref{fig:ali_baseline} which demonstrates that  the baseline model is not
confused by short-range repetitions. We can also see
from the figure that it
prefers to select frames that are near the beginning or even slightly before the
phoneme location provided as a part of the dataset. The alignments produced by
the other models were very similar visually.

\begin{figure}[t]
  \centering
  \vspace{.1cm} %only to visually separate from Table 1
  \includegraphics[width=\textwidth]{alis_corrected.png}
  %\includegraphics[width=\textwidth]{alis_corrected.pdf}
  %\vspace{-.8cm}
  \caption[Results of force-aligning of long utterances with corrections.]{
    Results of force-aligning the concatenated utterances. Each dot
    represents a single utterance created by either concatenating
    multiple copies of the same utterance, or of different, randomly
    chosen utterances. 
    %We compare the models with either a content-based only
    %or a hybrid attention mechanism.
    We clearly see that the highest robustness is achieved
    when the hybrid attention mechanism is combined with the proposed sharpening
    technique (see the bottom-right plot.)
    %We compare the baseline network having a
    %content-based only attention mechanism (top row) with a hybrid
    %attention mechanism that uses convolutional features (bottom
    %row). 
  }
  \label{fig:forced_ali_corrected}

  \vspace{-4mm}
\end{figure}

\subsection{Forced Alignment of Long Utterances}

The good performance of the baseline model led us to the question of how it
distinguishes between repetitions of similar phoneme sequences and how reliably
it decodes longer sequences with more repetitions. We created two datasets of
long utterances; one by repeating each test utterance, and the other by
concatenating randomly chosen utterances. In both cases, the waveforms were
cross-faded with a 0.05s silence inserted as the ``pau'' phone. We concatenated
up to $15$ utterances.

First, we checked the forced alignment with these longer utterances by forcing
the generator to emit the correct phonemes. Each alignment was considered
correct if 90\% of the alignment weight lies inside the ground-truth phoneme
window extended by 20 frames on each side. Under this definition, all
phones but the
$\left<\text{eos}\right>$ shown in Fig.~\ref{fig:ali_baseline} are properly
aligned.

The first column of Fig.~\ref{fig:forced_ali_corrected} shows the number of
correctly aligned frames w.r.t. the utterance length (in frames) for some of the
considered models. One can see that the baseline model was able to decode
sequences up to about 120 phones when a single utterance was repeated, and up to
about 150 phones when different utterances were concatenated. Even when it
failed, it correctly aligned about 50 phones. On the other hand, the model with
the hybrid attention mechanism with convolutional features was able to align
sequences up to 200 phones long. However, once it began to fail, the model was
not able to align almost all phones. The model with the smoothing behaved
similarly to the one with convolutional features only.

%Todo: plots for the appendix, or maybe show them here??
We examined failed alignments to understand these two different modes of
failure. Some of the examples are shown in the Supplementary Materials.

We found that the baseline model properly aligns about 40 first phones, then
makes a jump to the end of the recording and cycles over the last 10 phones.
This behavior suggests that it learned to track its approximate location in the
source sequence. However, the tracking capability is limited to the lengths
observed during training. Once the tracker saturates, it jumps to the end of the
recording.
%(the distance to the end was supposedly learned by the reverse RNN
%in the Encoder). 

In contrast, when the location-aware network failed it just stopped aligning --
no particular frames were selected for each phone.  We attribute this behavior
to the issue of noisy glimpse discussed in Sec.~\ref{sec:sharpening}. With a
long utterance there are many 
%hundred 
irrelevant frames negatively affecting the weight assigned to the correct
frames. In line with this conjecture, the location-aware network works slightly
better on the repetition of the same utterance, where all frames are somehow
relevant, than on the concatenation of different utterances, where each
misaligned frame is irrelevant. % to the currently decoded phone.

To gain more insight 
%about the ways the networks failed 
we applied the alignment sharpening schemes described in
Sec.~\ref{sec:sharpening}. In the remaining columns of
Fig.~\ref{fig:forced_ali_corrected}, we see that the sharpening
methods help the location-aware network to find proper alignments, while they
show little effect on the baseline network. 
%Spatially constraining the alignment
%to the vicinity of the previous one (i.e. windowing) 
The windowing technique helps both the baseline and location-aware networks,
with the location-aware network properly aligning nearly all sequences. 

During visual inspection, we noticed that in the middle of very long utterances
the baseline model was confused by repetitions of similar content within the
window, and that such confusions did not happen in the beginning. This supports
our conjecture above.
% DIMA: figure in Appendix would be so great


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{decode_longs}
  %\vspace{-.8cm}
  \caption[Phoneme error rates obtained on decoding long sequences.]{ 
      Phoneme error rates obtained on decoding long sequences. Each network was
      decoded with alignment sharpening techniques that produced proper forced
      alignments. The proposed ARSG's are clearly more robust to the length of
      the utterances than the baseline one is.
  }
  \label{fig:decoding_longs}

  \vspace{-4mm}
\end{figure}

\subsection{Decoding Long Utterances}

We evaluated the models on long sequences. Each model was decoded using the
alignment sharpening techniques that helped to obtain proper forced alignments.
The results are presented in Fig.~\ref{fig:decoding_longs}. The baseline model
fails to decode long utterances, even when a narrow window is used to constrain
the alignments it produces. The two other location-aware networks 
%with convolutional features added to the attention mechanism 
are able to decode utterances formed by concatenating up to 11 test utterances.
Better results were obtained with a wider window, presumably because it
resembles more the training conditions when at each step the attention mechanism
was seeing the whole input sequence.  With the wide window, both of the networks
scored about 20\% PER on the long utterances, indicating that the proposed
location-aware attention mechanism can scale to sequences much longer than those
in the training set with only minor modifications required at the decoding
stage.

\section{Conclusions}

We proposed and evaluated a novel end-to-end trainable speech recognition
architecture based on a hybrid attention mechanism which combines both content
and location information in order to select the next position in the input
sequence for decoding. One desirable property of the proposed model is that it
can recognize utterances much longer than the ones it was trained on. In the
future, we expect this model to be used to directly recognize text from
speech~\cite{hannun2014_deepspeech,graves_2014_towards}, in which case  it may
become important to incorporate a monolingual language model to the ARSG
architecture \cite{gulcehre_2015}.

This work has contributed two novel ideas for attention mechanisms: a better
normalization approach yielding smoother alignments and a generic principle for
extracting and using features from the previous alignments. 
%and using this features as additional low-level inputs to the
%attention mechanism. 
Both of these can potentially be applied beyond speech recognition. For
instance, the proposed attention can be used without
modification in neural Turing machines, or by using 2--D
convolution instead of 1--D, for improving image caption
generation~\cite{xu_show_2015}.


\subsubsection*{Acknowledgments}
All experiments were conducted using Theano
\cite{bergstra+al:2010-scipy,Bastien-Theano-2012}, PyLearn2
\cite{pylearn2_arxiv_2013}, and Blocks
\cite{vanmerrienboer_blocks_2015} libraries.

The authors would like to acknowledge the support of the following agencies for
research funding and computing support: National Science Center (Poland), 
NSERC, Calcul Qu\'{e}bec, Compute Canada,
the Canada Research Chairs and CIFAR. Bahdanau also thanks Planet
Intelligent Systems GmbH and Yandex.


{\small
\bibliographystyle{unsrt}
\bibliography{paper}
}

\clearpage
\appendix 
\input{supp_contents}

\end{document}

%\section{Goodies and TODO list}
%Automatic evaluation of networks on concatened timit sequences.

