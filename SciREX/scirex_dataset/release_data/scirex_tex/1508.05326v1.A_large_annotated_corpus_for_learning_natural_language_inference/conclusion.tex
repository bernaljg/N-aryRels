\section{Conclusion}\label{sec:conclusion}

Natural languages are powerful vehicles for reasoning, 
and nearly all questions about meaningfulness
in language can be reduced to questions of entailment
and contradiction in context. This suggests that NLI is an ideal testing ground
for theories of semantic representation, and that training for NLI
tasks can provide rich domain-general semantic representations.  To
date, however, it has not been possible to fully realize this
potential due to the limited nature of existing NLI resources.  This
paper sought to remedy this with a new, large-scale, naturalistic
corpus of sentence pairs labeled for entailment, contradiction, and
independence. We used this corpus to evaluate a range of models,
and found that both simple lexicalized models and neural network
models perform well, and that the representations learned by
a neural network model on our corpus can be used to dramatically 
improve performance on a standard challenge dataset. We hope that
SNLI presents valuable training data and a challenging testbed for the continued 
application of machine learning to semantic representation.