\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore, Murray, Steiner,
  Tucker, Vasudevan, Warden, Wicke, Yu, and Zheng]{Abadi2016tf}
Mart\'{\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
  Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Murray,
  Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
  Yu, and Xiaoqiang Zheng.
\newblock Tensor{F}low: A system for large-scale machine learning.
\newblock In \emph{OSDI}, 2016.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{arjovsky2017wgan}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Barratt \& Sharma(2018)Barratt and Sharma]{barratt2018note}
Shane Barratt and Rishi Sharma.
\newblock A note on the {I}nception {S}core.
\newblock In \emph{arXiv preprint arXiv:1801.01973}, 2018.

\bibitem[Bellemare et~al.(2017)Bellemare, Danihelka, Dabney, Mohamed,
  Lakshminarayanan, Hoyer, and Munos]{bellemare2017cramergan}
Marc~G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji
  Lakshminarayanan, Stephan Hoyer, and R{\'{e}}mi Munos.
\newblock The {C}ramer distance as a solution to biased {W}asserstein
  gradients.
\newblock In \emph{arXiv preprint arXiv:1705.10743}, 2017.

\bibitem[Bi{\'n}kowski et~al.(2018)Bi{\'n}kowski, Sutherland, Arbel, and
  Gretton]{bińkowski2018demystifying}
Mikolaj Bi{\'n}kowski, Dougal~J. Sutherland, Michael Arbel, and Arthur Gretton.
\newblock Demystifying {MMD} {GAN}s.
\newblock In \emph{ICLR}, 2018.

\bibitem[Brock et~al.(2017)Brock, Lim, Ritchie, and Weston]{brock2017photo}
Andrew Brock, Theodore Lim, J.M. Ritchie, and Nick Weston.
\newblock Neural photo editing with introspective adversarial networks.
\newblock In \emph{ICLR}, 2017.

\bibitem[Chen et~al.(2016)Chen, Duan, Houthooft, Schulman, Sutskever, and
  Abbeel]{chen2016infogan}
Xi~Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In \emph{NIPS}, 2016.

\bibitem[de~Vries et~al.(2017)de~Vries, Strub, Mary, Larochelle, Pietquin, and
  Courville]{devries2017modulating}
Harm de~Vries, Florian Strub, J{\'e}r{\'e}mie Mary, Hugo Larochelle, Olivier
  Pietquin, and Aaron Courville.
\newblock Modulating early visual processing by language.
\newblock In \emph{NIPS}, 2017.

\bibitem[Denton et~al.(2015)Denton, Chintala, Szlam, and
  Fergus]{denton2015lapgan}
Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus.
\newblock Deep generative image models using a laplacian pyramid of adversarial
  networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Dumoulin et~al.(2017)Dumoulin, Shlens, and
  Kudlur]{dumoulin2017artistic}
Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
\newblock A learned representation for artistic style.
\newblock In \emph{ICLR}, 2017.

\bibitem[Fedus et~al.(2018)Fedus, Rosca, Lakshminarayanan, Dai, Mohamed, and
  Goodfellow]{fedus2018many}
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew~M. Dai, Shakir
  Mohamed, and Ian Goodfellow.
\newblock Many paths to equilibrium: {GAN}s do not need to decrease a
  divergence at every step.
\newblock In \emph{ICLR}, 2018.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010init}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{AISTATS}, 2010.

\bibitem[Golub \& der Vorst(2000)Golub and der Vorst]{golub2000eigenvalue}
Gene Golub and Henk~Van der Vorst.
\newblock Eigenvalue computation in the 20th century.
\newblock \emph{Journal of Computational and Applied Mathematics},
  123:\penalty0 35--65, 2000.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, and Bengio]{goodfellow2014gans}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, and Aaron Courville~Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{NIPS}, 2014.

\bibitem[Google(2018)]{tpu}
Google.
\newblock {Cloud TPUs}.
\newblock \url{https://cloud.google.com/tpu/}, 2018.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Mart{\'{\i}}n Arjovsky, Vincent Dumoulin, and
  Aaron~C. Courville.
\newblock Improved training of {W}asserstein {GAN}s.
\newblock In \emph{NIPS}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnets}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, Klambauer,
  and Hochreiter]{heusel2017ttur}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
  G{\"{u}}nter Klambauer, and Sepp Hochreiter.
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In \emph{NIPS}, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and
  Lehtinen]{karras2018progan}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kodali et~al.(2017)Kodali, Abernethy, Hays, and
  Kira]{kodali2014dragan}
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira.
\newblock On convergence and stability of {GAN}s.
\newblock In \emph{arXiv preprint arXiv:1705.07215}, 2017.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{krizhevsky2009cifar}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lim \& Ye(2017)Lim and Ye]{lim2017geometric}
Jae~Hyun Lim and Jong~Chul Ye.
\newblock Geometric {GAN}.
\newblock In \emph{arXiv preprint arXiv:1705.02894}, 2017.

\bibitem[Mao et~al.(2016)Mao, Li, Xie, Lau, and Wang]{mao2016lsgan}
Xudong Mao, Qing Li, Haoran Xie, Raymond Y.~K. Lau, and Zhen Wang.
\newblock Least squares generative adversarial networks.
\newblock In \emph{arXiv preprint arXiv:1611.04076}, 2016.

\bibitem[Marchesi(2016)]{marchesi2017megapixel}
Marco Marchesi.
\newblock Megapixel size image creation using generative adversarial networks.
\newblock In \emph{arXiv preprint arXiv:1706.00082}, 2016.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{mescheder2018r1gp}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for {GAN}s do actually converge?
\newblock In \emph{ICML}, 2018.

\bibitem[Mirza \& Osindero(2014)Mirza and Osindero]{mirza2014conditional}
Mehdi Mirza and Simon Osindero.
\newblock Conditional generative adversarial nets.
\newblock In \emph{arXiv preprint arXiv:1411.1784}, 2014.

\bibitem[Miyato \& Koyama(2018)Miyato and Koyama]{miyato2018cgans}
Takeru Miyato and Masanori Koyama.
\newblock c{GAN}s with projection discriminator.
\newblock In \emph{ICLR}, 2018.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{nowozin2016fgan}
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.
\newblock f-{GAN}: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{NIPS}, 2016.

\bibitem[Odena et~al.(2016)Odena, Dumoulin, and Olah]{odena2016deconvolution}
Augustus Odena, Vincent Dumoulin, and Chris Olah.
\newblock Deconvolution and checkerboard artifacts.
\newblock \emph{Distill}, 2016.

\bibitem[Odena et~al.(2017)Odena, Olah, and Shlens]{odena2017acgan}
Augustus Odena, Christopher Olah, and Jonathon Shlens.
\newblock Conditional image synthesis with auxiliary classifier {GAN}s.
\newblock In \emph{ICML}, 2017.

\bibitem[Odena et~al.(2018)Odena, Buckman, Olsson, Brown, Olah, Raffel, and
  Goodfellow]{odena2018causal}
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom~B. Brown, Christopher
  Olah, Colin Raffel, and Ian Goodfellow.
\newblock Is generator conditioning causally related to {GAN} performance?
\newblock In \emph{ICML}, 2018.

\bibitem[Perez et~al.(2018)Perez, Strub, de~Vries, Dumoulin, and
  Courville]{perez2018film}
Ethan Perez, Florian Strub, Harm de~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock {FiLM}: Visual reasoning with a general conditioning layer.
\newblock In \emph{AAAI}, 2018.

\bibitem[Pieters \& Wiering(2014)Pieters and Wiering]{pieters2018bachelors}
Mathijs Pieters and Marco Wiering.
\newblock Comparing generative adversarial network techniques for image
  creation and modificatio.
\newblock In \emph{arXiv preprint arXiv:1803.09093}, 2014.

\bibitem[Radford et~al.(2016)Radford, Metz, and Chintala.]{radford2016dcgan}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In \emph{ICLR}, 2016.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, and Bernstein]{ILSVRC2015}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein.
\newblock Image{N}et large scale visual recognition challenge.
\newblock \emph{IJCV}, 115:\penalty0 211--252, 2015.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weightnorm}
Tim Salimans and Diederik Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{NIPS}, 2016.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training {GAN}s.
\newblock In \emph{NIPS}, 2016.

\bibitem[Salimans et~al.(2018)Salimans, Zhang, Radford, and
  Metaxas]{salimans2016otgan}
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas.
\newblock Improving {GAN}s using optimal transport.
\newblock In \emph{ICLR}, 2018.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2014ortho}
Andrew Saxe, James McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock In \emph{ICLR}, 2014.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan15very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[S{\o}nderby et~al.(2017)S{\o}nderby, Caballero, Theis, Shi, and
  Huszár]{sonderby2017map}
Casper~Kaae S{\o}nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc
  Huszár.
\newblock Amortised map inference for image super-resolution.
\newblock In \emph{ICLR}, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 15:\penalty0 1929--1958, 2014.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun17revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{ICCV}, 2017.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2015rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and
  Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Theis et~al.(2015)Theis, van~den Oord, and Bethge]{theis2015note}
Lucas Theis, A{\"{a}}ron van~den Oord, and Matthias Bethge.
\newblock A note on the evaluation of generative models.
\newblock In \emph{arXiv preprint arXiv:1511.01844}, 2015.

\bibitem[Tran et~al.(2017)Tran, Ranganath, and Blei]{tran2017hierarchical}
Dustin Tran, Rajesh Ranganath, and David~M. Blei.
\newblock Hierarchical implicit models and likelihood-free variational
  inference.
\newblock In \emph{NIPS}, 2017.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018nonlocal}
Xiaolong Wang, Ross~B. Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Wu et~al.(2017)Wu, Burda, Salakhutdinov, and Grosse]{wu2017ais}
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger~B. Grosse.
\newblock On the quantitative analysis of decoder-based generative models.
\newblock In \emph{ICLR}, 2017.

\bibitem[Yazıcı et~al.(2018)Yazıcı, Foo, Winkler, Yap, Piliouras, and
  Chandrasekhar]{yazici2018ema}
Yasin Yazıcı, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios
  Piliouras, and Vijay Chandrasekhar.
\newblock The unusual effectiveness of averaging in gan training.
\newblock In \emph{arXiv preprint arXiv:1806.04498}, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Goodfellow, Metaxas, and
  Odena]{zhang2018sagan}
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.
\newblock Self-attention generative adversarial networks.
\newblock In \emph{arXiv preprint arXiv:1805.08318}, 2018.

\end{thebibliography}
