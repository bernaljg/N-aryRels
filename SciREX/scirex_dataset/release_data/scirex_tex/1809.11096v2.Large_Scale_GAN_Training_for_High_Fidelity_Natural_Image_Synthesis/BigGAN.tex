\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{listings}
\usepackage{url}
\usepackage{soul}
\usepackage{pifont} % Source of checkmark and red X
\usepackage{graphicx} % For figures
\usepackage{caption} % For subfigures
\usepackage{subcaption} % For the architectural tables
\usepackage{booktabs} % For the architecture tables
\usepackage{multirow}
\usepackage{comment} % For commenting out blocks
\usepackage{fmtcount} % For counting appendices
\usepackage[titletoc,title]{appendix}

\usepackage{algorithmic} % For algorithms
\usepackage{algorithm} % For algorithms


\title{Large Scale GAN Training for\\ High Fidelity Natural Image Synthesis}

\author{Andrew Brock\thanks{Work done at DeepMind} \ \thanks{Equal contribution} \\
Heriot-Watt University\\
\texttt{ajb5@hw.ac.uk} \\
\And
Jeff Donahue\footnotemark[2] \\
DeepMind \\
\texttt{jeffdonahue@google.com} \\
\And
Karen Simonyan\footnotemark[2] \\
DeepMind \\
\texttt{simonyan@google.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bmb}{{\bm b}}
\newcommand{\bms}{{\bm s}}
\newcommand{\bmu}{{\bm u}}
\newcommand{\bmv}{{\bm v}}
\newcommand{\bmw}{{\bm w}}
\newcommand{\bmx}{{\bm x}}
\newcommand{\bmh}{{\bm h}}
\newcommand{\bmy}{{\bm y}}
\newcommand{\bmz}{{\bm z}}
\newcommand{\bmm}{{\bm m}}
\newcommand{\bmSigma}{{\bm \Sigma}}
\newcommand{\bmLambda}{{\bm \Lambda}}
\newcommand{\bmxi}{{\bm \xi}}
\newcommand{\bmzero}{{\bm 0}}
\newcommand{\bmg}{{\bm g}}
\newcommand{\bmeta}{{\bm \eta}}
\newcommand{\bmdelta}{{\bm \delta}}
\newcommand{\bmgamma}{{\bm \gamma}}
\newcommand{\bmmu}{{\bm \mu}}
\newcommand{\bmphi}{{\bm \phi}}
\newcommand{\set}[1]{\{#1\}}
%\newcommand{\E}{\mathop{\mathrm{E}}}

% Subfigure
\newcommand{\subf}[2]{%
  {\small\begin{tabular}[b]{@{}c@{}}
  #1\\#2
  \end{tabular}}%
}

% Checkmark and red X
\definecolor{mygreen}{rgb}{0.032, 0.6392, 0.2039}
\newcommand{\cmark}{\textcolor{mygreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}%

% Generator and Discriminator Symbols
\newcommand{\gen}{\textbf{\texttt{G}}}
\newcommand{\discr}{\textbf{\texttt{D}}}

% "phantom" zero to align numbers in tables
\newcommand{\phz}{\phantom{0}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ``truncation trick,'' allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128$\times$128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Fr\'echet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.


\end{abstract}


\section{Introduction} 
\label{intro}

\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cccc}
\includegraphics[width=0.24\textwidth]{images/samples0/dog3.jpg} & 
\includegraphics[width=0.24\textwidth]{images/samples0/landscape0.jpg} &
\includegraphics[width=0.24\textwidth]{images/samples0/Monarch0.jpg} & 
\includegraphics[width=0.24\textwidth]{images/samples0/Burger0.jpg} 
\end{tabular}
\caption{Class-conditional samples generated by our model.}
\label{samples0}
\end{figure}


The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, \citet{goodfellow2014gans}) at the forefront of efforts to generate high-fidelity, diverse images with models learned directly from data. GAN training is dynamic, and sensitive to nearly every aspect of its setup (from optimization parameters to model architecture), but a torrent of research has yielded empirical and theoretical insights enabling stable training in a variety of settings. Despite this progress, the current state of the art in conditional ImageNet modeling \citep{zhang2018sagan} achieves an Inception Score \citep{salimans2016improved} of 52.5, compared to 233 for real data.

In this work, we set out to close the gap in fidelity and variety between images generated by GANs and real-world images from the ImageNet dataset. We make the following three contributions towards this goal:

\begin{itemize}
  \item We demonstrate that GANs benefit dramatically from scaling, and train models with two to four times as many parameters and eight times the batch size compared to prior art. We introduce two simple, general architectural changes that improve scalability, and modify a regularization scheme to improve conditioning, demonstrably boosting performance.

  \item As a side effect of our modifications, our models become amenable to the ``truncation trick,'' a simple sampling technique that allows explicit, fine-grained control of the trade-off between sample variety and fidelity.

  \item We discover instabilities specific to large scale GANs, and characterize them empirically. Leveraging insights from this analysis, we demonstrate that a combination of novel and existing techniques can reduce these instabilities, but complete training stability can only be achieved at a dramatic cost to performance.

  
  
\end{itemize}

Our modifications substantially improve class-conditional GANs. When trained on ImageNet at 128$\times$128 resolution, our models (BigGANs) improve the state-of-the-art Inception Score (IS) and Fr\'echet Inception Distance (FID) from 52.52 and 18.65 to 166.5 and 7.4 respectively.
We also successfully train BigGANs on ImageNet at 256$\times$256 and 512$\times$512 resolution, and achieve IS and FID of 232.5 and 8.1 at 256$\times$256 and IS and FID of 241.5 and 11.5 at 512$\times$512. Finally, we train our models on an even larger
dataset -- JFT-300M --
%internal dataset,
and demonstrate that our design choices transfer well from ImageNet. Code and weights for our pretrained generators are publicly available 
% \textcolor{blue}{\href{https://tfhub.dev/s?q=biggan}{here.}}
\footnote{\scriptsize \url{https://tfhub.dev/s?q=biggan}}.


\section{Background} 
\label{background}

A Generative Adversarial Network (GAN) involves Generator (\gen{}) and Discriminator (\discr{}) networks whose purpose, respectively, is to map random noise to samples and discriminate real and generated samples. Formally, the GAN objective, in its original form \citep{goodfellow2014gans} involves finding a Nash equilibrium to the following two player min-max problem:

\begin{align}
	\min_{G} \max_{D} \E_{x\sim q_{\rm data}({\bm x})} [ \log D({\bm x})] +  \E_{{\bm z}\sim p({\bm z})} [\log(1-D(G({\bm z})))], \label{eq:advloss}
\end{align}

where $\bmz \in \bbR^{d_z}$ is a latent variable drawn from distribution $p(\bmz)$ such as $\mathcal{N}(0, I)$ or $\mathcal{U}[-1, 1]$. When applied to images, \gen{} and \discr{} are usually convolutional neural networks \citep{radford2016dcgan}. Without auxiliary stabilization techniques, this training procedure is notoriously brittle, requiring finely-tuned hyperparameters and architectural choices to work at all. 

Much recent research has accordingly focused on modifications to the vanilla GAN procedure to impart stability, drawing on a growing body of empirical and theoretical insights \citep{nowozin2016fgan, sonderby2017map, fedus2018many}. One line of work is focused on changing the objective function \citep{arjovsky2017wgan, mao2016lsgan, lim2017geometric, bellemare2017cramergan, salimans2016otgan}  to encourage convergence. Another line is focused on constraining \discr{} through gradient penalties \citep{gulrajani2017improved, kodali2014dragan, mescheder2018r1gp} or normalization \citep{miyato2018spectral}, both to counteract the use of unbounded loss functions and ensure \discr{} provides gradients everywhere to \gen{}. 

Of particular relevance to our work is Spectral Normalization \citep{miyato2018spectral}, which enforces Lipschitz continuity on \discr{} by normalizing its parameters with running estimates of their first singular values, inducing backwards dynamics that adaptively regularize the top singular direction. Relatedly \citet{odena2018causal} analyze the condition number of the Jacobian of \gen{} and find that performance is dependent on \gen{}'s conditioning. \citet{zhang2018sagan} find that employing Spectral Normalization in \gen{} improves stability, allowing for fewer \discr{} steps per iteration. We extend on these analyses to gain further insight into the pathology of GAN training.




Other works focus on the choice of architecture, such as SA-GAN \citep{zhang2018sagan} which adds the self-attention block from \citep{wang2018nonlocal} to improve the ability of both \gen{} and \discr{} to model global structure. ProGAN \citep{karras2018progan} trains high-resolution GANs in the single-class setting by training a single model across a sequence of increasing resolutions.

In conditional GANs \citep{mirza2014conditional} class information can be fed into the model in various ways.
In \citep{odena2017acgan} it is provided to \gen{} by concatenating a 1-hot class vector to the noise vector, and the objective is modified to encourage conditional samples to maximize the corresponding class probability predicted by an auxiliary classifier. \citet{devries2017modulating} and \citet{dumoulin2017artistic} modify the way class conditioning is passed to \gen{} by supplying it with class-conditional gains and biases in BatchNorm \citep{ioffe2015batchnorm} layers. In \cite{miyato2018cgans}, \discr{} is conditioned by using the cosine similarity between its features and a set of learned class embeddings as additional evidence for distinguishing real and generated samples, effectively encouraging generation of samples whose features match a learned class prototype.


Objectively evaluating implicit generative models is difficult \citep{theis2015note}. A variety of works have proposed heuristics for measuring the sample quality of models without tractable likelihoods \citep{salimans2016improved, heusel2017ttur, bi≈Ñkowski2018demystifying, wu2017ais}. Of these, the Inception Score (IS, \citet{salimans2016improved}) and Fr\'echet Inception Distance (FID, \citet{heusel2017ttur}) have become popular despite their 
notable flaws \citep{barratt2018note}. We employ them as approximate measures of sample quality, and to enable comparison against previous work.



\section{Scaling Up GANs}
\input{ablation_table}



In this section, we explore methods for scaling up GAN training to reap the performance benefits of larger models and larger batches. As a baseline, we employ the SA-GAN architecture of \citet{zhang2018sagan}, which uses the hinge loss \citep{lim2017geometric, tran2017hierarchical} GAN objective. We provide class information to \gen{} with class-conditional BatchNorm \citep{dumoulin2017artistic, devries2017modulating} and to \discr{} with projection \citep{miyato2018cgans}. The optimization settings follow \citet{zhang2018sagan} (notably employing Spectral Norm in \gen{}) with the modification that we halve the learning rates and take two \discr{} steps per \gen{} step.  For evaluation, we employ moving averages of \gen{}'s weights following \citet{karras2018progan, mescheder2018r1gp, yazici2018ema}, with a decay of $0.9999$. We use Orthogonal Initialization \citep{saxe2014ortho}, whereas previous works used $\mathcal{N}(0,0.02I)$ \citep{radford2016dcgan} or Xavier initialization \citep{glorot2010init}. Each model is trained on 128 to 512 cores of a Google TPUv3
Pod~\citep{tpu}, and computes BatchNorm statistics in \gen{} across all devices, rather than per-device as is typical. We find progressive growing \citep{karras2018progan} unnecessary even for our 512$\times$512 models. Additional details are in Appendix~\ref{appendix_experimental_details}.

We begin by increasing the batch size for the baseline model, and immediately find tremendous benefits in doing so. Rows 1-4 of Table~\ref{ablation_table} show that simply increasing the batch size by a factor of 8 improves the state-of-the-art IS by 46\%. We conjecture that this is a result of each batch covering more modes, providing better gradients for both networks. One notable side effect of this scaling is that our models reach better final performance in fewer iterations, but become unstable and undergo complete training collapse. We discuss the causes and ramifications of this in Section~\ref{sec:analysis}. For these experiments, we report scores from checkpoints saved just before collapse.

We then increase the width (number of channels) in each layer by 50\%, approximately doubling the number of parameters in both models. This leads to a further IS improvement of 21\%, which we posit is due to the increased capacity of the model relative to the complexity of the dataset. 
% Doubling the depth does not appear to have the same effect on ImageNet models, instead degrading performance.
Doubling the depth did not initially lead to improvement -- we addressed this later in the BigGAN-deep model, which uses a different residual block structure.


We note that class embeddings $c$ used for the conditional BatchNorm layers in \gen{} contain a large number of weights. Instead of having a separate layer for each embedding~\citep{miyato2018spectral,zhang2018sagan}, we opt to use a shared embedding, which is linearly projected to each layer's gains and biases~\citep{perez2018film}. This reduces computation and memory costs, and  improves training speed (in number of iterations required to reach a given performance) by 37\%.
Next, we 
% employ a variant of hierarchical latent spaces, where the noise vector $z$ is fed into multiple layers of \gen{} rather than just the initial layer.
add direct skip connections (skip-$z$) from the noise vector $z$ to multiple layers of \gen{} rather than just the initial layer.
The intuition behind this design is to allow \gen{} to use the latent space to directly influence features at different resolutions and levels of hierarchy. 
% For our architecture, this is easily accomplished by splitting $z$ into one chunk per resolution, and concatenating each chunk to the conditional vector $c$ which gets projected to the BatchNorm gains and biases.
In BigGAN, this is accomplished by splitting $z$ into one chunk per resolution, and concatenating each chunk to the conditional vector $c$ which gets projected to the BatchNorm gains and biases.
In BigGAN-deep, we use an even simpler design, concatenating the entire $z$ with the conditional vector without splitting it into chunks.
Previous works \citep{goodfellow2014gans, denton2015lapgan} have considered variants of this concept; our implementation is a minor modification of this design. 
% Hierarchical latents improve memory and compute costs (primarily by reducing the parametric budget of the first linear layer), 
Skip-$z$ provides a modest performance improvement of around 4\%, and improves training speed  by a further 18\%.  



\subsection{Trading off variety and fidelity with the Truncation Trick}
\label{subsec_truncation}


\begin{figure}[tbp]
  \centering
  \begin{tabular}{cc}
  \subf{
  \setlength{\tabcolsep}{1pt}
  \begin{tabular}{cccc}
  \includegraphics[width=0.18\textwidth]{images/trunc_figure2/Truncfigure1full5.jpg} & 
\includegraphics[width=0.18\textwidth]{images/trunc_figure2/Truncfigure1full3.jpg} &
\includegraphics[width=0.18\textwidth]{images/trunc_figure2/Truncfigure1full2.jpg} & 
\includegraphics[width=0.18\textwidth]{images/trunc_figure2/Truncfigure1full1.jpg}
 

 
\end{tabular}
}{(a)}

&
 \subf{\includegraphics[width=0.18\textwidth]{images/rainbowdogs0.jpg}}{(b)}
  
 \end{tabular}
\caption{(a) The effects of increasing truncation. From left to right, the threshold is set to 2, 1, 0.5, 0.04. (b) Saturation artifacts from applying truncation to a poorly conditioned model.}

\label{trunc_figure}
\end{figure}


Unlike models which need to backpropagate through their latents, GANs can employ an arbitrary prior $p(z)$, yet the vast majority of previous works have chosen to draw $z$ from either $\mathcal{N}(0, I)$ or $\mathcal{U}[-1, 1]$. We question the optimality of this choice and explore alternatives in Appendix~\ref{appendix_latents}. 

Remarkably, our best results come from using a different latent distribution for sampling than was used in training. Taking a model trained with $z\sim\mathcal{N}(0, I)$ and sampling $z$ from a \textit{truncated normal} (where values which fall outside a range are resampled to fall inside that range) immediately provides a boost to IS and FID. We call this the \textit{Truncation Trick}: truncating a $z$ vector by resampling the values with magnitude above a chosen threshold leads to improvement in individual sample quality at the cost of reduction in overall sample variety. Figure~\ref{trunc_figure}(a) demonstrates this: as the threshold is reduced, and elements of $z$ are truncated towards zero (the mode of the latent distribution), individual samples approach the mode of \gen{}'s output distribution. Related observations about this trade-off were made in \citep{marchesi2017megapixel, pieters2018bachelors}.

This technique allows fine-grained, post-hoc selection of the trade-off between sample quality and variety for a given \gen{}. Notably, we can compute FID and IS for a range of thresholds, obtaining the variety-fidelity curve reminiscent of the precision-recall curve (Figure~\ref{appendix_ISvFID128}). As IS does not penalize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct increase in IS (analogous to precision). FID penalizes lack of variety (analogous to recall) but also rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches zero and variety diminishes, the FID sharply drops. The distribution shift caused by sampling with different latents than those seen in training is problematic for many models. Some of our larger models are not amenable to truncation, producing saturation artifacts (Figure~\ref{trunc_figure}(b)) when fed truncated noise. To counteract this, we seek to enforce amenability to truncation by conditioning \gen{} to be smooth, so that the full space of $z$ will map to good output samples. For this, we turn to Orthogonal Regularization \citep{brock2017photo}, which directly enforces the orthogonality condition:

\begin{equation}
    R_\beta(W) = \beta\|W^\top W - I\|_{\mathrm{F}}^2,
\end{equation}




where $W$ is a weight matrix and $\beta$ a hyperparameter. This regularization is known to often be too limiting \citep{miyato2018spectral}, so we explore several variants designed to relax the constraint while still imparting the desired smoothness to our models. The version we find to work best removes the diagonal terms from the regularization, and aims to minimize the pairwise cosine similarity between filters but does not constrain their norm:


\begin{equation}
\label{eq:ortho3}
    R_\beta(W) = \beta\|W^\top W \odot (\mathbf{1} - I)\|_{\mathrm{F}}^2,
\end{equation}
where $\mathbf{1}$ denotes a matrix with all elements set to $1$.
We sweep $\beta$ values and select $10^{-4}$, finding this small added penalty sufficient to improve the likelihood that our models will be amenable to truncation. Across runs in Table~\ref{ablation_table}, we observe that without Orthogonal Regularization, only 16\% of models are amenable to truncation, compared to 60\% when trained with Orthogonal Regularization.

%Across runs in Table~\ref{ablation_table}, we observe that without Orthogonal Regularization, only 16\% of models are amenable to truncation, compared to 60\% when trained with Orthogonal Regularization.



\subsection{Summary}
We find that current GAN techniques are sufficient to enable scaling to large models and distributed, large-batch training. We find that we can  dramatically improve the state of the art and train models up to 512$\times$512 resolution without need for explicit multiscale methods like \cite{karras2018progan}. Despite these improvements, our models undergo training collapse, necessitating early stopping in practice. In the next two sections we investigate why settings which were stable in previous works become unstable when applied at scale. 


\section{Analysis}
\label{sec:analysis}

 
\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cc}

\subf{\includegraphics[width=0.48\textwidth]{images/GSV0_newa.jpg}}{(a) \gen{}} &
\subf{\includegraphics[width=0.48\textwidth]{images/DSV0_newa.jpg}}{(b) \discr{}}
\end{tabular}
\caption{A typical plot of the first singular value $\sigma_0$ in the layers of \gen{} (a) and \discr{} (b) before Spectral Normalization. Most layers in \gen{} have well-behaved spectra, but without constraints a small subset grow throughout training and explode at collapse. \discr{}'s spectra are noisier but otherwise better-behaved. Colors from red to violet indicate increasing depth.}
\label{spectra}
\end{figure}


\subsection{Characterizing Instability: The Generator}
\label{subsec:gen_instability}
Much previous work has investigated GAN stability from a variety of analytical angles and on toy problems, but the instabilities we observe occur for settings which are stable at small scale, necessitating direct analysis at large scale. 
We monitor a range of weight, gradient, and loss statistics during training, in search of a metric which might presage the onset of training collapse, similar to \citep{odena2018causal}. We found the top three singular values
$\sigma_0, \sigma_1, \sigma_2$ of each weight matrix to be the most informative.
They can be efficiently computed using the Alrnoldi iteration method~\citep{golub2000eigenvalue}, which extends the power iteration method, used in~\citet{miyato2018spectral}, to estimation of additional singular vectors and values. A clear pattern emerges, as can be seen in Figure~\ref{spectra}(a) and Appendix~\ref{appendix_monitored_stats}: most \gen{} layers have well-behaved spectral norms, but some layers (typically the first layer in \gen{}, which is over-complete and not convolutional) are ill-behaved, with spectral norms that grow throughout training and explode at collapse.



To ascertain if this pathology is a cause of collapse or merely a symptom, we study the effects of imposing additional conditioning on \gen{} to explicitly counteract spectral explosion. First, we directly regularize the top singular values $\sigma_0$ of each weight, either towards a fixed value $\sigma_{reg}$ or towards some ratio $r$ of the second singular value, $r \cdot sg(\sigma_1)$ (with $sg$ the stop-gradient operation to prevent the regularization from increasing $\sigma_1$). Alternatively, we employ a partial singular value decomposition to instead clamp $\sigma_0$.  Given a weight $W$, its first singular vectors $u_0$ and $v_0$, and $\sigma_{clamp}$ the value to which the $\sigma_0$ will be clamped, our weights become:
\begin{equation}
    W = W - \max(0, \sigma_0 - \sigma_{clamp}) v_0 u_0^\top,
\end{equation}
where $\sigma_{clamp}$ is set to either $\sigma_{reg}$ or $r \cdot sg(\sigma_1)$. We observe that both with and without Spectral Normalization these techniques have the effect of preventing the gradual increase and explosion of either $\sigma_0$ or $\frac{\sigma_0}{\sigma_1}$, but even though in some cases they mildly improve performance, no combination prevents training collapse. This evidence suggests that while conditioning \gen{} might improve stability, it is insufficient to ensure stability. We accordingly turn our attention to \discr{}.



\subsection{Characterizing Instability: The Discriminator}
\label{subsec:discr_instability}

As with \gen{}, we analyze the spectra of \discr{}'s weights to gain insight into its behavior, then seek to stabilize training by imposing additional constraints. Figure~\ref{spectra}(b)  displays a typical plot of $\sigma_0$ for \discr{} (with further plots in Appendix~\ref{appendix_monitored_stats}). Unlike \gen{}, we see that the spectra are noisy, $\frac{\sigma_0}{\sigma_1}$ is well-behaved, and the singular values grow throughout training but only jump at collapse, instead of exploding.

The spikes in \discr{}'s spectra might suggest that it periodically receives very large gradients, but we observe that the Frobenius norms are smooth (Appendix~\ref{appendix_monitored_stats}), suggesting that this effect is primarily concentrated on the top few singular directions. We posit that this noise is a result of optimization through the adversarial training process, where \gen{} periodically produces batches which strongly perturb \discr{} . If this spectral noise is causally related to instability, a natural counter is to employ gradient penalties, which explicitly regularize changes in \discr{}'s Jacobian. We explore the $R_1$ zero-centered gradient penalty from \citet{mescheder2018r1gp}:
\begin{equation}
\label{eq:discriminator}
R_1:=
\frac{\gamma}{2} \E_{ p_{\mathcal D}(x)}
\left[
\|\nabla D(x)\|_F^2
\right].
\end{equation}

With the default suggested $\gamma$ strength of 10, training becomes stable and improves the smoothness and boundedness of spectra in both \gen{} and \discr{}, but performance severely degrades, resulting in a 45\% reduction in IS. Reducing the penalty partially alleviates this degradation, but results in increasingly ill-behaved spectra; even with the penalty strength reduced to $1$ (the lowest strength for which sudden collapse does not occur) the IS is reduced by 20\%.
Repeating this experiment with various strengths of Orthogonal Regularization, DropOut \citep{srivastava2014dropout}, and L2 (See Appendix~\ref{appendix_sweeps} for details), reveals similar behaviors for these regularization strategies: with high enough penalties on \discr{}, training stability can be achieved, but at a substantial cost to performance.



We also observe that \discr{}'s loss approaches zero during training, but undergoes a sharp upward jump at collapse (Appendix~\ref{appendix_monitored_stats}).
One possible explanation for this behavior is that \discr{} is overfitting to the training set, memorizing training examples rather than learning some meaningful boundary between real and generated images.  As a simple test for \discr{}'s memorization (related to \cite{gulrajani2017improved}), we evaluate uncollapsed discriminators on the ImageNet training and validation sets, and measure what percentage of samples are classified as real or generated. While the training accuracy is consistently above 98\%, the validation accuracy falls in the range of 50-55\%, no better than random guessing (regardless of regularization strategy). This confirms that \discr{} 
is indeed memorizing the training set;
we deem this in line with \discr{}'s role, which is not explicitly to generalize, but to distill the training data and provide a useful learning signal for \gen{}. Additional experiments and discussion are provided in Appendix~\ref{appendix_additional_discussion}.



\subsection{Summary}
We find that stability does not come solely from \gen{} or \discr{}, but from their interaction through the adversarial training process. While the symptoms of their poor conditioning can be used to track and identify instability, ensuring reasonable conditioning proves necessary for training but insufficient to prevent eventual training collapse. It is possible to enforce stability by strongly constraining \discr{}, but doing so incurs a dramatic cost in performance. With current techniques, better final performance can be achieved by relaxing this conditioning and allowing collapse to occur at the later stages of training, by which time a model is sufficiently trained to achieve good results.

\section{Experiments}
\begin{figure}[htbp]
\centering
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cccc}
\subf{
\includegraphics[width=0.24\textwidth]{images/samples1/thatchroof0.jpg}}{(a) 128$\times$128} & 
\subf{
\includegraphics[width=0.24\textwidth]{images/samples1/1760464_bird0.jpg}}{(b) 256$\times$256} &
\subf{
\includegraphics[width=0.24\textwidth]{images/samples1/512fox0.jpg}
}{(c) 512$\times$512} &
\subf{\includegraphics[width=0.24\textwidth]{images/badsamples/TennisBallDog.png}}{(d)}



\end{tabular}
\caption{Samples from our BigGAN model with truncation threshold 0.5 (a-c) and an example of class leakage in a partially trained model (d).}
\label{results_samples}
\end{figure}

\input{results_table}
\subsection{Evaluation on ImageNet}
We evaluate our models on ImageNet ILSVRC 2012~\citep{ILSVRC2015} at 128$\times$128, 256$\times$256, and 512$\times$512 resolutions, employing the settings from Table~\ref{ablation_table}, row 8. 
The samples generated by our models are presented in Figure~\ref{results_samples}, with additional samples in Appendix~\ref{appendix_samples}, and online
\footnote{\scriptsize \url{https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002}}.
We report IS and FID in Table~\ref{results_table}. As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we accordingly report values at three settings, with complete curves in Appendix~\ref{appendix_additional_plots}. First, we report the FID/IS values at the truncation setting which attains the best FID. Second, we report the FID at the truncation setting for which our model's IS is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of ``objectness.''  Third, we report FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS and FID scores achieved by \citet{miyato2018spectral} and \cite{zhang2018sagan}.

In addition to the BigGAN model introduced in the first version of the paper and used in the majority of experiments (unless otherwise stated), we also present a 4x deeper model (BigGAN-deep) which uses a different configuration of residual blocks. As can be seen from Table~\ref{results_table}, BigGAN-deep substantially outperforms BigGAN across all resolutions and metrics. This confirms that our findings extend to other architectures, and that increased depth leads to improvement in sample quality.
Both BigGAN and BigGAN-deep architectures are described in Appendix~\ref{appendix_architecture}.

Our observation that \discr{} overfits to the training set, coupled with our model's sample quality, raises the obvious question of whether or not \gen{} simply memorizes training points. To test this, we perform class-wise nearest neighbors analysis in pixel space and the feature space of pre-trained classifier networks (Appendix~\ref{appendix_samples}). In addition, we present both interpolations between samples and class-wise interpolations (where $z$ is held constant) in Figures~\ref{appendix_ZCinterp} and \ref{appendix_Cinterp}. Our model convincingly interpolates between disparate samples, and the nearest neighbors for its samples are visually distinct, suggesting that our model does not simply memorize training data.

We note that some failure modes of our partially-trained models are distinct from those previously observed. Most previous failures involve local artifacts \citep{odena2016deconvolution}, images consisting of texture blobs instead of  objects \citep{salimans2016improved}, or the canonical mode collapse. We observe \textit{class leakage}, where images from one class contain properties of another, as exemplified by Figure~\ref{results_samples}(d).  We also find that many classes on ImageNet are more difficult than others for our model; our model is more successful at generating dogs (which make up a large portion of the dataset, and are mostly distinguished by their texture) than crowds  (which comprise a small portion of the dataset and have more large-scale structure). Further discussion is available in Appendix~\ref{appendix_samples}.

\begin{comment}  % ANONYMOUS VERSION
\subsection{Additional evaluation on an internal dataset}
To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on an internal dataset.
The dataset contains 292M images labeled with 8.5K categories -- two orders of magnitude larger than ImageNet. For images with multiple labels, we sample a single label randomly and independently whenever an image is sampled.
To compute IS and FID for the GANs trained on this dataset, we use an Inception v2 classifier~\citep{szegedy2015rethinking} trained on this dataset.
Quantitative results are presented in Table~\ref{jft_table}.
All models are trained with batch size 2048.
We compare an ablated version of our model --
comparable to SA-GAN~\citep{zhang2018sagan} but with the larger batch size --
against a ``full'' version that makes uses of all of the techniques applied to obtain the best results on ImageNet (shared embedding, hierarchical latents, and orthogonal regularization).
Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels).
We further show that for a dataset of this scale, we see significant additional improvements from expanding the capacity of our models to 128 base channels, while for ImageNet GANs that additional capacity was not beneficial.
 
In Figure~\ref{appendix_jft_trunc} (Appendix~\ref{appendix_additional_plots}), we present truncation plots for models trained on this dataset.
Unlike for ImageNet, where truncation limits of $\sigma\approx0$ tend to produce the highest fidelity scores, here IS is typically maximized when the truncation value $\sigma$ ranges from 0.5 to 1.
We suspect that this is at least partially due to the dataset's intra-class variability, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales.
Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization (Section~\ref{sec:analysis}), the models trained on the larger dataset remain stable over many hundreds of thousands of iterations.
This suggests that moving beyond ImageNet to larger datasets may partially alleviate GAN stability issues.

The improvement over the baseline GAN model that we achieve on this dataset
without changes to the underlying models or training and regularization techniques (beyond expanded capacity) demonstrates that
our findings extend from ImageNet
to datasets with scale and complexity thus far unprecedented for generative models of images.
\end{comment}

%%%% BEGIN NON-ANONYMOUS VERSION
%\begin{comment}
\input{jft_table}
\subsection{Additional evaluation on JFT-300M}
To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M~\citep{sun17revisiting}.
The full JFT-300M dataset contains 300M real-world images labeled with 18K categories.
Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels.
The resulting dataset contains 292M images -- two orders of magnitude larger than ImageNet. For images with multiple labels, we sample a single label randomly and independently whenever an image is sampled.
To compute IS and FID for the GANs trained on this dataset, we use an Inception v2 classifier~\citep{szegedy2015rethinking} trained on this dataset.
Quantitative results are presented in Table~\ref{jft_table}.
All models are trained with batch size 2048.
We compare an ablated version of our model --
comparable to SA-GAN~\citep{zhang2018sagan} but with the larger batch size --
against a ``full'' BigGAN model that makes uses of all of the techniques applied to obtain the best results on ImageNet (shared embedding, skip-$z$, and orthogonal regularization).
Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels).
We further show that for a dataset of this scale, we see significant additional improvements from expanding the capacity of our models to 128 base channels, while for ImageNet GANs that additional capacity was not beneficial.
 
In Figure~\ref{appendix_jft_trunc} (Appendix~\ref{appendix_additional_plots}), we present truncation plots for models trained on this dataset.
Unlike for ImageNet, where truncation limits of $\sigma\approx0$ tend to produce the highest fidelity scores, IS is typically maximized for our JFT-300M models when the truncation value $\sigma$ ranges from 0.5 to 1.
We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales.
Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization (Section~\ref{sec:analysis}), the models trained on JFT-300M remain stable over many hundreds of thousands of iterations.
This suggests that moving beyond ImageNet to larger datasets may partially alleviate GAN stability issues.

The improvement over the baseline GAN model that we achieve on this dataset
without changes to the underlying models or training and regularization techniques (beyond expanded capacity) demonstrates that
our findings extend from ImageNet
to datasets with scale and complexity thus far unprecedented for generative models of images.
%\end{comment}
%%%% END NON-ANONYMOUS VERSION



\section{Conclusion}
We have demonstrated that Generative Adversarial Networks trained to model natural images of multiple categories highly benefit from scaling up, both in terms of fidelity and variety of the generated samples. As a result, our models set a new level of performance among ImageNet GAN models, improving on the state of the art by a large margin.
We have also presented an analysis of the training behavior of large scale GANs, characterized their stability in terms of the singular values of their weights, and discussed the interplay between stability and performance.


\subsubsection*{Acknowledgments}
We would like to thank 
Kai Arulkumaran, Matthias Bauer, Peter Buchlovsky, Jeffrey Defauw, Sander Dieleman, Ian Goodfellow, Ariel Gordon, Karol Gregor, Dominik Grewe, Chris Jones, Jacob Menick, Augustus Odena, Suman Ravuri, Ali Razavi, Mihaela Rosca, and Jeff Stanway.

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\begin{appendices}

\newpage
\section{Additional Samples, Interpolations, and Nearest Neighbors from ImageNet models}
\label{appendix_samples}
\input{Appendix_Samples_Nearest_Interp}


\newpage
\section{Architectural details}
\label{appendix_architecture}
\input{Appendix_Architecture}

\newpage
\section{Experimental Details}
\label{appendix_experimental_details}
\input{Appendix_Experimental_Details}

\newpage
\section{Additional Plots}
\label{appendix_additional_plots}
\input{Appendix_Additional_Plots}
\clearpage

\newpage
\section{Choosing Latent Spaces}
\label{appendix_latents}
\input{Appendix_Latents}




\newpage
\section{Monitored Training Statistics}
\label{appendix_monitored_stats}
\input{Appendix_MonitoredStats}

\newpage
\section{Additional Discussion: Stability and Collapse}
\label{appendix_additional_discussion}
\input{Appendix_Additional_Discussion.tex}


\newpage
\section{Negative Results}
\label{appendix_negative_results}
\input{Appendix_Negative_Results}


\newpage
\section{Hyperparameters}
\label{appendix_sweeps}
\input{Appendix_Sweeps}

\end{appendices}

\end{document}
