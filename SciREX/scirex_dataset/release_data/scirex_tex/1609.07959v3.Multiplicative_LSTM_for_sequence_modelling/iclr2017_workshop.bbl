\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{Bengio-1994}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{Neural Networks, IEEE Transactions on}, 5\penalty0
  (2):\penalty0 157--166, 1994.

\bibitem[Chung et~al.(2017)Chung, Ahn, and Bengio]{chung2017}
J.~Chung, S.~Ahn, and Y.~Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock \emph{ICLR}, 2017.

\bibitem[Cooijmans et~al.(2017)Cooijmans, Ballas, Laurent, and
  Courville]{cooijmans2017}
Tim Cooijmans, Nicolas Ballas, C{\'e}sar Laurent, and Aaron Courville.
\newblock Recurrent batch normalization.
\newblock \emph{ICLR}, 2017.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016}
Yarin Gal and Zoubin Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1019--1027, 2016.

\bibitem[Grave et~al.(2017)Grave, Joulin, and Usunier]{grave2017}
Edouard Grave, Armand Joulin, and Nicolas Usunier.
\newblock Improving neural language models with a continuous cache.
\newblock \emph{ICLR}, 2017.

\bibitem[Graves(2013)]{Graves-2013}
A.~Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1308.0850}, 2013.

\bibitem[Ha et~al.(2017)Ha, Dai, and Lee]{Ha2017}
D.~Ha, A.~Dai, and Q.~Lee.
\newblock Hypernetworks.
\newblock \emph{ICLR}, 2017.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{Hochreiter-1997}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9:\penalty0 1735--1780, 1997.

\bibitem[Hutter(2012)]{Hutter2012}
M.~Hutter.
\newblock The human knowledge compression contest.
\newblock \emph{URL http://prize.hutter1.net}, 2012.

\bibitem[Inan et~al.(2017)Inan, Khosravi, and Socher]{inan2017}
Hakan Inan, Khashayar Khosravi, and Richard Socher.
\newblock Tying word vectors and word classifiers: A loss framework for
  language modeling.
\newblock \emph{ICLR}, 2017.

\bibitem[Kalchbrenner et~al.(2016)Kalchbrenner, Espeholt, Simonyan, Oord,
  Graves, and Kavukcuoglu]{Kalchbrenner2016}
N.~Kalchbrenner, L.~Espeholt, K.~Simonyan, A.~Oord, A.~Graves, and
  K.~Kavukcuoglu.
\newblock Neural machine translation in linear time.
\newblock \emph{arXiv preprint arXiv:1610.10099}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krause et~al.(2017)Krause, Kahembwe, Murray, and Renals]{krause2017}
Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals.
\newblock Dynamic evaluation of neural sequence models.
\newblock \emph{arXiv preprint arXiv:1709.07432}, 2017.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and Santorini]{marcus1993}
M.~P. Marcus, M.~A. Marcinkiewicz, and B.~Santorini.
\newblock Building a large annotated corpus of {English}: The {Penn Treebank}.
\newblock \emph{Computational linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017}
G{\'a}bor Melis, Chris Dyer, and Phil Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2017{\natexlab{a}})Merity, Keskar, and
  Socher]{merity2017}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing lstm language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017{\natexlab{a}}.

\bibitem[Merity et~al.(2017{\natexlab{b}})Merity, Xiong, Bradbury, and
  Socher]{Merity2016}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{ICLR}, 2017{\natexlab{b}}.

\bibitem[Mikolov et~al.(2012)Mikolov, Sutskever, Deoras, Le, Kombrink, and
  Cernocky]{mikolov2012c}
T.~Mikolov, I.~Sutskever, A.~Deoras, H.~Le, S.~Kombrink, and J.~Cernocky.
\newblock Subword language modeling with neural networks.
\newblock \emph{preprint (http://www.fit.vutbr.cz/~imikolov/rnnlm/char.pdf)},
  2012.

\bibitem[Mujika et~al.(2017)Mujika, Meier, and Steger]{mujika2017}
Asier Mujika, Florian Meier, and Angelika Steger.
\newblock Fast-slow recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1705.08639}, 2017.

\bibitem[Pachitariu \& Sahani(2013)Pachitariu and Sahani]{pachitariu2013}
M.~Pachitariu and M.~Sahani.
\newblock Regularization and nonlinearities for neural language models: when
  are they needed?
\newblock \emph{arXiv preprint arXiv:1301.5650}, 2013.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{pascanu2013}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1310--1318, 2013.

\bibitem[Press \& Wolf(2017)Press and Wolf]{press2017}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock \emph{EACL 2017}, pp.\  157, 2017.

\bibitem[Radford et~al.(2017)Radford, Jozefowicz, and Sutskever]{radford2017}
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
\newblock Learning to generate reviews and discovering sentiment.
\newblock \emph{arXiv preprint arXiv:1704.01444}, 2017.

\bibitem[Rocki(2016{\natexlab{a}})]{rocki2016}
K.~Rocki.
\newblock Recurrent memory array structures.
\newblock \emph{arXiv preprint arXiv:1607.03085}, 2016{\natexlab{a}}.

\bibitem[Rocki(2016{\natexlab{b}})]{rocki2016b}
K.~Rocki.
\newblock Surprisal-driven feedback in recurrent networks.
\newblock \emph{arXiv preprint arXiv:1608.06027}, 2016{\natexlab{b}}.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016}
Tim Salimans and Diederik~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  901--909, 2016.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sutskever et~al.(2011)Sutskever, Martens, and Hinton]{Sutskever-2011}
I.~Sutskever, J.~Martens, and G.~E. Hinton.
\newblock Generating text with recurrent neural networks.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pp.\  1017--1024, 2011.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012}
T.~Tieleman and G.~E. Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 4\penalty0
  (2), 2012.

\bibitem[Wu et~al.(2016)Wu, Zhang, Zhang, Bengio, and Salakhutdinov]{wu2016}
Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan~R
  Salakhutdinov.
\newblock On multiplicative integration with recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2856--2864, 2016.

\bibitem[Zhang et~al.(2016)Zhang, Wu, Che, Lin, Memisevic, Salakhutdinov, and
  Bengio]{zhang2016}
S.~Zhang, Y.~Wu, T.~Che, Z.~Lin, R.~Memisevic, R.~Salakhutdinov, and Y.~Bengio.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock In \emph{NIPS}, 2016.
\newblock arXiv preprint arXiv:1602.08210.

\bibitem[Zilly et~al.(2017)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber]{zilly2017}
J.~G. Zilly, R.~K. Srivastava, J.~Koutn{\'\i}k, and J.~Schmidhuber.
\newblock Recurrent highway networks.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}
