\section{ Related Work}
\label{sec:related}
In this section, we briefly cover the most representative techniques for the two problems tackled in this paper.

\mypar{Visual object tracking.}
Arguably, until very recently, the most popular paradigm for tracking arbitrary objects has been to train online a discriminative classifier exclusively from the ground-truth information provided in the first frame of a video (and then update it online).

In the past few years, the Correlation Filter, a simple algorithm that allows to discriminate between the template of an arbitrary target and its 2D translations, rose to prominence as particularly fast and effective strategy for tracking-by-detection thanks to the pioneering work of Bolme \etal~\cite{bolme2010visual}.
Performance of Correlation Filter-based trackers has then been notably improved with the adoption of multi-channel formulations~\cite{kiani2013multi,henriques2015tracking}, spatial constraints~\cite{kiani2015correlation,danelljan2015learning,lukezic2017discriminative,li2018learning} and deep features (\eg~\cite{danelljan2017eco,valmadre2017end}). 

Recently, a radically different approach has been introduced~\cite{bertinetto2016fully,held2016learning,tao2016siamese}. 
Instead of learning a discrimative classifier online, these methods train offline a similarity function on pairs of video frames.
At test time, this function can be simply evaluated on a new video, once per frame.
In particular, evolutions of the fully-convolutional Siamese approach~\cite{bertinetto2016fully} considerably improved tracking performance by making use of region proposals~\cite{SiamRPN}, hard negative mining~\cite{zhu2018distractor}, ensembling~\cite{he2018towards} and memory networks~\cite{yang2018learning}.

Most modern trackers, including all the ones mentioned above, use a rectangular bounding box both to initialise the target \emph{and} to estimate its position in the subsequent frames.
Despite its convenience, a simple rectangle often fails to properly represent an object, as it is evident in the examples of Figure~\ref{fig:video_page1}.
This motivated us to propose a tracker able to produce binary segmentation masks while still only relying on a bounding box initialisation.

Interestingly, in the past it was not uncommon for trackers to produce a coarse binary mask of the target object (\eg~\cite{comaniciu2000real,perez2002color}).
However, to the best of our knowledge, the only recent tracker that, like ours, is able to operate online and produce a binary mask starting from a bounding box initialisation is the superpixel-based approach of Yeo \etal~\cite{yeo2017superpixel}.
However, at 4 frames per seconds (fps), its fastest variant is significantly slower than our proposal.
Furthermore, when using CNN features, its speed is affected by a 60-fold decrease, plummeting below 0.1 fps.
Finally, it has not demonstrated to be competitive on modern tracking or VOS benchmarks.
Similar to us, the methods of Perazzi \etal~\cite{perazzi2017learning} and Ci \etal~\cite{ci2018video} can also start from a rectangle and output per-frame masks.
However, they require fine-tuning at test time, which makes them slow.

\mypar{Semi-supervised video object segmentation.}
Benchmarks for arbitrary object tracking (\eg~\cite{smeulders2014visual,kristan2016visual,wu2013online}) assume that trackers receive input frames in a sequential fashion.
This aspect is generally referred to with the attributes \emph{online} or \emph{causal}~\cite{kristan2016visual}.
Moreover, methods are often focused on achieving a speed that exceeds the ones of typical video framerates~\cite{VOT2018}.
Conversely, semi-supervised VOS algorithms have been traditionally more concerned with an accurate representation of the object of interest~\cite{perazzi2017video,perazzi2016benchmark}.

In order to exploit consistency between video frames, several methods propagate the supervisory segmentation mask of the first frame to the temporally adjacent ones via graph labeling approaches (\eg~\cite{wen2015jots,perazzi2015fully,tsai2016video,marki2016bilateral,bao2018cnn}). 
In particular, Bao \etal~\cite{bao2018cnn} recently proposed a very accurate method that makes use of a spatio-temporal MRF in which temporal dependencies are modelled by optical flow, while spatial dependencies are expressed by a CNN.

Another popular strategy is to process video frames independently (\eg~\cite{maninis2017video,perazzi2017learning,voigtlaender2017online}), similarly to what happens in most tracking approaches.
For example, in OSVOS-S Maninis \etal~\cite{maninis2017video} do not make use of any temporal information.
They rely on a fully-convolutional network pre-trained for classification and then, at test time, they fine-tune it using the ground-truth mask provided in the first frame.
MaskTrack~\cite{perazzi2017learning} instead is trained from scratch on individual images, but it does exploit some form of temporality at test time by using the latest mask prediction and optical flow as additional input to the network.

Aiming towards the highest possible accuracy, at test time VOS methods often feature computationally intensive techniques such as fine-tuning~\cite{maninis2017video,perazzi2017learning,bao2018cnn,voigtlaender2017online}, data augmentation~\cite{LucidDataDreaming_CVPR17_workshops,li2018video} and optical flow~\cite{tsai2016video,bao2018cnn,perazzi2017learning,li2018video,cheng2018fast}.
Therefore, these approaches are generally characterised by low framerates and the inability to operate online.
For example, it is not uncommon for methods to require minutes~\cite{perazzi2017learning,cheng2017segflow} or even hours~\cite{tsai2016video,bao2018cnn} for videos that are just a few seconds long, like the ones of DAVIS.

Recently, there has been an increasing interest in the VOS community towards \emph{faster} methods~\cite{marki2016bilateral,wug2018fast,cheng2018fast,chen2018blazingly,jampani2017video,hu2018videomatch}.
To the best of our knowledge, the fastest approaches with a performance competitive with the state of the art are the ones of Yang \etal~\cite{Yang_2018_CVPR} and Wug \etal~\cite{wug2018fast}.
The former uses a meta-network ``modulator'' to quickly adapt the parameters of a segmentation network during test time, while the latter does not use any fine-tuning and adopts an encoder-decoder Siamese architecture trained in multiple stages.
Both these methods run below 10 frames per second, while we are more than six times faster and only rely on a bounding box initialisation.
