\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{torch7}
R.~Collobert, K.~Kavukcuoglu, and C.~Farabet.
\newblock Torch7: A matlab-like environment for machine learning.
\newblock In {\em BigLearn, NIPS Workshop}, 2011.

\bibitem{decaf}
J.~Donahue, Y.~Jia, O.~Vinyals, J.~Hoffman, N.~Zhang, E.~Tzeng, and T.~Darrell.
\newblock Decaf: A deep convolutional activation feature for generic visual
  recognition.
\newblock In {\em ICML}, 2014.

\bibitem{rcnn}
R.~Girshick, J.~Donahue, T.~Darrell, and J.~Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In {\em CVPR}, 2014.

\bibitem{fracc}
B.~Graham.
\newblock Fractional max-pooling.
\newblock {\em arXiv preprint arXiv:1412.6071}, 2014.

\bibitem{torchblog}
S.~Gross and M.~Wilber.
\newblock Training and investigating residual nets.
\newblock 2016.
\newblock http://torch.ch/blog/2016/02/04/resnets.html.

\bibitem{prelu}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em ICCV}, 2015.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{preresnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em ECCV}, 2016.

\bibitem{densenet}
G.~Huang, Z.~Liu, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock {\em arXiv preprint arXiv:1608.06993}, 2016.

\bibitem{stochasticdepth}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em ECCV}, 2016.

\bibitem{BN}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{cifar}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock In {\em Tech Report}, 2009.

\bibitem{alexnet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock {ImageNet Classification with Deep Convolutional Neural Networks}.
\newblock In {\em NIPS}, 2012.

\bibitem{fractalnet}
G.~Larsson, M.~Maire, and G.~Shakhnarovich.
\newblock Fractalnet: Ultra-deep neural networks without residuals.
\newblock {\em arXiv preprint arXiv:1605.07648}, 2016.

\bibitem{backprop}
Y.~LeCun, B.~Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~Hubbard, and
  L.~D. Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1(4):541--551, 1989.

\bibitem{Lenet}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{DSN}
C.-Y. Lee, S.~Xie, P.~Gallagher, Z.~Zhang, and Z.~Tu.
\newblock Deeply-supervised nets.
\newblock In {\em AISTATS}, 2015.

\bibitem{NiN}
M.~Lin, Q.~Chen, and S.~Yan.
\newblock Network in network.
\newblock In {\em ICLR}, 2014.

\bibitem{FCN}
J.~Long, E.~Shelhamer, and T.~Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em CVPR}, 2015.

\bibitem{ReLU}
V.~Nair and G.~E. Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em ICML}, 2010.

\bibitem{fitnet}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock In {\em ICLR}, 2015.

\bibitem{ImageNet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{overfeat}
P.~Sermanet, D.~Eigen, X.~Zhang, M.~Mathieu, R.~Fergus, and Y.~LeCun.
\newblock Overfeat: Integrated recognition, localization and detection using
  convolutional networks.
\newblock In {\em ICLR}, 2014.

\bibitem{weightedresnet}
F.~Shen and G.~Zeng.
\newblock Weighted residuals for very deep networks.
\newblock {\em arXiv preprint arXiv:1605.08831}, 2016.

\bibitem{VGG}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em ICLR}, 2015.

\bibitem{swapout}
S.~Singh, D.~Hoiem, and D.~Forsyth.
\newblock Swapout: Learning an ensemble of deep architectures.
\newblock In {\em NIPS}, 2016.

\bibitem{allcnn}
J.~T. Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock In {\em ICLR Workshop}, 2015.

\bibitem{dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15:1929--1958, 2014.

\bibitem{Highway}
R.~K. Srivastava, K.~Greff, and J.~Schmidhuber.
\newblock Training very deep networks.
\newblock In {\em NIPS}, 2015.

\bibitem{InceptionResnet}
C.~Szegedy, S.~Ioffe, and V.~Vanhoucke.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock In {\em ICLR Workshop}, 2016.

\bibitem{GoogleNet}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em CVPR}, 2015.

\bibitem{Inceptionv3}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em CVPR}, 2016.

\bibitem{ensemble}
A.~Veit, M.~Wilber, and S.~Belongie.
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock In {\em NIPS}, 2016.

\bibitem{wideresnet}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock In {\em BMVC}, 2016.

\bibitem{zfnet}
M.~D. Zeiler and R.~Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em ECCV}, 2014.

\end{thebibliography}
