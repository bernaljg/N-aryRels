\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{pretrain}
Y.~Bengio, P.~Lamblin, D.~Popovici, H.~Larochelle, U.~D. Montréal, and
  M.~Québec.
\newblock Greedy layer-wise training of deep networks.
\newblock In {\em NIPS}, 2007.

\bibitem{theano}
J.~Bergstra, O.~Breuleux, F.~Bastien, P.~Lamblin, R.~Pascanu, G.~Desjardins,
  J.~Turian, D.~Warde-Farley, and Y.~Bengio.
\newblock Theano: a {CPU} and {GPU} math expression compiler.
\newblock In {\em Proceedings of the Python for Scientific Computing Conference
  ({SciPy})}, June 2010.

\bibitem{Bottou98}
L.~Bottou.
\newblock Online algorithms and stochastic approximations.
\newblock {\em Cambridge University Press}, 1998.

\bibitem{Dahl12}
G.~E. Dahl, D.~Yu, L.~Deng, and A.~Acero.
\newblock Context-dependent pre-trained deep neural networks for
  large-vocabulary speech recognition.
\newblock {\em IEEE Tran. on Audio, Speech, and Lang. Proc.}, 20(1):30--42,
  2012.

\bibitem{DeCAF}
J.~Donahue, Y.~Jia, O.~Vinyals, J.~Hoffman, N.~Zhang, E.~Tzeng, and T.~Darrell.
\newblock Decaf: A deep convolutional activation feature for generic visual
  recognition.
\newblock In {\em arXiv}, 2013.

\bibitem{Eigen14}
D.~Eigen, J.~Rolfe, R.~Fergus, and Y.~LeCun.
\newblock Understanding deep architectures using a recursive convolutional
  network.
\newblock In {\em arXiv:1312.1847v2}, 2014.

\bibitem{Elman91}
J.~L. Elman.
\newblock Distributed representations, simple recurrent networks, and
  grammatical.
\newblock {\em Machine Learning}, 7:195--225, 1991.

\bibitem{Glorot10}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em AISTAT}, 2010.

\bibitem{maxout}
I.~J. Goodfellow, D.~Warde-Farley, M.~Mirza, A.~C. Courville, and Y.~Bengio.
\newblock Maxout networks.
\newblock In {\em ICML}, 2013.

\bibitem{Hinton06}
G.~E. Hinton, S.~Osindero, and Y.~W. Teh.
\newblock A fast learning algorithm for deep belief nets.
\newblock {\em Neural computation}, 18:1527--1554, 2006.

\bibitem{dropout}
G.~E. Hinton, N.~Srivastava, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock In {\em CoRR, abs/1207.0580}, 2012.

\bibitem{CNN}
F.~J. Huang and Y.~LeCun.
\newblock Large-scale learning with svm and convolutional for generic object
  categorization.
\newblock In {\em CVPR}, 2006.

\bibitem{objREC}
K.~Jarrett, K.~Kavukcuoglu, M.~Ranzato, and Y.~LeCun.
\newblock What is the best multi-stage architecture for object recognition?
\newblock In {\em ICCV}, 2009.

\bibitem{caffe}
Y.~Jia.
\newblock {Caffe}: An open source convolutional architecture for fast feature
  embedding.
\newblock \url{http://caffe.berkeleyvision.org/}, 2013.

\bibitem{imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em NIPS}, 2012.

\bibitem{Le10}
Q.~Le, J.~Ngiam, Z.~Chen, D.~Chia, P.~W. Koh, and A.~Ng.
\newblock Tiled convolutional neural networks.
\newblock In {\em NIPS}, 2010.

\bibitem{Lecun98}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock In {\em Proceedings of the IEEE}, 1998.

\bibitem{Lee09}
H.~Lee, R.~Grosse, R.~Ranganath, and A.~Y. Ng.
\newblock Convolutional deep belief networks for scalable unsupervised learning
  of hierarchical representations.
\newblock In {\em ICML}, 2009.

\bibitem{dropcon}
W.~Li, M.~Zeiler, S.~Zhang, Y.~LeCun, and R.~Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In {\em ICML}, 2013.

\bibitem{NIN}
M.~Lin, Q.~Chen, and S.~Yan.
\newblock Network in network.
\newblock In {\em ICLR}, 2014.

\bibitem{Loh13}
P.-L. Loh and M.~J. Wainwright.
\newblock Regularized m-estimators with nonconvexity : statistical and
  algorithmic theory for local optima.
\newblock In {\em arXiv:1305.2436v1}, 2013.

\bibitem{Pascanu14}
R.~Pascanu, T.~Mikolov, and Y.~Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em arXiv:1211.5063v2}, 2014.

\bibitem{Rakhlin12}
A.~Rakhlin, O.~Shamir, and K.~Sridharan.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In {\em ICML}, 2012.

\bibitem{mulCOL}
J.~Schmidhuber.
\newblock Multi-column deep neural networks for image classification.
\newblock In {\em CVPR}, 2012.

\bibitem{Shamir13}
O.~Shamir and T.~Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In {\em ICML}, 2013.

\bibitem{Snoek12}
J.~Snoek, R.~P. Adams, and H.~Larochelle.
\newblock Nonparametric guidance of autoencoder representations using label
  information.
\newblock {\em J. of Machine Learning Research}, 13:2567--2588, 2012.

\bibitem{tree}
N.~Srivastava and R.~Salakhutdinov.
\newblock Discriminative transfer learning with tree-based priors.
\newblock In {\em NIPS}, 2013.

\bibitem{Tang13}
Y.~Tang.
\newblock Deep learning using linear support vector machines.
\newblock In {\em Workshop on Representational Learning, ICML}, 2013.

\bibitem{vapnik}
V.~N. Vapnik.
\newblock The nature of statistical learning theory.
\newblock In {\em Springer, New York}, 1995.

\bibitem{Weston12}
J.~Weston and F.~Ratle.
\newblock Deep learning via semi-supervised embedding.
\newblock In {\em ICML}, 2008.

\bibitem{Zeiler13}
M.~Zeiler and R.~Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em arXiv 1311.2901}, 2013.

\bibitem{SPool}
M.~D. Zeiler and R.~Fergus.
\newblock Stochastic pooling for regularization of deep convolutional neural
  networks.
\newblock In {\em ICLR}, 2013.

\end{thebibliography}
