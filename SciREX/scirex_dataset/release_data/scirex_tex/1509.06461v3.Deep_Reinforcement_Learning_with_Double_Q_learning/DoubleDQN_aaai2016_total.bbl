\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal(1995)]{Agrawal:1995}
R.~Agrawal.
\newblock Sample mean based index policies with {O}(log n) regret for the
  multi-armed bandit problem.
\newblock \emph{Advances in Applied Probability}, pages 1054--1078, 1995.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{Auer:2002}
P.~Auer, N.~Cesa-Bianchi, and P.~Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2-3):\penalty0 235--256, 2002.

\bibitem[Baird(1995)]{Baird:1995}
L.~Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning: Proceedings of the Twelfth International
  Conference}, pages 30--37, 1995.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare:2013}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{J. Artif. Intell. Res. {(JAIR)}}, 47:\penalty0 253--279, 2013.

\bibitem[Brafman and Tennenholtz(2003)]{Brafman:2003}
R.~I. Brafman and M.~Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{The Journal of Machine Learning Research}, 3:\penalty0
  213--231, 2003.

\bibitem[Fukushima(1988)]{Fukushima:1988}
K.~Fukushima.
\newblock Neocognitron: A hierarchical neural network capable of visual pattern
  recognition.
\newblock \emph{Neural networks}, 1\penalty0 (2):\penalty0 119--130, 1988.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and Moore]{Kaelbling:1996}
L.~P. Kaelbling, M.~L. Littman, and A.~W. Moore.
\newblock Reinforcement learning: A survey.
\newblock \emph{Journal of Artificial Intelligence Research}, 4:\penalty0
  237--285, 1996.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{Lecun:1998}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lin(1992)]{Lin:1992}
L.~Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 293--321, 1992.

\bibitem[Maei(2011)]{Maei:2011}
H.~R. Maei.
\newblock \emph{Gradient temporal-difference learning algorithms}.
\newblock PhD thesis, University of Alberta, 2011.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih:2015}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, S.~Petersen,
  C.~Beattie, A.~Sadik, I.~Antonoglou, H.~King, D.~Kumaran, D.~Wierstra,
  S.~Legg, and D.~Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nair et~al.(2015)Nair, Srinivasan, Blackwell, Alcicek, Fearon, Maria,
  Panneershelvam, Suleyman, Beattie, Petersen, Legg, Mnih, Kavukcuoglu, and
  Silver]{Nair:2015}
A.~Nair, P.~Srinivasan, S.~Blackwell, C.~Alcicek, R.~Fearon, A.~D. Maria,
  V.~Panneershelvam, M.~Suleyman, C.~Beattie, S.~Petersen, S.~Legg, V.~Mnih,
  K.~Kavukcuoglu, and D.~Silver.
\newblock Massively parallel methods for deep reinforcement learning.
\newblock In \emph{Deep Learning Workshop, ICML}, 2015.

\bibitem[Riedmiller(2005)]{Riedmiller:2005}
M.~Riedmiller.
\newblock Neural fitted {Q} iteration - first experiences with a data efficient
  neural reinforcement learning method.
\newblock In J.~Gama, R.~Camacho, P.~Brazdil, A.~Jorge, and L.~Torgo, editors,
  \emph{Proceedings of the 16th European Conference on Machine Learning
  (ECML'05)}, pages 317--328. Springer, 2005.

\bibitem[Sallans and Hinton(2004)]{Sallans:2004}
B.~Sallans and G.~E. Hinton.
\newblock Reinforcement learning with factored states and actions.
\newblock \emph{The Journal of Machine Learning Research}, 5:\penalty0
  1063--1088, 2004.

\bibitem[Strehl et~al.(2009)Strehl, Li, and Littman]{Strehl:2009}
A.~L. Strehl, L.~Li, and M.~L. Littman.
\newblock Reinforcement learning in finite {MDP}s: {PAC} analysis.
\newblock \emph{The Journal of Machine Learning Research}, 10:\penalty0
  2413--2444, 2009.

\bibitem[Sutton(1988)]{Sutton:1988}
R.~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton(1990)]{Sutton:1990}
R.~S. Sutton.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In \emph{Proceedings of the seventh international conference on
  machine learning}, pages 216--224, 1990.

\bibitem[Sutton and Barto(1998)]{SuttonBarto:1998}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Introduction to reinforcement learning}.
\newblock MIT Press, 1998.

\bibitem[Sutton et~al.(2008)Sutton, Szepesv{\'a}ri, and Maei]{Sutton:2008}
R.~S. Sutton, C.~Szepesv{\'a}ri, and H.~R. Maei.
\newblock A convergent {O}(n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock \emph{Advances in Neural Information Processing Systems 21
  (NIPS-08)}, 21:\penalty0 1609--1616, 2008.

\bibitem[Sutton et~al.(2015)Sutton, Mahmood, and White]{Sutton:2015}
R.~S. Sutton, A.~R. Mahmood, and M.~White.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{arXiv preprint arXiv:1503.04269}, 2015.

\bibitem[Szita and L{\H{o}}rincz(2008)]{Szita:2008}
I.~Szita and A.~L{\H{o}}rincz.
\newblock The many faces of optimism: a unifying approach.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 1048--1055. ACM, 2008.

\bibitem[Tesauro(1995)]{Tesauro:1995}
G.~Tesauro.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Communications of the ACM}, 38\penalty0 (3):\penalty0 58--68,
  1995.

\bibitem[Thrun and Schwartz(1993)]{Thrun:1993}
S.~Thrun and A.~Schwartz.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In M.~Mozer, P.~Smolensky, D.~Touretzky, J.~Elman, and A.~Weigend,
  editors, \emph{Proceedings of the 1993 Connectionist Models Summer School},
  Hillsdale, NJ, 1993. Lawrence Erlbaum.

\bibitem[Tsitsiklis and {Van Roy}(1997)]{Tsitsiklis:1997}
J.~N. Tsitsiklis and B.~{Van Roy}.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{{IEEE} Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.

\bibitem[{van Hasselt}(2010)]{vanHasselt:2010}
H.~{van Hasselt}.
\newblock Double {Q}-learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  23:\penalty0 2613--2621, 2010.

\bibitem[{van Hasselt}(2011)]{vanHasselt:2011}
H.~{van Hasselt}.
\newblock \emph{Insights in Reinforcement Learning}.
\newblock PhD thesis, Utrecht University, 2011.

\bibitem[Watkins(1989)]{Watkins:1989}
C.~J. C.~H. Watkins.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, University of Cambridge England, 1989.

\end{thebibliography}
