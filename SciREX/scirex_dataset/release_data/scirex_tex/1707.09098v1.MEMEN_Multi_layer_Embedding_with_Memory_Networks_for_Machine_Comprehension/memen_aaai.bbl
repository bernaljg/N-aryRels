\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Chen \bgroup \em et al.\egroup
  }{2016}]{chen2016thorough}
Danqi Chen, Jason Bolton, and Christopher~D Manning.
\newblock A thorough examination of the cnn/daily mail reading comprehension
  task.
\newblock {\em arXiv preprint arXiv:1606.02858}, 2016.

\bibitem[\protect\citeauthoryear{Cui \bgroup \em et al.\egroup
  }{2016}]{cui2016attention}
Yiming Cui, Zhipeng Chen, Si~Wei, Shijin Wang, Ting Liu, and Guoping Hu.
\newblock Attention-over-attention neural networks for reading comprehension.
\newblock {\em arXiv preprint arXiv:1607.04423}, 2016.

\bibitem[\protect\citeauthoryear{Dhingra \bgroup \em et al.\egroup
  }{2016}]{dhingra2016gated}
Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William~W Cohen, and Ruslan
  Salakhutdinov.
\newblock Gated-attention readers for text comprehension.
\newblock {\em arXiv preprint arXiv:1606.01549}, 2016.

\bibitem[\protect\citeauthoryear{Hermann \bgroup \em et al.\egroup
  }{2015}]{hermann2015teaching}
Karl~Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will
  Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1693--1701, 2015.

\bibitem[\protect\citeauthoryear{Hill \bgroup \em et al.\egroup
  }{2015}]{hill2015goldilocks}
Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston.
\newblock The goldilocks principle: Reading children's books with explicit
  memory representations.
\newblock {\em arXiv preprint arXiv:1511.02301}, 2015.

\bibitem[\protect\citeauthoryear{Hu \bgroup \em et al.\egroup
  }{2017}]{hu2017mnemonic}
Minghao Hu, Yuxing Peng, and Xipeng Qiu.
\newblock Mnemonic reader for machine comprehension.
\newblock {\em arXiv preprint arXiv:1705.02798}, 2017.

\bibitem[\protect\citeauthoryear{Joshi \bgroup \em et al.\egroup
  }{2017}]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock {\em arXiv preprint arXiv:1705.03551}, 2017.

\bibitem[\protect\citeauthoryear{Kadlec \bgroup \em et al.\egroup
  }{2016}]{kadlec2016text}
Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst.
\newblock Text understanding with the attention sum reader network.
\newblock {\em arXiv preprint arXiv:1603.01547}, 2016.

\bibitem[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup
  }{2017}]{liu2017structural}
Rui Liu, Junjie Hu, Wei Wei, Zi~Yang, and Eric Nyberg.
\newblock Structural embedding of syntactic trees for machine comprehension.
\newblock {\em arXiv preprint arXiv:1703.00572}, 2017.

\bibitem[\protect\citeauthoryear{Mikolov \bgroup \em et al.\egroup
  }{2010}]{mikolov2010recurrent}
Tomas Mikolov, Martin Karafi{\'a}t, Lukas Burget, Jan Cernock{\`y}, and Sanjeev
  Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In {\em Interspeech}, volume~2, page~3, 2010.

\bibitem[\protect\citeauthoryear{Mikolov \bgroup \em et al.\egroup
  }{2013}]{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg~S Corrado, and Jeff Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In {\em Advances in neural information processing systems}, pages
  3111--3119, 2013.

\bibitem[\protect\citeauthoryear{Onishi \bgroup \em et al.\egroup
  }{2016}]{onishi2016did}
Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester.
\newblock Who did what: A large-scale person-centered cloze dataset.
\newblock {\em arXiv preprint arXiv:1608.05457}, 2016.

\bibitem[\protect\citeauthoryear{Paperno \bgroup \em et al.\egroup
  }{2016}]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock {\em arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[\protect\citeauthoryear{Pennington \bgroup \em et al.\egroup
  }{2014}]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher~D. Manning.
\newblock Glove: Global vectors for word representation.
\newblock In {\em Empirical Methods in Natural Language Processing (EMNLP)},
  pages 1532--1543, 2014.

\bibitem[\protect\citeauthoryear{Rajpurkar \bgroup \em et al.\egroup
  }{2016}]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock {\em arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[\protect\citeauthoryear{Richardson \bgroup \em et al.\egroup
  }{2013}]{richardson2013mctest}
Matthew Richardson, Christopher~JC Burges, and Erin Renshaw.
\newblock Mctest: A challenge dataset for the open-domain machine comprehension
  of text.
\newblock In {\em EMNLP}, volume~3, page~4, 2013.

\bibitem[\protect\citeauthoryear{Seo \bgroup \em et al.\egroup
  }{2016}]{seo2016bidirectional}
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi.
\newblock Bidirectional attention flow for machine comprehension.
\newblock {\em arXiv preprint arXiv:1611.01603}, 2016.

\bibitem[\protect\citeauthoryear{Shen \bgroup \em et al.\egroup
  }{2016}]{shen2016reasonet}
Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen.
\newblock Reasonet: Learning to stop reading in machine comprehension.
\newblock {\em arXiv preprint arXiv:1609.05284}, 2016.

\bibitem[\protect\citeauthoryear{Srivastava \bgroup \em et al.\egroup
  }{2014}]{srivastava2014dropout}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(1):1929--1958, 2014.

\bibitem[\protect\citeauthoryear{Sukhbaatar \bgroup \em et al.\egroup
  }{2015}]{sukhbaatar2015end}
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et~al.
\newblock End-to-end memory networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2440--2448, 2015.

\bibitem[\protect\citeauthoryear{Vinyals \bgroup \em et al.\egroup
  }{2015}]{vinyals2015pointer}
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
\newblock Pointer networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2692--2700, 2015.

\bibitem[\protect\citeauthoryear{Wang and Jiang}{2016}]{wang2016machine}
Shuohang Wang and Jing Jiang.
\newblock Machine comprehension using match-lstm and answer pointer.
\newblock {\em arXiv preprint arXiv:1608.07905}, 2016.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup
  }{2016}]{wang2016multi}
Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu Florian.
\newblock Multi-perspective context matching for machine comprehension.
\newblock {\em arXiv preprint arXiv:1612.04211}, 2016.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup }{2017}]{rnet}
Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou.
\newblock Gated self-matching networks for reading comprehension and question
  answering.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics}, 2017.

\bibitem[\protect\citeauthoryear{Xiong \bgroup \em et al.\egroup
  }{2016}]{xiong2016dynamic}
Caiming Xiong, Victor Zhong, and Richard Socher.
\newblock Dynamic coattention networks for question answering.
\newblock {\em arXiv preprint arXiv:1611.01604}, 2016.

\bibitem[\protect\citeauthoryear{Zeiler}{2012}]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock {\em arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
