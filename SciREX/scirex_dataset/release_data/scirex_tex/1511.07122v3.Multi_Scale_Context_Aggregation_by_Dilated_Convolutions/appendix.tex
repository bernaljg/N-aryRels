%\section{Appendix - Results on More Datasets}
%\subsection{hello}
\begin{appendices}

\section{Urban Scene Understanding}

In this appendix, we report experiments on three datasets for urban scene understanding: the CamVid dataset~\citep{Brostow:2009:PRL}, the KITTI dataset \citep{GeigerLSU13}, and the new Cityscapes dataset~\citep{Cordts:2016:CVPR}. As the accuracy measure we use the mean IoU~\citep{Everingham2010}. We only train our model on the training set, even when a validation set is available. The results reported in this section do not use conditional random fields or other forms of structured prediction. They were obtained with convolutional networks that combine a front-end module and a context module, akin to the ``Front + Basic" network evaluated in Table \ref{tab:controlled}. The trained models can be found at \url{https://github.com/fyu/dilation}.

We now summarize the training procedure used for training the front-end module. This procedure applies to all datasets. Training is performed with stochastic gradient descent. Each mini-batch contains 8 crops from randomly sampled images. Each crop is of size $628\!\times\!628$ and is randomly sampled from a padded image. Images are padded using reflection padding. No padding is used in the intermediate layers. The learning rate is $10^{-4}$ and momentum is set to $0.99$. The number of iterations depends on the number of images in the dataset and is reported for each dataset below.

The context modules used for these datasets are all derived from the ``Basic" network, using the terminology of Table \ref{tab:layers}. The number of channels in each layer is the number of predicted classes $C$. (For example, $C\!=\!19$ for the Cityscapes dataset.) Each layer in the context module is padded such that the input and response maps have the same size. The number of layers in the context module depends on the resolution of the images in the dataset.
%Specifically, we set the number of layers such that the receptive field of the complete model is roughly twice as large as the input images.
Joint training of the complete model, composed of the front-end and the context module, is summarized below for each dataset.

\subsection{CamVid}

We use the split of~\cite{SturgessALT09}, which partitions the dataset into 367 training images, 100 validation images, and 233 test images. 11 semantic classes are used. The images are downsampled to $640 \timess 480$.

The context module has 8 layers, akin to the model used for the Pascal VOC dataset in the main body of the paper. The overall training procedure is as follows. First, the front-end module is trained for 20K iterations. Then the complete model (front-end + context) is jointly trained by sampling crops of size $852 \timess 852$ with batch size 1. The learning rate for joint training is set to
$10^{-5}$ and the momentum is set to $0.9$.

Results on the CamVid test set are reported in Table~\ref{tab:camvid}. We refer to our complete convolutional network (front-end + context) as Dilation8, since the context module has 8 layers. Our model outperforms the prior work. This model was used as the unary classifier in the recent work of~\cite{Kundu2016}.

\begin{table}[htbp]
  \small
  \setlength{\tabcolsep}{5.3pt}
  \small
  \begin{tabular}{l||c|c|c|c|c|c|c|c|c|c|c||c}
     & \ver{Building} & \ver{Tree} & \ver{Sky} & \ver{Car} & \ver{Sign} & \ver{Road} & \ver{~~Pedestrian~~} &
    \ver{Fence} & \ver{Pole} & \ver{Sidewalk} & \ver{~Bicyclist~} & \ver{mean IoU} \\ \hline
ALE & 73.4 & 70.2 & \textbf{91.1} & 64.2 & 24.4 & 91.1 & 29.1 & 31.0 &
13.6 & 72.4 & 28.6 & 53.6 \\
SuperParsing & 70.4 & 54.8 & 83.5 & 43.3 & 25.4 & 83.4 & 11.6 & 18.3
& 5.2 & 57.4 & 8.9 & 42.0 \\
Liu and He & 66.8 & 66.6 & 90.1 & 62.9 & 21.4 & 85.8 & 28.0 & 17.8 &
8.3 & 63.5 & 8.5 & 47.2 \\
SegNet & 68.7 & 52.0 & 87.0 & 58.5 & 13.4 & 86.2 & 25.3 & 17.9 & 16.0
& 60.5 & 24.8 & 46.4 \\
DeepLab-LFOV & 81.5 & 74.6 & 89.0 & 82.2 & 42.3 & \textbf{92.2} & 48.4 & 27.2 & 14.3 & \textbf{75.4} & 50.1 & 61.6 \\
Dilation8 & \textbf{82.6} & \textbf{76.2} & 89.9 & \textbf{84.0} &
\textbf{46.9} & \textbf{92.2} & \textbf{56.3} & \textbf{35.8} &
\textbf{23.4} & 75.3 & \textbf{55.5} & \textbf{65.3}
\\ \hline
  \end{tabular}
  \caption{Semantic segmentation results on the CamVid dataset. Our model (Dilation8) is compared to ALE \citep{LadickyRKT09}, SuperParsing \citep{TigheLazebnik2013}, Liu and He \citep{LiuHe2015}, SegNet \citep{Badrinarayanan2015}, and the DeepLab-LargeFOV model~\citep{Chen2015ICLR}. Our model outperforms the prior work.}
  \label{tab:camvid}
\end{table}


\subsection{KITTI}

We use the training and validation split of~\cite{Ros:2015:WACV}: 100 training images and 46 test images. The images were all collected from the KITTI visual odometry/SLAM dataset. The image resolution is $1226 \timess 370$. Since the vertical resolution is small compared to the other datasets, we remove Layer 6 in Table~\ref{tab:layers}. The resulting context module has 7 layers. The complete network (front-end + context) is referred to as Dilation7.

The front-end is trained for 10K iterations. Next, the front-end and the context module are trained jointly. For joint training, the crop size is $900\timess 900$ and momentum is set to 0.99, while the other parameters are the same as the ones used for the CamVid dataset. Joint training is performed for 20K iterations.

%For comparison, we also trained the DeepLab-LargeFOV model~\citep{Chen2015ICLR}.
%In addition, we report the performance of the front-end separately for calibration.
The results are shown in Table~\ref{tab:kitti}. As the table demonstrates, our model outperforms the prior work.

\begin{table}[htbp]
\small
\setlength{\tabcolsep}{5pt}
\centering
\begin{tabular}{l||c|c|c|c|c|c|c|c|c|c|c||c}
 & \ver{Building} & \ver{Tree} & \ver{Sky} & \ver{Car} & \ver{Sign} & \ver{Road} & \ver{~~Pedestrian~~} &
\ver{Fence} & \ver{Pole} & \ver{Sidewalk} & \ver{Bicyclist} &
\ver{mean IoU} \\ \hline
Ros et al. & 71.8 & 69.5 & \textbf{84.4} & 51.2 & 4.2 & 72.4 & 1.7 &
32.4 & 2.6 & 45.3 & 3.2 & 39.9 \\
DeepLab-LFOV  & 82.8 & 78.6 & 82.4 & 78.0 & 28.8 & 91.3 & 0.0 &
39.4 & 29.9 & 72.4 & 12.9 & 54.2 \\
%Front end & 84.4 & 80.8 & 83.5 & 81.2 & 40.6 & \textbf{93} & 0.5 &
%44.8 & 34.7 & \textbf{73.3} & 24.6 & 58.3 \\
Dilation7 & \textbf{84.6} & \textbf{81.1} & 83 & \textbf{81.4} &
\textbf{41.8} & \textbf{92.9} & \textbf{4.6} & \textbf{47.1} & \textbf{35.2} &
\textbf{73.1} & \textbf{26.4} & \textbf{59.2} \\ \hline
\end{tabular}
\caption{Semantic segmentation results on the KITTI dataset. We compare our results to~\cite{Ros:2015:WACV} and to the DeepLab-LargeFOV model~\citep{Chen2015ICLR}. Our network (Dilation7) yields higher accuracy than the prior work.}
\label{tab:kitti}
\end{table}


\subsection{Cityscapes}

%% Dilation10 is a convolutional network based on the work of Yu and
%% Koltun~\cite{YuKoltun2016}.
%% The network consists of a front-end
%% prediction module and a context aggregation module. The front-end
%% module is an adaptation of the VGG-16 network based on dilated
%% convolutions. The context module uses dilated convolutions to
%% systematically expand the receptive field and aggregate contextual
%% information. This module is derived from the ``Basic'' network: each
%% layer has $C=19$ feature maps.

The Cityscapes dataset contains 2975 training images, 500 validation images, and 1525 test images~\citep{Cordts:2016:CVPR}. Due to the high image resolution ($2048 \timess 1024$), we add two layers to the context network after Layer 6 in Table~\ref{tab:layers}. These two layers have dilation 32 and 64, respectively. The total number of layers in the context module is 10 and we refer to the complete model (front-end + context) as Dilation10.

The Dilation10 network was trained in three stages. First, the front-end prediction module was trained for 40K iterations. Second, the context module was trained for 24K iterations on whole (uncropped) images, with learning rate $10^{-4}$, momentum $0.99$, and batch size 100. Third, the complete model (front-end + context) was jointly trained for 60K iterations on halves of images (input size
$1396 \timess 1396$, including padding), with learning rate $10^{-5}$, momentum $0.99$, and batch size
1.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figs/cityscapes_stages_sm.pdf}
  \caption{Results produced by the Dilation10 model after different training stages. (a) Input image. (b) Ground truth segmentation. (c) Segmentation produced by the model after the first stage of training (front-end only). (d) Segmentation produced after the second stage, which trains the context module. (e) Segmentation produced after the third stage, in which both modules are trained jointly.}
  \label{fig:cityscapes_stages}
\end{figure}

Figure~\ref{fig:cityscapes_stages} visualizes the effect of the training stages on the performance of the model. Quantitative results are given in Tables~\ref{tab:cityscape-class}
and~\ref{tab:cityscape-category}.

The performance of Dilation10 was compared to prior work on the Cityscapes dataset by~\cite{Cordts:2016:CVPR}. In their evaluation, Dilation10 outperformed all prior models~\citep{Cordts:2016:CVPR}. Dilation10 was also used as the unary classifier in the recent work of~\cite{Kundu2016}, which used structured prediction to increase accuracy further.

%    \renewcommand{\arraystretch}{10}

\begin{table}[htbp]
  \setlength{\tabcolsep}{1.5pt}
  \small
  \centering
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c||c}
    \ver{Road} & \ver{Sidewalk} & \ver{Building} & \ver{Wall} &
    \ver{Fence} & \ver{Pole} & \ver{Light} & \ver{Sign} & \ver{Vegetation} & \ver{Terrain} & \ver{Sky} & \ver{Person} & \ver{Rider}
    & \ver{Car} & \ver{Truck} & \ver{Bus} & \ver{Train} &
    \ver{~~Motorcycle~~} & \ver{Bicycle} & \ver{mean IoU}
    \\ \hline
    \multicolumn{20}{c}{Validation set} \\ \hline%[15pt]
    97.2 & 79.5 & 90.4 & 44.9 & 52.4 & 55.1 & 56.7 & 69 & 91 &
    58.7 & 92.6 & 75.7 & 50 & 92.2 & 56.2 & 72.6 & 54.3 & 46.2 & 70.1
    & 68.7 \\
%    iIoU &  &  &  &  &  &  &  &  &  &  &  & 56.8 & 35.8 & 83.5 & 32.9
%    & 47.2 & 38.5 & 29.2 & 50.7 & 46.8 \\
    \hline
    \multicolumn{20}{c}{Test set} \\ \hline % [15pt]
    97.6 & 79.2 & 89.9 & 37.3 & 47.6 & 53.2 & 58.6 & 65.2 & 91.8
    & 69.4 & 93.7 & 78.9 & 55 & 93.3 & 45.5 & 53.4 & 47.7 & 52.2 & 66
    & 67.1 \\
%    iIoU &  &  &  &  &  &  &  &  &  &  &  & 56.3 & 34.5 & 85.8 & 21.8
%    & 32.7 & 27.6 & 28 & 49.1 & 42 \\
    \hline
  \end{tabular}
  \caption{Per-class and mean class-level IoU achieved by our model (Dilation10) on the Cityscapes dataset.}
  \label{tab:cityscape-class}
\end{table}

\begin{table}[htbp]
  \small
  \centering
  \renewcommand{\arraystretch}{1.1}
  \newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
  \setlength{\tabcolsep}{1.5pt}
  \begin{tabular}{C{1.6cm}|C{1.6cm}|C{1.6cm}|C{1.6cm}|C{1.8cm}|C{1.6cm}|C{1.6cm}||c}
    Flat & Nature & Object & Sky & Construction & Human & Vehicle &
    ~mean IoU~ \\ \hline
    \multicolumn{8}{c}{Validation set} \\ \hline%[15pt]
    98.2 & 91.4 & 62.3 & 92.6 & 90.7 & 77.6 & 91 & 86.3
    \\
    %iIoU &  &  &  &  &  & 60.7 & 81.9 & 71.3 \\
    \hline
    \multicolumn{8}{c}{Test set} \\ \hline%[15pt]
    98.3 & 91.4 & 60.5 & 93.7 & 90.2 & 79.8 & 91.8 & 86.5
    \\
    %iIoU &  &  &  &  &  & 58.3 & 83.9 & 71.1 \\
    \hline
  \end{tabular}
  \caption{Per-category and mean category-level IoU on the Cityscapes dataset.}
  \label{tab:cityscape-category}
\end{table}


\end{appendices}
