\section{Introduction}\label{section:intro}

The prediction of click-through rate (CTR) is critical in recommender system, where the task is to estimate the probability a user will click on a recommended item. In many recommender systems the goal is to maximize the number of clicks, and so the items returned to a user can be ranked by estimated CTR; while in other application scenarios such as online advertising it can be equally important to improve both user experience and revenue, and so the ranking strategy can be adjusted as CTR$\times$bid across all candidates, where ``bid'' is the benefit the system receives once the item is clicked by a user. In either case, the key is in estimating CTR correctly.
\subsection{Background}
Quite a few learning models have been proposed for CTR prediction. According to the ability of learning feature interactions, existing models can be divided into three categories: Wide, Deep, and Wide\&Deep. 

The Wide models have shown great benefits in industrial applications. Generalized linear models, such as FTRL~\cite{FTRL} {\color{blue} \footnote{FTRL is one of optimization algorithms to train LR models. To abuse the notation in this paper, we refer an LR model trained by FTRL to a FTRL model.} (Zhenguo: how about Logistic Regression? If FTRL is a variant of LR. use a footnote to mention it.)}


Generalized linear models such as FTRL~\cite{FTRL} {\color{blue} \footnote{FTRL is one of optimization algorithms to train LR models. To abuse the notation in this paper, we refer an LR model trained by FTRL to a FTRL model.}
(Zhenguo: how about Logistic Regression? If FTRL is a variant of LR. use a footnote to mention it.)}, despite their simplicity,  have shown great benefits in industry applications. However, generalized linear models lack the ability of learning feature interactions. Even with expensive feature engineering (i.e., cross-product transformation {\color{blue} (Zhenguo: use a footnote to explain what this mean.)} on features), these models are not able to generalize to the cross-product pairs not presented in the training data~\cite{wide-n-deep} {\color{blue} (Zhenguo: not clear. pls elaborate on it.)}. Factorization Machines (FM)~\cite{fm}, as an extension of Matrix Factorization (MF) {\color{blue} (Zhenguo: add citation.)}, are proposed to learn feature interactions. They model feature interactions as inner product between latent vectors between features. However, it is very complicated to learn high-order feature interactions with FM. Tree-based models (such as GBDT~\cite{GBDT}, random forest~\cite{RF}) {\color{blue} (Zhenguo: what's the relationship between GBDT and random forest? GBDT is an instance of random forest?)} are also proposed to explore unseen feature interactions, however, these models are still unable to fully explore useful feature patterns due to the limit of the model expressiveness.

As a model to learn feature interactions automatically, deep neural networks (DNN) are promising to improve the performance of CTR prediction by fully exploring and identifying useful feature interactions. Recently, more and more works are proposed in this topic. \emph{feature representation}, \emph{feature interaction} and \emph{network design} are important factors to improve the performance of DNN for recommendations, that are studied in the works under this topic. \cite{fnn} studies feature representations and proposes Factorization-machine supported Neural Network (FNN), which enhances the feature representation by initializing the parameters in the embedding layer with FM model. However, in certain real-world industry applications, it is not feasible to do a pre-training before starting training a neural network. Moreover, addition operation in FNN may not be able to fully explore the feature interactions. Feature interaction is studied in~\cite{pnn}, by introducing a cross-product layer between embedding layer and fully-connected layer in the model named PNN. The functionality of the cross-product layer is to conduct pairwise feature interaction between any two features. Due to the network structure, PNN's prediction considers only the high-order transformation on the pairwise feature interaction, but ignores the low-order transformation and even pairwise feature interaction itself, which are also important in the CTR prediction (as stated in~\cite{wide-n-deep}). In the aspect of network design, \cite{wide-n-deep} proposes a hybrid network structure combining a linear model and a deep model in order to take into account both memorization {\color{blue} (Zhenguo: use a footnote to explain what do mean by "memorization")} and generalization to make CTR predictions. Expertise feature engineering efforts are still required in the ``wide part" of their proposed model. For instance, in the presented model for app recommendation in~\cite{wide-n-deep}, the features fed into the ``wide part" of the model are the cross-product transformation of user's installed apps and impression apps {\color{blue} (Zhenguo: manually done? if so state it explicitely)}; this cross-product transformation is identified as one of the most important signals to affect CTR prediction, according to expertise knowledge {\color{blue} (Zhenguo: citation needed)}.  {\color{blue} (Zhenguo: it seems that we use low-order feature interaction, high-order feature interaction, cross-product feature transformation, pairwise feature interaction, without definition. It is quite confusing. I suggest to elaborate these concept very clearly in this paper.)}

To overcome the limitations of existing neural networks for CTR prediction, we propose a new deep neural network structure \textbf{DeepFM} to effectively and efficiently combine DNN and FM {\color{blue} (Zhenguo: right before this paragraph, we should discuss pros and cons of DNN and FM, why they exhibit complementary characteristics deserved to be integrated). It seems to me that DNN is able to learn both low- and high-order feature interaction. So it is quite important for us to motivate the necessity of combining the power of DNN and FM}, which 1) \textbf{does not} need to have FM pre-training for the embedding layer, as required in FNN~\cite{fnn}; 2) includes fully-connected layers to capture \textbf{high-order} feature interactions in the final output and also feeds the \textbf{low-order} feature interactions in the final output directly; and 3) does not need feature engineering overhead to identify important signals, as required in~\cite{wide-n-deep}. We demonstrate empirically the effectiveness and efficiency of DeepFM over the state-of-the-art models (\cite{fnn,pnn})\footnote{In~\cite{fnn,pnn}, the authors showed that FNN and PNN outperforms the other models in CTR prediction.} on two large-scale real-world datasets (Criteo and a commercial dataset).

The rest of the paper is organized as follows. We start by introducing our proposed Factorization-Machine based Neural Network (DeepFM) in details and also presenting the relationship between its network structure and that of some state-of-the-art models in Section~\ref{section:App}. Experiments on two large-scale real-world datasets are conducted to empirically demonstrate the effectiveness and efficiency of DeepFM. A brief discussion of related work is presented in Section~\ref{section:related}. Finally, the work is concluded in Section~\ref{section:conclusion}.


