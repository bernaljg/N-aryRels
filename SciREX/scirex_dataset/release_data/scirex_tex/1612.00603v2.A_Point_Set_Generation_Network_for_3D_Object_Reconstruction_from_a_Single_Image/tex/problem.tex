\begin{figure*}[t!]
  \centering
  \begin{overpic}[width=\linewidth,unit=1mm]{./fig/pointnetsmall} 
    \put(47, -0.5){vanilla version}
  \end{overpic}  
  \qquad\\  
  \qquad\\
  \begin{overpic}[width=\linewidth, trim={0cm, 0cm, 0cm, 0.47cm}, clip,unit=1mm]{./fig/pointnet}
    \put(40,-0.5){two prediction branch version}
  \end{overpic}    
  \qquad\\  
  \qquad\\  
  \begin{overpic}[width=\linewidth, trim={0cm, 0.47cm, 0cm, 0cm}, clip,unit=1mm]{./fig/pointnetbig}
    \put(47,-0.5){hourglass version}
  \end{overpic} 
  \caption{PointOutNet structure}
  \label{fig:pointnet}
\end{figure*}
\label{sec:problem}
Our goal is to reconstruct the \emph{complete} 3D shape of an object from a single 2D image (RGB or RGB-D). We represent the 3D shapes in the form of unordered point set $S=\set{(x_i, y_i, z_i)}_{i=1}^{N}$ where $N$ is a predefined constant. We observed that for most objects using $N=1024$ is sufficient to preserve the major structures. 

One advantage of point set comes from its unordered-ness. Unlike 2D based representations like the depth map no topological constraint is put on the represented object. Compared to 3D grids, the point set enjoys higher efficiency by encoding only the points on the surface. Also, the coordinate values $(x_i,y_i,z_i)$ go over simple linear transformations when the object is rotated or scaled, which is in contrast to the case in volumetric representations.

To model the problem's uncertainty, we define the groundtruth as a probability distribution $\prob(\cdot|\image)$ over the shapes conditioned on the input $\image$. 
In training we have access to one sample from $\prob(\cdot|\image)$ for each image $\image$.

We train a neural network $\network$ as a conditional sampler from $\prob(\cdot|\image)$: 
\vspace{-3mm}
\begin{align}
\label{eqn:main}
  \shape = \network(\image, r;\Theta)
\end{align}
where $\Theta$ denotes network parameter, $r\sim \mathbb{N}(\mathbf{0}, \mathbf{I})$ is a random variable to perturb the input~\footnote{Similar to the Conditional Generative Adversarial Network~\cite{mirza2014conditional}.}. During test time multiple samples of $r$ could be used to generate different predictions.
%Our training set is a collection of image-shape pairs $\mathcal{T}=\{({\image}_k, {\shape}_k)\}_{k=1}^K$. This collection is generated by rendering a large-scale shape collection into an image set, as in \cite{su2015render}. 
%(see Sec~\ref{sec:exp:traindata}) --- 
%the groundtruth shape ${\shape}_k$ for each input image ${\image}_k$ is viewed as a sample from $\prob(\cdot|{\image}_k)$. ${\shape}_k$ has the complete geometry of the imaged object, and is oriented as in the image.
  

























