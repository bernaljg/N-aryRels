\section{Supervised training data for reading comprehension}
\label{data}

The reading comprehension task naturally lends itself to a formulation as a
supervised learning problem. Specifically we seek to estimate the conditional
probability $p(a|c,q)$, where $c$ is a context document, $q$ a query relating to
that document, and $a$ the answer to that query.
For a focused evaluation we wish to be able to exclude additional information,
such as world knowledge gained from co-occurrence statistics, in order to test a
model's core capability to detect and understand the linguistic relationships
between entities in the context document.

Such an approach requires a large training corpus of document--query--answer
triples and until now such corpora have been limited to hundreds of examples and
thus mostly of use only for testing \cite{Richardson:2013:MCT}. This limitation
has meant that most work in this area has taken the form of unsupervised
approaches which use templates or syntactic/semantic analysers to extract
relation tuples from the document to form a knowledge graph that can be queried.

Here we propose a methodology for creating real-world, large scale supervised
training data for learning reading comprehension models. Inspired by work in
summarisation \cite{Svore:2007:CNN,Woodsend:2010:CNN}, we create two machine
reading corpora by exploiting online newspaper articles and their matching
summaries. We have collected 93k articles from the CNN\footnote{\url{www.cnn.com}} and 220k articles from
the Daily Mail\footnote{\url{www.dailymail.co.uk}} websites. Both news providers supplement their articles with a
number of bullet points, summarising aspects of the information contained in the
article. Of key importance is that these summary points are abstractive and do
not simply copy sentences from the documents.
We construct a corpus of document--query--answer triples by turning these
bullet points into Cloze \cite{Taylor:1953:CLOZE} style questions by replacing
one entity at a time with a placeholder. This results in a combined corpus of
roughly 1M data points (Table \ref{tab:corpora}).
Code to replicate our datasets---and to apply this method to other sources---is
available online\footnote{\url{http://www.github.com/deepmind/rc-data/}}.

\begin{table}[t]
\footnotesize
{\centering
  \begin{minipage}[t]{0.65\textwidth}
  \centering
  \begin{tabular}[t]{@{}l@{~}r@{~~}r@{~~}r@{}l@{}r@{~~}r@{~~}r@{}}
    \toprule
    & \multicolumn{3}{c}{{\bf CNN}} &\phantom{aa}& \multicolumn{3}{c}{{\bf
Daily Mail}} \\
    \cmidrule{2-4} \cmidrule{6-8}
    & train & valid & test && train & valid & test \\
    \midrule
    \# months    & 95       & 1     & 1     &&      56 & 1      & 1 \\
    \# documents &  90,266  & 1,220 & 1,093 && 196,961 & 12,148 & 10,397 \\
    \# queries   & 380,298  & 3,924 & 3,198 && 879,450 & 64,835 & 53,182 \\
    Max \# entities & 527   & 187   & 396   && 371     & 232    & 245 \\
    Avg \# entities & 26.4  & 26.5  & 24.5  && 26.5    & 25.5   & 26.0 \\
    Avg \# tokens  & 762    & 763   & 716   && 813     & 774    & 780  \\
    Vocab size & \multicolumn{3}{c}{{118,497}} && \multicolumn{3}{c}{{208,045}} \\
    \bottomrule
  \end{tabular}
  \captionof{table}{Corpus statistics. Articles were collected starting in
    April 2007 for CNN and June 2010 for the Daily Mail,
    both until the end of April~2015. Validation data is from March, test data from April~2015.
    Articles of over 2000 tokens and  queries whose answer entity did not appear in the context were filtered out.}
  \label{tab:corpora}
\end{minipage}
\hfill
\begin{minipage}[t]{0.33\textwidth}
\footnotesize
  \centering
  \begin{tabular}[t]{@{}l@{~~}r@{~~}r@{}}
    \toprule
    \textbf{Top N} & \multicolumn{2}{c}{{\bf Cumulative \%}} \\
    \cmidrule{2-3}
    & \textbf{CNN} & \textbf{Daily Mail} \\
    \midrule
    1  & 30.5 & 25.6 \\
    2  & 47.7 & 42.4 \\
    3  & 58.1 & 53.7 \\
    5  & 70.6 & 68.1 \\
    10 & 85.1 & 85.5 \\
    \bottomrule
  \end{tabular}
  \captionof{table}{Percentage of time that the correct answer is contained in the
  top $N$ most frequent entities in a given document.}
  \label{tab:cloze_hardness}
\end{minipage}
}
\end{table}

\subsection{Entity replacement and permutation}

\begin{table}[t]
  \footnotesize

  \begin{tabularx}{\textwidth}{@{}lY@{}l@{}Y@{}}
    \toprule

    & \textbf{Original Version} & \phantom{b}
                & \textbf{Anonymised Version} \\
    \midrule
    \multicolumn{4}{@{}l}{\textbf{Context}} \\
    & The BBC producer allegedly struck by Jeremy Clarkson will not
    press charges against the ``Top Gear'' host, his lawyer said Friday.
    Clarkson, who hosted one of the most-watched television shows in the world,
    was dropped by the BBC Wednesday after an internal investigation by the
    British broadcaster found he had subjected producer Oisin Tymon ``to an
    unprovoked physical and verbal attack.'' \dots
    &
    & the \textit{ent381} producer allegedly struck by \textit{ent212} will not
    press charges against the `` \textit{ent153} '' host , his lawyer said
    friday .  \textit{ent212} , who hosted one of the most - watched television
    shows in the world , was dropped by the \textit{ent381} wednesday after an
    internal investigation by the \textit{ent180} broadcaster found he had
    subjected producer \textit{ent193} `` to an unprovoked physical and verbal
    attack . '' \dots
    \\
    \midrule
    \multicolumn{4}{@{}l}{\textbf{Query}} \\
    & Producer \textbf{X} will not press charges against Jeremy Clarkson, his
    lawyer says.
    &
    & producer \textbf{X} will not press charges against \textit{ent212} , his
    lawyer says .
    \\
    \midrule
    \multicolumn{4}{@{}l}{\textbf{Answer}} \\
    & Oisin Tymon
    &
    & \textit{ent193} \\
    \bottomrule
  \end{tabularx}
  \caption{Original and anonymised version of a data point from the Daily Mail
  validation set. The anonymised entity markers are constantly permuted during
training and testing.}
  \label{tab:saccharin}
\end{table}

Note that the focus of this paper is to provide a corpus for evaluating a model's
ability to read and comprehend a single document, not world knowledge or
co-occurrence. To understand that distinction consider for instance the
following Cloze form queries (created from headlines in the Daily Mail
validation set):
\begin{inparaenum}[\itshape a\upshape)]
  \item The hi-tech bra that helps you beat breast \textbf{X};
  \item Could Saccharin help beat \textbf{X} ?;
  \item Can fish oils help fight prostate \textbf{X} ?
\end{inparaenum}
An ngram language model trained on the Daily Mail would easily correctly predict
that (\textbf{X} = \textit{cancer}), regardless of the contents of the context
document, simply because this is a very frequently cured entity in the Daily Mail
corpus.

To prevent such degenerate solutions and create a focused task we anonymise and
randomise our corpora with the following procedure,
\begin{inparaenum}[\itshape a\upshape)]
  \item use a coreference system to establish coreferents in each
    data point;
  \item replace all entities with abstract entity markers according to
    coreference;
  \item randomly permute these entity markers whenever a data point is loaded.
\end{inparaenum}

Compare the original and anonymised version of the example in Table
\ref{tab:saccharin}. Clearly a human reader can answer both queries correctly.
However in the anonymised setup the context document is required for answering
the query, whereas the original version could also be answered by someone with
the requisite background knowledge.
Therefore, following this procedure, the only remaining strategy for answering
questions is to do so by exploiting the context presented with each question.
Thus performance on our two corpora truly measures reading comprehension
capability.  Naturally a production system would benefit from using all
available information sources, such as clues through language and co-occurrence
statistics.

Table \ref{tab:cloze_hardness} gives an indication of the difficulty of the
task, showing how frequent the correct answer is contained in the top $N$ entity
markers in a given document. Note that our models don't distinguish between
entity markers and regular words. This makes the task harder and the models more
general.
