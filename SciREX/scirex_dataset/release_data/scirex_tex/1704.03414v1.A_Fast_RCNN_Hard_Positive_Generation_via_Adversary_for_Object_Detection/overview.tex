\section{Adversarial Learning for Object Detection}
Our goal is to learn an object detector that is robust to different conditions such as occlusion, deformation and illumination. We hypothesize that even in large-scale datasets, it is impossible to cover all potential occlusions and deformations. Instead of relying heavily on the dataset or sifting through data to find hard examples, we take an alternative approach. We actively generate examples which are hard for the object detector to recognize. However, instead of generating the data in the pixel space, we focus on a restricted space for generation: occlusion and deformation.

Mathematically, let us assume the original object detector network is represented as $\mathcal{F}(X)$ where $X$ is one of the object proposals. A detector gives two outputs $\mathcal{F}_c$ which represents class output and $\mathcal{F}_l$ represent predicted bounding box location. Let us assume that the ground-truth class for $X$ is $C$ with spatial location being $L$. Our original detector loss can be written down as, 
{\small
\begin{equation}
    \nonumber
    \mathcal{L}_\mathcal{F} = \mathcal{L}_{\text{softmax}}(\mathcal{F}_c(X), C) + \left[C \notin \text{bg}\right] \mathcal{L}_{\text{bbox}}(\mathcal{F}_l(X), L), 
\end{equation}
}
where the first term is the SoftMax loss and the second term is the loss based on predicted bounding box location and ground truth box location (foreground classes only). 

Let's assume the adversarial network is represented as $\mathcal{A}(X)$ which given a feature $X$ computed on image $I$, generates a new adversarial example. The loss function for the detector remains the same just that the mini-batch now includes fewer original and some adversarial examples. 

However, the adversarial network has to learn to predict the feature on which the detector would fail. We train this adversarial network via the following loss function,
{\small
\begin{equation}
    \nonumber
    \mathcal{L}_\mathcal{A} = - \mathcal{L}_{\text{softmax}}(\mathcal{F}_c(\mathcal{A}(X)), C).
\end{equation}
}
Therefore, if the feature generated by the adversarial network is easy for the detector to classify, we get a high loss for the adversarial network. On the other hand, if after adversarial feature generation it is difficult for the detector, we get a high loss for the detector and a low loss for the adversarial network.





