\section{Related Work}\label{sec:related_work}

A vast majority of past work in summarization has been extractive, which consists of identifying key sentences or passages in the source document and reproducing them as summary \cite{neto:2002:ATS,extractive,wong:2008:ESU,filippovaA13:EMNLP,colmenares:NAACL2015,graph_based,key_phrases,key_passages}.
%Recently, Colmenares et al. \shortcite{colmenares:NAACL2015} proposed an extractive summarization technique that models the problem as a discrete optimization task in a feature-rich space. Their task consists into generating headlines for news articles, which is similar to our task on the Gigaword corpus.

Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC-2003 and DUC-2004 competitions.\footnote{http://duc.nist.gov/} The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. 
%The task is to automatically generate an abstractive summary for each document that resembles one or more of the reference summaries. 
The best performing system on the DUC-2004 task, called TOPIARY \cite{topiary}, used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches \cite{mt4summ}, compression using weighted tree-transformation rules \cite{cohn_lapata} and quasi-synchronous grammar approaches \cite{woodsend}.

%The  DUC-2003  and  DUC-2004  competitions helped to standardize the task of headline generation and fostered the research on this topic \cite{over:2007:DC,dorr:2003:HTP,}.

With the emergence of deep learning as a viable alternative for many NLP tasks \cite{nlp_from_scratch}, researchers have started considering this framework as an  attractive, fully data-driven alternative to abstractive summarization. 
%In the framework of deep learning, two papers are closely related to the models we propose.
In \newcite{namas}, the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, \newcite{chopra} used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets.

In another paper that is closely related to our work, \newcite{hu:2015:EMNLP} introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora. 

In another very recent work, \newcite{jianpeng} used RNN based encoder-decoder for extractive summarization of documents. This model is not directly comparable to ours since their framework is extractive while ours and that of \cite{namas}, \cite{hu:2015:EMNLP} and \cite{chopra} is abstractive.

Our work starts with the same framework as \cite{hu:2015:EMNLP}, where we use RNNs for both source and target, but we go beyond the standard architecture and propose novel models that address critical problems in summarization. We also note that this work is an extended version of \newcite{nallapati}. In addition to performing more extensive experiments compared to that work, we also propose a novel dataset for document summarization on which we establish benchmark numbers too.

Below, we analyze the similarities and differences of our proposed models with related work on summarization.

\noindent{\bf Feature-rich encoder} (Sec. \ref{sec:feats}): Linguistic features such as POS tags, and named-entities as well as TF and IDF information were used in many extractive approaches to summarization \cite{linguistic_extractive}, but they are novel in the context of deep learning approaches for abstractive summarization, to the best of our knowledge.

\noindent{\bf Switching generator-pointer model} (Sec. \ref{sec:switch}): This model combines extractive and abstractive approaches to summarization in a single end-to-end framework. \newcite{namas} also used a combination of extractive and abstractive approaches, but their extractive model is a separate log-linear classifier with handcrafted features. Pointer networks \cite{pointer_networks} have also been used earlier for the problem of rare words in the context of machine translation \cite{luongACL15}, but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source (e.g., for named entities and OOV) and when it is allowed to be creative. We believe such a process arguably mimics how human produces summaries. For a more detailed treatment of this model, and experiments on multiple tasks, please refer to the parallel work published by some of the authors of this work \cite{caglar_acl}.

%The model's ability to strike a balance between abstraction and extraction is demonstrated qualitatively in Figure 3 where the model points only to rare-words in the source, and generates other words on its own.
\noindent{\bf Hierarchical attention model} (Sec. \ref{sec:hierarchical}): Previously proposed hierarchical encoder-decoder models use attention only at sentence-level \cite{hiero_encdec}. The novelty of our approach lies in joint modeling of attention at both sentence and word levels, where the word-level attention is further influenced by sentence-level attention, thus capturing the notion of important sentences and important words within those sentences. Concatenation of positional embeddings with the hidden state at sentence-level is also new.

