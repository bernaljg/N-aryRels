\section{Introduction}
Abstractive text summarization is the task of generating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage. We use the adjective `abstractive' to denote a summary that is not a mere selection of a few existing passages or sentences extracted from the source, but a compressed paraphrasing of the main contents of the document, potentially using vocabulary unseen in the source document. 

This task can also be naturally cast as mapping an input sequence of words in a source document to a target sequence of words called summary. In the recent past, deep-learning based models that map an input sequence into another output sequence, called sequence-to-sequence models, have been successful in many problems such as machine translation \cite{nmt}, speech recognition \cite{speech} and video captioning \cite{video_captioning}. In the framework of sequence-to-sequence models, a very relevant model to our task is the attentional Recurrent Neural Network (RNN) encoder-decoder model proposed in 
\newcite{nmt}, which has produced state-of-the-art performance in machine translation (MT), which is also a natural language task.
%Since MT is also a task that maps one word-sequence to another, the attentional RNN encoder-decoder is a natural candidate for summarization too. 

Despite the similarities, abstractive summarization is a very different problem from MT. Unlike in MT, the target (summary) is typically very short and does not depend very much on the length of the source (document) in summarization. Additionally, a key challenge in summarization is to optimally compress the original document in a {\it lossy manner} such that the key concepts in the original document are preserved, whereas in MT, the translation is expected to be loss-less. In translation, there is a strong notion of almost one-to-one word-level alignment between source and target, but in summarization, it is less obvious. 
%Also, often-times, the summary may incorporate domain knowledge that is not contained in the source.\footnote{{\it E.g.}, the source document may refer to Russia as the protagonist of a political story, while the summary may just use the word `Moscow' to refer to the same entity.}
%Hence, it remains unclear whether the models that succeeded in machine translation would perform equally well here.

We make the following main contributions in this work: (i) We apply the off-the-shelf attentional encoder-decoder RNN that was originally developed for machine translation to summarization, and show that it already outperforms state-of-the-art systems on two different English corpora. (ii) Motivated by concrete problems in summarization that are not sufficiently addressed by the machine translation based model, we propose novel models and show that they provide additional improvement in performance. (iii) We propose a new dataset for the task of abstractive summarization of a document into multiple sentences and establish benchmarks.  

The rest of the paper is organized as follows. In Section \ref{sec:models}, we describe each specific problem in abstractive summarization that we aim to solve, and present a novel model that addresses it. Section \ref{sec:related_work} contextualizes our models with respect to closely related work on the topic of abstractive text summarization. We present the results of our experiments on three different data sets in Section \ref{sec:exp}. We also present some qualitative analysis of the output from our models in Section \ref{sec:analysis} before concluding the paper with remarks on our future direction in Section \ref{sec:conclusion}.
%\vspace{-.1in}
%\section{Related work}
%\vspace{-.1in}

%The rest of the presentation is organized as follows. We set the context of our work in relation to the work done by other researchers in Section \ref{sec:related_work}. Section \ref{sec:models} describes the attentional encoder-decoder architecture and several novel variants that we propose for the task of summarization. In section \ref{sec:exp}, we present experimental results from our proposed models on three different datasets including the new proposed dataset. In Section \ref{sec:analysis}, we display sample output from our models for qualitative analysis. Finally, we conclude the discussion in Sec \ref{conclusion} with a few thoughts on future work.

% A vast majority of the past work in summarization has been extractive, which consists of identifying key sentences or passages  in the source and reproducing them as summary (\cite{extractive}). There has also been some work on abstractive summarization using traditional machine translation based models (\cite{mt4summ}). In the framework of deep learning, the closest to our model is the recent work of \cite{namas}, in which, the authors use convolutional models to encode the source and a context-sensitive attentional feed-forward neural network to generate the summary, and they produced state-of-the-art results on the Gigaword and DUC datasets. 
%We will use this model's reported numbers on the Gigaword corpus for performance comparison.
%  For a more thorough comparison of past work, please refer to the aforementioned work.
