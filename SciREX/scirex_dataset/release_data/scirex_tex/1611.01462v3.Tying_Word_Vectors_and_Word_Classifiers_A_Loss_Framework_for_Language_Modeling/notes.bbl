\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(2001)Bengio, Ducharme, and Vincent]{nnlm:2001:nips}
Yoshua Bengio, R{\'{e}}jean Ducharme, and Pascal Vincent.
\newblock A neural probabilistic language model.
\newblock 2001.
\newblock URL \url{http://www.iro.umontreal.ca/~lisa/pointeurs/nips00_lm.ps}.

\bibitem[Bj{\"o}rck \& Golub(1973)Bj{\"o}rck and Golub]{bjorck1973numerical}
$\accentset{\circ}{\text{A}}$ke Bj{\"o}rck and Gene~H Golub.
\newblock Numerical methods for computing angles between linear subspaces.
\newblock \emph{Mathematics of computation}, 27\penalty0 (123):\penalty0
  579--594, 1973.

\bibitem[Cheng et~al.(2014)Cheng, Kok, Pham, Chieu, and
  Chai]{cheng2014language}
Wei-Chen Cheng, Stanley Kok, Hoai~Vu Pham, Hai~Leong Chieu, and Kian Ming~Adam
  Chai.
\newblock Language modeling with sum-product networks.
\newblock 2014.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Bahdanau, and
  Bengio]{cho2014properties}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock \emph{arXiv preprint arXiv:1409.1259}, 2014.

\bibitem[Firat et~al.(2016)Firat, Cho, and Bengio]{firat2016multi}
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
\newblock Multi-way, multilingual neural machine translation with a shared
  attention mechanism.
\newblock \emph{arXiv preprint arXiv:1601.01073}, 2016.

\bibitem[Frogner et~al.(2015)Frogner, Zhang, Mobahi, Araya, and
  Poggio]{frogner2015learning}
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso~A
  Poggio.
\newblock Learning with a wasserstein loss.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2053--2061, 2015.

\bibitem[Gal(2015)]{gal2015theoretically}
Yarin Gal.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock \emph{arXiv preprint arXiv:1512.05287}, 2015.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Inan \& Khosravi(2016)Inan and Khosravi]{inanimproved}
Hakan Inan and Khashayar Khosravi.
\newblock Improved learning through augmenting the loss.
\newblock \emph{Stanford CS 224D: Deep Learning for Natural Language
  Processing, Spring 2016}, 2016.

\bibitem[Irie et~al.(2016)Irie, T{\"u}ske, Alkhouli, Schl{\"u}ter, and
  Ney]{irie2016lstm}
Kazuki Irie, Zolt{\'a}n T{\"u}ske, Tamer Alkhouli, Ralf Schl{\"u}ter, and
  Hermann Ney.
\newblock Lstm, gru, highway and a bit of attention: an empirical overview for
  language modeling in speech recognition.
\newblock \emph{Interspeech, San Francisco, CA, USA}, 2016.

\bibitem[Jordan(1875)]{jordan1875essai}
Camille Jordan.
\newblock Essai sur la g{\'e}om{\'e}trie {\`a} $ n $ dimensions.
\newblock \emph{Bulletin de la Soci{\'e}t{\'e} math{\'e}matique de France},
  3:\penalty0 103--174, 1875.

\bibitem[Kim et~al.(2015)Kim, Jernite, Sontag, and Rush]{kim2015character}
Yoon Kim, Yacine Jernite, David Sontag, and Alexander~M Rush.
\newblock Character-aware neural language models.
\newblock \emph{arXiv preprint arXiv:1508.06615}, 2015.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and
  Santorini]{marcus1993building}
Mitchell~P Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock \emph{Computational linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Mikolov \& Zweig()Mikolov and Zweig]{mikolov2012context}
Tomas Mikolov and Geoffrey Zweig.
\newblock Context dependent recurrent neural network language model.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and
  Khudanpur]{mikolov2010recurrent}
Tomas Mikolov, Martin Karafi{\'a}t, Lukas Burget, Jan Cernock{\`y}, and Sanjeev
  Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Interspeech}, volume~2, pp.\ ~3, 2010.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and
  Dean]{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg~S Corrado, and Jeff Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3111--3119, 2013.

\bibitem[Mnih \& Hinton(2007)Mnih and Hinton]{mnih2007three}
Andriy Mnih and Geoffrey Hinton.
\newblock Three new graphical models for statistical language modelling.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  641--648. ACM, 2007.

\bibitem[Nallapati et~al.(2016)Nallapati, Zhou, Gul{\c{c}}ehre, and
  Xiang]{nallapatiabstractive}
Ramesh Nallapati, Bowen Zhou, {\c{C}}aglar Gul{\c{c}}ehre, and Bing Xiang.
\newblock Abstractive text summarization using sequence-to-sequence rnns and
  beyond.
\newblock 2016.

\bibitem[Pascanu et~al.(2013{\natexlab{a}})Pascanu, G{\"{u}}l{\c{c}}ehre, Cho,
  and Bengio]{PascanuGCB13}
Razvan Pascanu, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho, and Yoshua
  Bengio.
\newblock How to construct deep recurrent neural networks.
\newblock \emph{CoRR}, abs/1312.6026, 2013{\natexlab{a}}.

\bibitem[Pascanu et~al.(2013{\natexlab{b}})Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock \emph{ICML (3)}, 28:\penalty0 1310--1318, 2013{\natexlab{b}}.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher~D Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{EMNLP}, volume~14, pp.\  1532--43, 2014.

\bibitem[Press \& Wolf(2016)Press and Wolf]{press2016using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock \emph{arXiv preprint arXiv:1608.05859}, 2016.

\bibitem[Rush et~al.(2015)Rush, Chopra, and Weston]{rush2015neural}
Alexander~M Rush, Sumit Chopra, and Jason Weston.
\newblock A neural attention model for abstractive sentence summarization.
\newblock \emph{arXiv preprint arXiv:1509.00685}, 2015.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean~Y Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock Citeseer, 2013.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\bibitem[Zilly et~al.(2016)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber]{zilly2016recurrent}
Julian~Georg Zilly, Rupesh~Kumar Srivastava, Jan Koutn{\'\i}k, and J{\"u}rgen
  Schmidhuber.
\newblock Recurrent highway networks.
\newblock \emph{arXiv preprint arXiv:1607.03474}, 2016.

\end{thebibliography}
