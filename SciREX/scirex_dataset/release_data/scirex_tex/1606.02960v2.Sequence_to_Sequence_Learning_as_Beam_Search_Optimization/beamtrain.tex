%
% File emnlp2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{basecommon}
\usepackage{graphicx}
\usepackage{url}
\usepackage[font={small}]{caption}

\newcommand{\RNN}{\mathrm{\mathbf{RNN}}}
\newcommand{\BPTT}{\mathrm{\mathbf{BPTT}}}
\newcommand{\BRNN}{\mathrm{\mathbf{BRNN}}}
\newcommand{\longpfx}[1]{\ensuremath{w_1 \cdots w_{#1}}}
\newcommand{\longgoldpfx}[1]{\ensuremath{y_1 \cdots y_{#1}}}
\newcommand{\pfx}[1]{\ensuremath{w_{1:{#1}}}}
\newcommand{\goldpfx}[1]{\ensuremath{y_{1:{#1}}}}
\newcommand{\beampred}[2]{\ensuremath{\hat{y}_{1:{#1}}^{({#2})}}}
\DeclareMathOperator{\suk}{succ}
\DeclareMathOperator{\topK}{topK}
\DeclareMathOperator{\score}{score}
\newcommand{\nicein}{\ensuremath{\,{\in}\,}}
\newcommand{\niceq}{\ensuremath{\,{=}\,}}

% Uncomment this linlle for the final submission:
\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
\def\emnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Sequence-to-Sequence Learning \\ as Beam-Search Optimization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Sam Wiseman \and Alexander M. Rush\\
 School of Engineering and Applied Sciences \\ Harvard University \\ Cambridge, MA, USA \\ {\tt \{swiseman,srush\}@seas.harvard.edu }}

\date{}

\begin{document}

\maketitle

\begin{abstract}
  Sequence-to-Sequence (seq2seq) modeling has rapidly become an
  important general-purpose NLP tool that has proven effective for
  many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its
  remarkable accuracy in estimating local, next-word
  distributions. In this work, we introduce a model and beam-search training
  scheme, based on the work of \newcite{daume05learning}, that extends
  seq2seq to learn global sequence scores. This
  structured approach avoids classical biases associated with local
  training and unifies the training loss with the test-time usage,
  while preserving the proven model architecture of seq2seq and
  its efficient training approach. We show that our system outperforms a
  highly-optimized attention-based seq2seq system and other baselines
  on three different sequence to sequence tasks: word ordering,
  parsing, and machine translation.
\end{abstract}


\section{Introduction}
\input{sections/intro}

\section{Related Work}
\label{sec:relatedwork}
\input{sections/relatedwork}

\section{Background and Notation}
\label{sec:background}
\input{sections/background}

\section{Beam Search Optimization}
\input{sections/model}

%\section{Practical Considerations}
%\input{sections/practical}

%\section{Experiments}
%\label{sec:experiments}
\input{sections/experiments}

\section*{Acknowledgments} We thank Yoon Kim for helpful discussions and for providing the initial seq2seq code on which our implementations are based. We thank Allen Schmaltz for help with the word ordering experiments. We also gratefully acknowledge the support of a Google Research Award.

\nocite{bahdanau16an}

\bibliography{beamtrain}
\bibliographystyle{emnlp2016}

\end{document}
