\section{Convolutional Neural Networks}
\label{sec:background}

\paraV
\paragraph{\textit{Convolutional Layer}}
Convolutional Layers perform a convolution over the input. Let $f_k$ be the filter with a kernel size $n\times m$ applied to the input $x$. $n \times m$ is the number of input connections each CNN neuron has. The resulting output of the layer calculates as follows: 

\begin{center}
\begin{minipage}{0.8\columnwidth}
\begin{equation}
C(x_{u,v}) = \sum_{i=-\frac{n}{2}}^{\frac{n}{2}}\sum_{j=-\frac{m}{2}}^{\frac{m}{2}}f_k(i,j) x_{u-i,v-j}
\end{equation}
\end{minipage}
\end{center}

To calculate a more rich and diverse representation of the input, multiple filters $f_k$ with $k \in \mathbb{N}$ can be applied on the input. The filters $f_k$ are realized by sharing weights of neighboring neurons. This has the positive effect that lesser weights have to be trained in contrast to standard Multilayer Perceptrons, since multiple weights are bound together.



\paraV
\paragraph{\textit{Max Pooling}}

Max Pooling reduces the input by applying the maximum function over the input $x_i$. Let $m$ be the size of the filter, then the output calculates as follows:


\begin{center}
\begin{minipage}{1\columnwidth}
\begin{equation}
M(x_i) = \max\{x_{i+k, i+l} \mid |k| \leq \frac{m}{2}, |l| \leq \frac{m}{2}\ k,l \in \mathbb{N}\}
\end{equation}
\end{minipage}
\end{center}

This layer features translational invariance with respect to the filter size.

\paraV
\paragraph{\textit{Rectified Linear Unit}}
A Rectified Linear Unit (ReLU) is a cell of a neural network which uses the following activation function to calculate its output given $x$:


\begin{center}
\begin{minipage}{0.44\columnwidth}
\begin{equation}
R(x) = max(0, x)
\end{equation}
\end{minipage}
\end{center}


Using these cells is more efficient than sigmoid and still forwards more information compared to binary units. When initializing the weights uniformly, half of the weights are negative. This helps creating a sparse feature representation. Another positive aspect is the relatively cheap computation. No exponential function has to be calculated. This function also prevents the vanishing gradient error, since the gradients are linear functions or zero but in no case non-linear functions~\cite{AISTATS2011_GlorotBB11}.


\paraV
\paragraph{\textit{Fully Connected Layer}}

The fully connected layer also known as Multilayer Perceptron connects all neurons of the prior layer to every neuron of its own layer. Let the input be $x$ with size $k$ and $l$ be the number of neurons in the fully connected layer. This results in a Matrix $W_{l \times k}$.

\begin{center}
\begin{minipage}{0.4\columnwidth}
\begin{equation}
F(x) = \sigma(W*x)
\end{equation}
\end{minipage}
\end{center}

$\sigma$ is the so called activation function. In our network $\sigma$ is the identity function. 

\paraV
\paragraph{\textit{Output Layer}}
The output layer is a one hot vector representing the class of the given input image. It therefore has the dimensionality of the number of classes. The resulting class for the output vector $x$ is:

\begin{center}
\begin{minipage}{0.65\columnwidth}
\begin{equation}
C(x) = \{i\ |\ \exists i \forall j \neq i : x_j \leq x_i\}
\end{equation}
\end{minipage}
\end{center}

\paraV
\paragraph{\textit{Softmax Layer}}
The error is propagated back over a Softmax layer. Let N be the dimension of the input vector, then Softmax calculates a mapping such that:
$S(x): \mathbb{R}^N \rightarrow [0,1]^N$

For each component $1 \leq j \leq N$, the output is calculated as follows:

\begin{center}
\begin{minipage}{0.5\columnwidth}
\begin{equation}
S(x)_j = \frac{e^{x_j}}{\sum_{i=1}^Ne^{x_i}}
\end{equation}
\end{minipage}
\end{center}
