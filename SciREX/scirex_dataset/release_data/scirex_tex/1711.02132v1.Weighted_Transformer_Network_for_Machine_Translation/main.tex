\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{xspace}
\usepackage{graphicx}


\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\name}{Weighted Transformer\xspace}  
\title{\name Network for \\ Machine Translation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Karim Ahmed, Nitish Shirish Keskar \& Richard Socher \\
Salesforce Research \\
Palo Alto, CA 94103, USA\\
\texttt{\{karim.ahmed,nkeskar,rsocher\}@salesforce.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. \citet{vaswani2017attention} propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers. While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge. We propose \name, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges $15-40\%$ faster. 
Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process. 
%RS: Not a big fan of that sentence... hard for most readers to understand...
Our model improves the state-of-the-art performance by $0.5$ BLEU points on the WMT 2014 English-to-German translation task and by $0.4$ on the English-to-French translation task. %It achieves the same performance as the original Transformer network with only 30\% of its parameters.
\end{abstract}

\section{Introduction}
Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs) \citep{hochreiter1997long}, form an important building block for many tasks that require modeling of sequential data. RNNs have been successfully employed for several such tasks including language modeling \citep{melis2017state,merity2017regularizing}, speech recognition \citep{xiong2017microsoft,graves2013speech}, and machine translation \citep{wu2016google,bahdanau2014neural}. RNNs make output predictions at each time step by computing a hidden state vector $h_t$ based on the current input token and the previous states. This sequential computation underlies their ability to map arbitrary input-output sequence pairs. However, because of their auto-regressive property of requiring previous hidden states to be computed before the current time step, they cannot benefit from parallelization.

Variants of recurrent networks that use strided convolutions eschew the traditional time-step based computation \citep{kaiser2016can,lei2017training,bradbury2016quasi,gehring2016convenc,gehring2017convs2s,kalchbrenner2016neural}.  However, in these models, the operations needed to learn dependencies between distant positions can be difficult to learn \citep{hochreiter2001gradient,hochreiter1998vanishing}. Attention mechanisms, often used in conjunction with recurrent models, have become an integral part of complex sequential tasks because they facilitate learning of such dependencies \citep{luong2015effective,bahdanau2014neural,parikh2016decomposable,paulus2017deep,kim2017structured}.

In \citet{vaswani2017attention}, the authors introduce the Transformer network, a novel architecture that avoids the recurrence equation and maps the input sequences into hidden states solely using attention. Specifically, the authors use positional encodings in conjunction with a multi-head attention mechanism. This allows for increased parallel computation and reduces time to convergence. The authors report results for neural machine translation that show the Transformer networks achieves state-of-the-art performance on the WMT 2014 English-to-German and English-to-French tasks while being orders-of-magnitude faster than prior approaches. 

Transformer networks still require a large number of parameters to achieve state-of-the-art performance. In the case of the newstest2013 English-to-German translation task, the base model required $65$M parameters, and the large model required $213$M parameters. We propose a variant of the Transformer network which we call \name that uses self-attention branches in lieu of the multi-head attention. The branches replace the multiple heads in the attention mechanism of the original Transformer network, and the model learns to combine these branches during training. This branched architecture enables the network to achieve comparable performance at a significantly lower computational cost. Indeed, through this modification, we improve the state-of-the-art performance by $0.5$ and $0.4$ BLEU scores on the WMT 2014 English-to-German and English-to-French tasks, respectively. Finally, we present evidence that suggests a regularizing effect of the proposed architecture. 

\section{Related Work}
Most architectures for neural machine translation (NMT) use an encoder and a decoder that rely on deep recurrent neural networks like the LSTM \citep{luong2015effective,sutskever2014sequence,bahdanau2014neural,wu2016google,barone2017deep,cho2014learning}. Several architectures have been proposed to reduce the computational load associated with recurrence-based computation \citep{gehring2016convenc,gehring2017convs2s,kaiser2016can,kalchbrenner2016neural}. Self-attention, which relies on dot-products between elements of the input sequence to compute a weighted sum \citep{lin2017structured,bahdanau2014neural,parikh2016decomposable,kim2017structured}, has also been a critical ingredient in modern NMT architectures. The Transformer network \citep{vaswani2017attention} avoids the recurrence completely and uses only self-attention. 

We propose a modified Transformer network wherein the multi-head attention layer is replaced by a branched self-attention layer. The contributions of the various branches is learned as part of the training procedure. The idea of multi-branch networks has been explored in several domains \citep{ahmed2017branchconnect,gastaldi2017shake,shazeer2017outrageously,xie2016aggregated}. To the best of our knowledge, this is the first model using a branched structure in the Transformer network. In \citet{shazeer2017outrageously}, the authors use a large network, with billions of weights, in conjunction with a sparse expert model to achieve competitive performance. \citet{ahmed2017branchconnect} analyze learned branching, through gates, in the context of computer vision while in \citet{gastaldi2017shake}, the author analyzes a two-branch model with randomly sampled weights in the context of image classification.  


\subsection{Transformer Network}
The original Transformer network uses an encoder-decoder architecture with each layer consisting of a novel attention mechanism, which the authors call multi-head attention, followed by a feed-forward network. We describe both these components below.

%RS multi-head isn't defined here yet and not useful for readers who aren't familiar with it already.. -- Fixed (?)
From the source tokens, learned embeddings of dimension ${d_{\text{model}}}$ are generated which are then modified by an additive positional encoding. The positional encoding is necessary since the network does not otherwise possess any means of leveraging the order of the sequence since it contains no recurrence or convolution. The authors use additive encoding which is defined as:
\begin{align*}
\text{PE}(pos, 2i) &= \sin(pos/10000^{2i/d_{\text{model}}}) \\
\text{PE}(pos, 2i+1) &= \cos(pos/10000^{2i/d_{\text{model}}}),
\end{align*}

where $pos$ is the position of a word in the sentence and $i$ is the dimension of the vector.
The authors also experiment with learned embeddings \citep{gehring2016convenc,gehring2017convs2s} but found no benefit in doing so. The encoded word embeddings are then used as input to the encoder which consists of $N$ layers each containing two sub-layers: (a) a multi-head attention mechanism, and (b) a feed-forward network. 
 
A multi-head attention mechanism builds upon scaled dot-product attention, which operates on a query $Q$, key $K$ and a value $V$:
\begin{align}
\text{Attention}(Q,K,V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V \label{eqn:attn}
\end{align}
where $d_k$ is the dimension of the key. 

%RS and the softmax normalizes each column / row?
In the first layer, the inputs are concatenated such that each of $(Q,K,V)$ is equal to the word vector matrix. 
This is identical to dot-product attention except for the scaling factor $d_k$, which improves numerical stability. 

Multi-head attention mechanisms obtain $h$ different representations of ($Q$, $K$, $V$), compute scaled dot-product attention for each representation, concatenate the results, and project the concatenation with a feed-forward layer. This can be expressed in the same notation as Equation \eqref{eqn:attn}:
\begin{align}
\text{head}_i &= \text{Attention(}QW_i^Q,KW_i^K,VW_i^V\text{)} \\
\text{MultiHead}(Q,K,V) &= \text{Concat}_i (\text{head}_i)W^O \label{eqn:multihead}
\end{align} 
where the $W_i$ and $W^O$ are parameter projection matrices that are learned. Note that $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$,  $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ where $h$ denotes the number of heads in the multi-head attention. \citet{vaswani2017attention}  proportionally reduce $d_k = d_v = d_{\text{model}}$ so that the computational load of the multi-head attention is the same as simple self-attention.

The second component of each layer of the Transformer network is a feed-forward network. The authors propose using a two-layered network with a ReLU activation. Given trainable weights $W_1,W_2,b_1,b_2$, the sub-layer is defined as:
\begin{align}
\text{FFN}(x) = \max(0, xW_1+b_1)W_2 + b_2 \label{eqn:ffn}
\end{align}
The dimension of the inner layer is $d_{ff}$ which is set to $2048$ in their experiments. For the sake of brevity, we refer the reader to \cite{vaswani2017attention} for additional details regarding the architecture.

For regularization and ease of training, the network uses layer normalization \citep{ba2016layer} after each sub-layer and a residual connection around each full layer \citep{he2016deep}. Analogously, each layer of the decoder contains the two sub-layers mentioned above as well as an additional multi-head attention sub-layer that receives as inputs $(V,K)$ from the output of the corresponding encoding layer. 
%RS How? K or Q or what? -- Fixed
In the case of the decoder multi-head attention sub-layers, the scaled dot-product attention is masked to prevent future positions from being attended to, or in other words, to prevent illegal leftward-ward information flow.

One natural question regarding the Transformer network is why self-attention should be preferred to recurrent or convolutional models. \citet{vaswani2017attention} state three reasons for the preference: (a) computational complexity of each layer, (b) concurrency, and (c) {path length} between long-range dependencies. Assuming a sequence length of $n$ and vector dimension $d$, the complexity of each layer is $\mathcal{O}(n^2d)$ for self-attention layers while it is $\mathcal{O}(nd^2)$ for recurrent layers. Given that typically $d > n$, the complexity of self-attention layers is lower than that of recurrent layers. Further, the number of sequential computations is $\mathcal{O}(1)$ for self-attention layers and $\mathcal{O}(n)$ for recurrent layers. This helps improved utilization of parallel computing architectures. Finally, the maximum path length between dependencies is $\mathcal{O}(1)$ for the self-attention layer while it is $\mathcal{O}(n)$ for the recurrent layer. This difference is instrumental in impeding recurrent models' ability to learn long-range dependencies. 

\begin{figure}
\begin{center}
  \includegraphics[width=1.0\linewidth]{model.pdf}
\end{center}
\caption{Our proposed network architecture. \label{fig:model}}
\end{figure}

\section{Proposed Network Architecture}
\iffalse
Our proposed network architecture follows the overall encoder-decoder structure that is used by most of neural transduction models~\citet{cho2014learning,bahdanau2014neural,vaswani2017attention}. In this structure, the encoder learns to encode a variable-length input sequence into a sequence of continuous vector representation, and the decoder learns to decode a given continuous vector representation into an output sequence. More specifically, our proposed network is built on the state-of-the-art Transformer network architecture by~\citep{vaswani2017attention}. In Figure~\ref{fig:model}, we show the structure of the multi-branch encoder and the multi-branch decoder layers that build our proposed network architecture. 


\paragraph{Multi-branch Encoder.} Each encoder layer consists of $M$ identical branches. Each branch contains two sub-layers: a single dot-product attention head, and a position-wise fully connected feed-forward network~\citet{vaswani2017attention}. As in the Transformer network~\citet{vaswani2017attention}, a residual connection~\citep{he2016deep} is applied on each of these two sub-layers.
In addition, all branches are inter-connected to each other via a ``Learned Concatenation Scaling'' component and a ``Learned Addition Scaling'' component as shown in Figure~\ref{fig:model}. More details about these components will be discussed later in this section.


\paragraph{Multi-branch Decoder.} Each decoder layer also contains $M$ identical branches, where each branch contains three sub-layers: a single masked dot-product attention head, a single dot-product attention head, and a position-wise fully connected feed-forward network~\citet{vaswani2017attention}. We apply a residual connection~\citep{he2016deep} around each of these three sub-layers. Moreover, all the branches are inter-connected with each other using a ``Learned Concatenation Scaling'' component and a ``Learned Addition Scaling'' component as shown in Figure~\ref{fig:model}. 


\paragraph{Learned Concatenation Scaling and Addition Scaling.} All the branches in both of the encoder and decoder layers are inter-connected with each other using a ``Learned Concatenation Scaling'' component and a ``Learned Addition Scaling'' components. 
\fi

{%\color{blue}
We now describe the proposed architecture, the \name, which is more efficient to train and makes better use of representational power. 


In Equations \eqref{eqn:multihead} and \eqref{eqn:ffn}, we described the attention layer proposed in \citet{vaswani2017attention} comprising the multi-head attention sub-layer and a FFN sub-layer. For the \name, we propose a branched attention that modifies the entire attention layer in the Transformer network (including both the multi-head attention and the feed-forward network). The proposed attention layer can be described as:
\begin{align}
\text{head}_i &=  \text{Attention}(QW_i^Q,KW_i^K,VW_i^V), \\
\overline{\text{head}}_i  &= \text{head}_i W^{O_i} \times \kappa_i, \label{eqn:proposed2} \\
\text{BranchedAttention}(Q,K,V) &= \sum_{i=1}^M \alpha_i \text{FFN}(\overline{\text{head}}_i).
\label{eqn:proposed}
\end{align} 
where $M$ denotes the total number of branches, $\kappa_i, \alpha_i \in \mathbb{R}^{+}$ are learned parameters and $W^{O_i} \in \mathbb{R}^{d_v \times d_{\text{model}}}$.  The FFN function above is identical to Equation \eqref{eqn:ffn}. Further, we require that $\sum \kappa_i = 1$ and $\sum \alpha_i = 1$ so that Equation \eqref{eqn:proposed} is a weighted sum of the individual branch attention values. %We emphasize that this modification is for the entire layer in the Transformer network, including the attention and feed-forward sub-layers. 

In the equations above, $\kappa$ can be interpreted as a learned \textit{concatenation} weight and $\alpha$ as the learned \textit{addition} weight. Indeed, $\kappa$ scales the contribution of the various branches before $\alpha$ is used to sum them in a weighted fashion. We ensure that all bounds are respected during each training step by projection.

While it is possible that $\alpha$ and $\kappa$ could be merged into one variable and trained, we found better training outcomes by separating them. It also improves the interpretability of the models gives that $(\alpha,\kappa)$ can be thought of as probability masses on the various branches. 

It can be shown that if $\kappa_i=1$ and $\alpha_i=1$ for all $i$, we recover the equation for the multi-head attention \eqref{eqn:multihead}. However, given the $\sum_i \kappa_i=1$ and $\sum_i \alpha_i=1$ bounds, these values are not permissible in the \name. One interpretation of our proposed architecture is that it replaces the multi-head attention by a multi-branch attention. Rather than concatenating the contributions of the different heads, they are instead treated as branches that a multi-branch network learns to combine.

This mechanism adds $\mathcal{O}(M)$ trainable weights. This is an insignificant increase compared to the total number of weights. Indeed, in our experiments, the proposed mechanism added $192$ weights to a model containing $213M$ weights already. Without these additional trainable weights, the proposed mechanism is identical to the multi-head attention mechanism in the Transformer. The proposed attention mechanism is used in both the encoder and decoder layers and is masked in the decoder layers as in the Transformer network. Similarly, the positional encoding, layer normalization, and residual connections in the encoder-decoder layers are retained. We eliminate these details from Figure~\ref{fig:model} for clarity. Instead of using $(\alpha,\kappa)$ learned weights, it is possible to also use a mixture-of-experts normalization via a softmax layer \citep{shazeer2017outrageously}. However, we found this to perform worse than our proposal. 

Unlike the Transformer, which weighs all heads equally, the proposed mechanism allows for ascribing importance to different heads. This in turn prioritizes their gradients and eases the optimization process. Further, as is known from multi-branch networks in computer vision \citep{gastaldi2017shake}, such mechanisms tend to cause the branches to learn decorrelated input-output mappings. This reduces co-adaptation and improves generalization. This observation also forms the basis for mixture-of-experts models \citep{shazeer2017outrageously}. %We show in the following section that such reduced co-adaptation is observed in the \name as well. 
}

\section{Experiments}

\subsection{Training Details}

The weights $\kappa$ and $\alpha$ are initialized randomly, as with the rest of the Transformer weights. 

In addition to the layer normalization and residual connections, we use label smoothing with $\epsilon_{\text{ls}}=0.1$, attention dropout, and residual dropout with probability $P_{\text{drop}}=0.1$. Attention dropout randomly drops out elements \citep{srivastava2014dropout} from the softmax in \eqref{eqn:attn}.

As in \citet{vaswani2017attention}, we used the Adam optimizer \citep{kingma2014adam} with $(\beta_1,\beta_2)=(0.9,0.98)$ and $\epsilon=10^{-9}$. We also use the learning rate warm-up strategy for Adam wherein the learning rate $lr$ takes on the form:
$$ lr = d_{\text{model}}^{-0.5} \cdot \min(\text{iterations}^{-0.5}, \text{iterations} \cdot 4000^{-1.5}),$$
for the all parameters except $(\alpha,\kappa)$ and 
$$ lr = {(d_{\text{model}}/\text{N})}^{-0.5} \cdot \min(\text{iterations}^{-0.5}, \text{iterations} \cdot 400^{-1.5}),$$
for $(\alpha,\kappa)$. 

This corresponds to the warm-up strategy used for the original Transformer network except that we use a larger peak learning rate for $(\alpha,\kappa)$ to compensate for their bounds. Further, we found that freezing the weights $(\kappa,\alpha)$ in the last $10K$ iterations aids convergence. During this time, we continue training the rest of the network. We hypothesize that this freezing process helps stabilize the rest of the network weights given the weighting scheme. 

We note that the number of iterations required for convergence to the final score is substantially reduced for the \name. We found that \name converges $15$--$40\%$ faster as measured by the total number of iterations to achieve optimal performance. We train the baseline model for $100$K steps for the smaller variant and $300$K for the larger. We train the \name for the respective variants for $60$K and $250$K iterations. We found that the objective did not significantly improve by running it for longer. Further, we do not use any averaging strategies employed in \citet{vaswani2017attention} and simply return the final model for testing purposes. 

In order to reduce the computational load associated with padding, sentences were batched such that they were approximately of the same length. All sentences were encoded using byte-pair encoding \citep{sennrich2015neural} and shared a common vocabulary. Weights for word embeddings were tied to corresponding entries in the final softmax layer \citep{inan2016tying,press2016using}. We trained all our networks on NVIDIA K80 GPUs with a batch containing roughly 25,000 source and target tokens. 

\subsection{Results on Benchmark Data Sets}

We benchmark our proposed architecture on the WMT 2014 English-to-German and English-to-French tasks. The WMT 2014 English-to-German data set contains $4.5$M sentence pairs. The English-to-French contains $36$M sentence pairs. 

Results of our experiments are summarized in Table~\ref{table:results}. The \name achieves a $1.1$ BLEU score improvement over the state-of-the-art on the English-to-German task for the smaller network and $0.5$ BLEU improvement for the larger network. In the case of the larger English-to-French task, we note a $0.8$ BLEU improvement for the smaller model and a $0.4$ improvement for the larger model. Also, note that the performance of the smaller model for \name is close to that of the larger baseline model, especially for the English-to-German task. This suggests that the \name better utilizes available model capacity since it needs only $30\%$ of the parameters as the baseline transformer for matching its performance. Our relative improvements do not hinge on using the BLEU scores for comparison; experiments with the GLEU score proposed in \citet{wu2016google} also yielded similar improvements. 

\iffalse 

Next, we report the rate of convergence for the two models on the news2013 English-to-German task in Figure~\ref{fig:convergence}. It is evident from the figure that the proposed branching strategy accelerates the convergence of the model. Given that the total number of parameters in both models is roughly equal, the reduced number of iterations also translates into reduction in time needed for convergence. In our experiments, we found that limiting the total number of iterations to $60-85\%$ of those required by the baseline Transformer sufficed. 
\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Rate of convergence of training and test loss for the baseline and Weighted Transformer on the newstest2013 English-to-German task. \label{fig:convergence}}
\end{figure}


\fi 

Finally, we comment on the regularizing effect of the \name.  Given the improved results, a natural question is whether the results stem from improved regularization of the model. To investigate this, we report the testing loss of the \name and the baseline Transformer against the training loss in Figure~\ref{fig:regu}. Models which have a regularizing effect tend to have lower testing losses for the same training loss. We see this effect in our experiments suggesting that the proposed architecture may have better regularizing properties. This is not unexpected given similar outcomes for other branching-based strategies such as Shake-Shake \cite{gastaldi2017shake} and mixture-of-experts \cite{shazeer2017outrageously}.



\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
%plot_train_dev_loss

\includegraphics[width=10.5cm]{plot_train_dev_loss}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Testing v/s Training Loss for the newstest2013 English-to-German task. The \name has lower testing loss compared to the baseline Transformer for the same training loss, suggesting a regularizing effect.  \label{fig:regu}}
\end{figure}



%%% Big/Small model Results Table %%%

\begin{table*}
%\small
\center
\begin{tabular}{l|c|c}
\toprule
\bf Model & \bf EN-DE BLEU & \bf EN-FR BLEU \\
\midrule
Transformer (small)~\citep{vaswani2017attention} & 27.3 & 38.1\\ 
\textbf{\name} (small) & \textbf{28.4} & \textbf{38.9} \\  
\midrule
Transformer (large)~\citep{vaswani2017attention} & 28.4  & 41.0\\ 
\textbf{\name} (large) & \textbf{28.9} & \textbf{41.4} \\ 
\midrule
ByteNet~\citep{kalchbrenner2016neural} & 23.7 & - \\
Deep-Att+PosUnk~\citep{zhou2016deep} & - & 39.2 \\
GNMT+RL~\citep{wu2016google} & 24.6 & 39.9 \\
ConvS2S~\citep{gehring2017convs2s} & 25.2 & 40.5 \\
MoE~\citep{shazeer2017outrageously} & 26.0 & 40.6 \\
\bottomrule
\end{tabular}
\caption{
Experimental results on the WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks. Our proposed model outperforms the state-of-the-art models including the Transformer~\citep{vaswani2017attention}. The small model corresponds to configuration (A) in Table~\ref{table:model_variations} while large corresponds to configuration (B).
}
\label{table:results}
\end{table*}



%%% Model Variations Table %%%
\begin{table*}
{\small
\center
\begin{tabular}{lccccccccc}
\toprule
\bf Model &  \multicolumn{7}{c}{\bf Settings} & \bf BLEU & \bf params \\
\cmidrule(lr){2-8} 
& $N$ & $d_{model}$ & $d_{ff}$ & $h$ & $M$  & $P_{drop}$ & train steps & & $\times10^{6}$ \\
\midrule

% For all models below: d_k = 64, d_v=64

Transformer (C) & 2 & 512 & 2048 & 8 & NA &  0.1 & 100K & 23.7 & $36$  \\
\textbf{\name} (C) & 2 & 512 & 2048 & 8 & 8 &  0.1 & 60K & \textbf{24.8} & $36$  \\
\midrule

Transformer& 4 & 512 & 2048 & 8 & NA &  0.1 & 100K & 25.3 & $50$  \\
\textbf{\name}& 4 & 512 & 2048 & 8 & 8 &  0.1 & 60K & \textbf{26.2} & $50$  \\
\midrule

Transformer (A)& 6 & 512 & 2048 & 8 & NA &  0.1 & 100K & 25.8 & $65$  \\
\textbf{\name} (A)& 6 & 512 & 2048 & 8 & 8  & 0.1 & 60K & \textbf{26.5} & $65$  \\
\midrule

Transformer& 8 & 512 & 2048 & 8 & NA & 0.1 & 100K & 25.5 & $80$  \\
\textbf{\name}& 8 & 512 & 2048 & 8 & 8 &  0.3 & 60K & \textbf{25.6} & $80$  \\
\midrule
\midrule


Transformer (B)& 6 & 1024 & 4096 & 16 & NA &  0.3 & 300K & 26.4 & $213$  \\
\textbf{\name} (B)& 6 & 1024 & 4096 & 16 & 16 &  0.3 & 250K & \textbf{27.2} & $213$  \\


\bottomrule
\end{tabular}
\caption{Experimental comparison between different variants of the Transformer~\citep{vaswani2017attention} architecture and our proposed \name. Reported BLEU scores are evaluated on the English-to-German translation development set, newstest2013.}
\label{table:model_variations}
}
\end{table*}




\subsection{Sensitivity Analysis}
In Table~\ref{table:model_variations}, we report sensitivity results on the newstest2013 English-to-German task. Specifically, we vary the number of layers in the encoder/decoder and compare the performance of the \name and the Transformer baseline. The results clearly demonstrate the benefit of the branched attention; for every experiment, the \name outperforms the baseline transformer, in some cases by up to $1.3$ BLEU points. As in the case of the baseline Transformer, increasing the number of layers does not necessarily improve performance; a modest improvement is seen when the number of layers $N$ is increased from $2$ to $4$ and $4$ to $6$ but the performance degrades when $N$ is increased to $8$. Increasing the number of heads from $8$ to $16$ in configuration (A) yielded an even better BLEU score. However, preliminary experiments with $h=16$ and $h=32$, like in the case with $N$, degrade the performance of the model. 
% \subsection{Convergence of Normalization Weights}
% In Figure \ref{fig:conv}, we report the convergence of $(\alpha,\kappa)$ weights for the English-to-German task for a subset of the layers. As can be seen from the figure, the weights for the encoder layers TODO while the weights for the decoder layer TODO. 

% \begin{figure}
% \label{fig:conv}
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Convergence of the normalization weights for a subset of the layers. The left column is for the encoder layers and the right column for the decoder layers. }
% \end{figure}


% \begin{figure}

%   \centering
%   \begin{minipage}{0.4\linewidth}
%     \includegraphics[width=0.5\linewidth]{figures/gate_real_gateInner_Enc1_1_txt.jpg}
%   \end{minipage}
  
%   \hfill
%   \begin{minipage}{0.4\linewidth}
%   \includegraphics[width=0.5\linewidth]{figures/gate_real_gateOutput_Enc0_0_txt.jpg}
%   \end{minipage}

% \caption{Convergence of the normalization weights for a subset of the layers. The left column is for the encoder layers and the right column for the decoder layers. \label{fig:conv}} 
% \end{figure}







In Figure~\ref{fig:conv}, we present the behavior of the weights $(\alpha,\kappa)$ for the second encoder layer of the configuration (C) for the English-to-German newstest2013 task. The figure shows that, in terms of relative weights, the network does prioritize some branches more than others; circumstantially by as much as $2\times$. Further, the relative ordering of the branches changes over time suggesting that the network is not purely exploitative. A purely exploitative network, which would learn to exploit a subset of the branches at the expense of the rest, would not be preferred since it would effectively reduce the number of available parameters and limit the representational power. Similar results are seen for other layers, including the decoder layers; we omit them for brevity. 


\begin{figure}

  \centering
  %\begin{minipage}{6cm}%{0.4\linewidth}
  \parbox{6.5cm}{
    \includegraphics[width=6.5cm]{gate_real_gateInner_Enc1_1_txt.jpg}
    %\caption{Encoder.}
  %\end{minipage}
  }
  %\hfill
\parbox{6.5cm}{
  \includegraphics[width=6.5cm]{gate_real_gateOutput_Enc0_0_txt.jpg}
  %\caption{Decoder.}
  %RS: This figure and caption doesn't show me much. You need to add some explanation of what the take away message should be here!
}
\caption{Convergence of the $(\alpha,\kappa)$ weights for the second encoder layer of Configuration (C) for the English-to-German newstest2013 task. We smoothen the curves using a mean filter. This shows that the network does prioritize some branches more than others and that the architecture does not exploit a subset of the branches while ignoring others.\label{fig:conv}} 
\end{figure}

%\subsection{Error Analysis}
%We briefly describe error analysis benchmarking the proposed architecture with the baseline transformer. We report the results in TODO.


\subsection{Randomization Baseline}
The proposed modification can also be interpreted as a form of Shake-Shake regularization proposed in \cite{gastaldi2017shake}. In this regularization strategy, random weights are sampled during forward and backward passes for weighing the various branches in a multi-branch network. During test time, they are weighed equally. In our strategy, the weights are \textit{learned} instead of being sampled randomly. Consequently, no changes to the model are required during test time.

In order to better understand whether the network benefits from the learned weights or if, at test time, random or uniform weights suffice, we propose the following experiment: the weights for the \name, including $(\alpha,\kappa)$ are trained as before, but, during test time, we replace them with (a) randomly sampled weights, and (b) $1/M$ where $M$ is the number of incoming branches. In Table~\ref{table:normalizationweights}, we report experimental results on the configuration (C) of the \name on the English-to-German newstest2013 data set (see Table~\ref{table:model_variations} for details regarding the configuration). It is evident that random or uniform weights cannot replace the learned weights during test time. Preliminary experiments suggest that a Shake-Shake-like strategy where the weights are sampled randomly during training also leads to inferior performance.

\begin{table*}
%\small
\center
\begin{tabular}{l|c}
\toprule
\bf Weights $(\alpha,\kappa)$ & \bf BLEU \\
\midrule
Learned & 24.8 \\ 
Random & 21.1 \\ 
Uniform & 23.4 \\ 
\end{tabular}
\caption{
Performance of the architecture with random and uniform normalization weights on the newstest2013 English-to-German task for configuration (C). This shows that the learned $(\alpha,\kappa)$ weights of the \name are crucial to its performance.
}
\label{table:normalizationweights}
\end{table*}

%\subsection{Correlation Analysis}
%An expected benefit of the normalization is the decorrelation between the outputs of the various branches. Specifically, it is expected that the proposed mechanism would discourage co-adaptation thus enabling diverse representations to be learned by the various branches. One way of measuring this phenomenon is through comparison of the average coefficient of variation between the outputs of the baseline and proposed transformer. We report the results in the Figure \ref{fig:corr}. It is clear that TODO.

%\begin{figure}
%\label{fig:corr}
%\begin{center}
%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%\end{center}
%\caption{Correlation between the outputs for the baseline and proposed transformer.}
%\end{figure}

\subsection{Gating}
In order to analyze whether a hard (discrete) choice through gating will outperform our normalization strategy, we experimented with using gates instead of the proposed concatenation-addition strategy. Specifically, we replaced the summation in Equation \eqref{eqn:proposed} by a gating structure that sums up the contributions of the top $k$ branches with the highest probabilities. This is similar to the sparsely-gated mixture of experts model in \citet{shazeer2017outrageously}. Despite significant hyper-parameter tuning of $k$ and $M$, we found that this strategy performs worse than our proposed mechanism by a large margin. We hypothesize that this is due to the fact that the number of branches is low, typically less than 16. Hence, sparsely-gated models lose representational power due to reduced capacity in the model. We plan to investigate the setup with a large number of branches and sparse gates in future work. 

\iffalse
\begin{table*}
%\small
\center
\begin{tabular}{l|c}
\toprule
\bf Model & \bf EN-DE BLEU \\
\midrule
Proposed (small) $M=8$ & TODO \\
Gated $M=8$, $K=4$ & TODO \\
$\vdots$ & $\vdots$ \\
TODO & TODO \\
\end{tabular}
\caption{
Experimental results of the Transformer and the proposed architecture on the WMT 2014 EN-DE and EN-FR tasks. 
}
\label{table:gates}
\end{table*}
\fi 

\section{Conclusions}
We present the \name that trains faster and achieves better performance than the original Transformer network. The proposed architecture replaces the multi-head attention in the Transformer network by a multiple self-attention branches whose contributions are learned as a part of the training process. We report numerical results on the WMT 2014 English-to-German and English-to-French tasks and show that the \name improves the state-of-the-art BLEU scores by $0.5$ and $0.4$ points respectively. Further, our proposed architecture trains $15-40\%$ faster than the baseline Transformer. Finally, we present evidence suggesting the regularizing effect of the proposal and emphasize that the relative improvement in BLEU score is observed across various hyper-parameter settings for both small and large models.

\bibliography{iclr2018_conference}
\bibliographystyle{iclr2018_conference}

\end{document}
