\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmed \& Torresani(2017)Ahmed and Torresani]{ahmed2017branchconnect}
Karim Ahmed and Lorenzo Torresani.
\newblock {BranchConnect: Large-Scale Visual Recognition with Learned Branch
  Connections}.
\newblock \emph{arXiv preprint arXiv:1704.06010}, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Barone et~al.(2017)Barone, Helcl, Sennrich, Haddow, and
  Birch]{barone2017deep}
Antonio Valerio~Miceli Barone, Jind{\v{r}}ich Helcl, Rico Sennrich, Barry
  Haddow, and Alexandra Birch.
\newblock Deep architectures for neural machine translation.
\newblock \emph{arXiv preprint arXiv:1707.07631}, 2017.

\bibitem[Bradbury et~al.(2016)Bradbury, Merity, Xiong, and
  Socher]{bradbury2016quasi}
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher.
\newblock Quasi-recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1611.01576}, 2016.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using {RNN} encoder-decoder for
  statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078}, 2014.

\bibitem[Gastaldi(2017)]{gastaldi2017shake}
Xavier Gastaldi.
\newblock {Shake-Shake regularization}.
\newblock \emph{arXiv preprint arXiv:1705.07485}, 2017.

\bibitem[Gehring et~al.(2016)Gehring, Auli, Grangier, and
  Dauphin]{gehring2016convenc}
Jonas Gehring, Michael Auli, David Grangier, and Yann~N Dauphin.
\newblock A convolutional encoder model for neural machine translation.
\newblock \emph{arXiv preprint arXiv:1611.02344}, 2016.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{gehring2017convs2s}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock {Convolutional Sequence to Sequence Learning}.
\newblock \emph{arXiv preprint arXiv:1705.03122}, 2017.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{Acoustics, speech and signal processing (icassp), 2013 ieee
  international conference on}, pp.\  6645--6649. IEEE, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hochreiter(1998)]{hochreiter1998vanishing}
Sepp Hochreiter.
\newblock The vanishing gradient problem during learning recurrent neural nets
  and problem solutions.
\newblock \emph{International Journal of Uncertainty, Fuzziness and
  Knowledge-Based Systems}, 6\penalty0 (02):\penalty0 107--116, 1998.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Bengio, Frasconi, Schmidhuber,
  et~al.]{hochreiter2001gradient}
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J{\"u}rgen Schmidhuber, et~al.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies, 2001.

\bibitem[Inan et~al.(2016)Inan, Khosravi, and Socher]{inan2016tying}
Hakan Inan, Khashayar Khosravi, and Richard Socher.
\newblock {Tying Word Vectors and Word Classifiers: A Loss Framework for
  Language Modeling}.
\newblock \emph{arXiv preprint arXiv:1611.01462}, 2016.

\bibitem[Kaiser \& Bengio(2016)Kaiser and Bengio]{kaiser2016can}
{\L}ukasz Kaiser and Samy Bengio.
\newblock Can active memory replace attention?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3781--3789, 2016.

\bibitem[Kalchbrenner et~al.(2016)Kalchbrenner, Espeholt, Simonyan, Oord,
  Graves, and Kavukcuoglu]{kalchbrenner2016neural}
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van~den Oord, Alex
  Graves, and Koray Kavukcuoglu.
\newblock Neural machine translation in linear time.
\newblock \emph{arXiv preprint arXiv:1610.10099}, 2016.

\bibitem[Kim et~al.(2017)Kim, Denton, Hoang, and Rush]{kim2017structured}
Yoon Kim, Carl Denton, Luong Hoang, and Alexander~M Rush.
\newblock Structured attention networks.
\newblock \emph{arXiv preprint arXiv:1702.00887}, 2017.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lei \& Zhang(2017)Lei and Zhang]{lei2017training}
Tao Lei and Yu~Zhang.
\newblock Training {RNNs} as fast as {CNNs}.
\newblock \emph{arXiv preprint arXiv:1709.02755}, 2017.

\bibitem[Lin et~al.(2017)Lin, Feng, Santos, Yu, Xiang, Zhou, and
  Bengio]{lin2017structured}
Zhouhan Lin, Minwei Feng, Cicero Nogueira~dos Santos, Mo~Yu, Bing Xiang, Bowen
  Zhou, and Yoshua Bengio.
\newblock A structured self-attentive sentence embedding.
\newblock \emph{arXiv preprint arXiv:1703.03130}, 2017.

\bibitem[Luong et~al.(2015)Luong, Pham, and Manning]{luong2015effective}
Minh-Thang Luong, Hieu Pham, and Christopher~D Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock \emph{arXiv preprint arXiv:1508.04025}, 2015.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017state}
G{\'a}bor Melis, Chris Dyer, and Phil Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2017)Merity, Keskar, and Socher]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017.

\bibitem[Parikh et~al.(2016)Parikh, T{\"a}ckstr{\"o}m, Das, and
  Uszkoreit]{parikh2016decomposable}
Ankur~P Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock \emph{arXiv preprint arXiv:1606.01933}, 2016.

\bibitem[Paulus et~al.(2017)Paulus, Xiong, and Socher]{paulus2017deep}
Romain Paulus, Caiming Xiong, and Richard Socher.
\newblock A deep reinforced model for abstractive summarization.
\newblock \emph{arXiv preprint arXiv:1705.04304}, 2017.

\bibitem[Press \& Wolf(2016)Press and Wolf]{press2016using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock \emph{arXiv preprint arXiv:1608.05859}, 2016.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich2015neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock \emph{arXiv preprint arXiv:1508.07909}, 2015.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3104--3112, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, et~al.]{wu2016google}
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V Le, Mohammad Norouzi, Wolfgang
  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}, 2016.

\bibitem[Xie et~al.(2016)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2016aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1611.05431}, 2016.

\bibitem[Xiong et~al.(2017)Xiong, Droppo, Huang, Seide, Seltzer, Stolcke, Yu,
  and Zweig]{xiong2017microsoft}
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas
  Stolcke, Dong Yu, and Geoffrey Zweig.
\newblock The {Microsoft 2016} conversational speech recognition system.
\newblock In \emph{Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE
  International Conference on}, pp.\  5255--5259. IEEE, 2017.

\bibitem[Zhou et~al.(2016)Zhou, Cao, Wang, Li, and Xu]{zhou2016deep}
Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.
\newblock Deep recurrent models with fast-forward connections for neural
  machine translation.
\newblock \emph{arXiv preprint arXiv:1606.04199}, 2016.

\end{thebibliography}
