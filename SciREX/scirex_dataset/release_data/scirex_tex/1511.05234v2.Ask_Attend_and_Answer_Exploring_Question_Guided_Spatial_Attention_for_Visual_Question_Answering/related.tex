\vspace{-0.05in}
\section{Related work}\label{sec:related}
\vspace{-0.05in}

Before the popularity of visual question answering (VQA), text question answering (QA) had already been established as a mature research problem in the area of natural language processing. Previous QA methods include searching for the key words of the question in a search engine~\cite{yahya2012natural}; parsing the question as a knowledge base (KB) query~\cite{berant2014semantic}; or embedding the question and using a similarity measurement to find evidence for the answer~\cite{bordes2014question}. 
Recently, memory networks were proposed for solving the QA problem.  \cite{DBLP:journals/corr/WestonCB14} first introduces the memory network as a general model that consists of a memory and four components: input feature map, generalization, output feature map and response. The model is investigated in the context of question answering, where the long-term memory acts as a dynamic knowledge base and the output is a textual response. 
\cite{sukhbaatar2015end} proposes a competitive memory network model that uses less supervision, called end-to-end memory network, which has a recurrent attention model over a large external memory. 
The Neural Turing Machine (NTM)~\cite{graves2014neural} couples a neural network to external memory and interacts with it by attentional processes to infer simple algorithms such as copying, sorting, and associative recall from input and output examples. 
In this paper, we solve the VQA problem using a multimodal memory network architecture that applies a spatial attention mechanism over an input image guided by an input text question. 

%\subsection{attention mechanism papers}
The neural attention mechanism has been widely used in different areas of computer vision and natural language processing, see  for example the attention models in image captioning~\cite{xu2015show}, video description generation~\cite{yao2015describing}, machine translation~\cite{bahdanau2014neural}\cite{luong2015effective} and machine reading systems~\cite{hermann2015teaching}. 
Most methods use the soft attention mechanism first proposed in~\cite{bahdanau2014neural}, which adds a layer to the network that predicts soft weights and uses them to compute a weighted combination of the items in memory. 
The two main types of soft attention mechanisms differ in the  function that aligns the input feature vector and the candidate feature vectors in order to compute the soft attention weights. 
The first type uses an alignment function based on ``concatenation'' of the input and each candidate (we use the term ``concatenation'' as described~\cite{luong2015effective}), and the second type uses an alignment function based on the dot product of the input and each candidate. 
The ``concatenation'' alignment function adds one input vector (e.g. hidden state vector of the LSTM) to each candidate feature vector, embeds the resulting vectors into scalar values, and then applies the softmax function to generate the attention weight for each candidate. 
\cite{xu2015show}\cite{yao2015describing}\cite{bahdanau2014neural}\cite{hermann2015teaching} use the ``concatenation'' alignment function in their soft attention models and \cite{cho2015describing} gives a literature review of such models applied to different tasks. 
On the other hand, the dot product alignment function first projects both inputs to a common vector embedding space, then takes the dot product of the two input vectors, and applies a softmax function to the resulting scalar value to produce the attention weight for each candidate. 
The end-to-end memory network~\cite{sukhbaatar2015end} uses the dot product alignment function. 
In~\cite{luong2015effective}, the authors compare these two alignment functions in an attention model for the neural machine translation task, and find that their implementation of the ``concatenation'' alignment function does not yield good performance on their task. 
Motivated by this, in this paper we use the dot product alignment function in our Spatial Memory Network. 

VQA is  related to image captioning. Several early papers about VQA directly adapt the image captioning models to solve the VQA problem~\cite{malinowski2015ask}\cite{DBLP:journals/corr/RenKZ15} by generating the answer using a recurrent LSTM network conditioned on the CNN output. But these models' performance is still limited~\cite{malinowski2015ask}\cite{DBLP:journals/corr/RenKZ15}.
\cite{zhu2015visual7w} proposes a new dataset and uses a similar attention model to that in image captioning~\cite{xu2015show}, but does not give results on the more common VQA benchmark~\cite{DBLP:journals/corr/AntolALMBZP15}, and our own implementation of this model is less accurate on~\cite{DBLP:journals/corr/AntolALMBZP15} than other baseline models. 
\cite{zhou2015simple} summarizes several recent  papers reporting results on the VQA dataset~\cite{DBLP:journals/corr/AntolALMBZP15} on arxiv.org and gives a simple but strong baseline model (iBOWIMG) on this dataset. This simple baseline concatenates the image features with the bag of word embedding question representation and feeds them into a softmax classifier to predict the answer. The iBOWIMG model beats most VQA models considered in the paper. Here, we compare our proposed model to the VQA models (namely, the ACK model~\cite{wu2015ask} and the DPPnet model~\cite{noh2015image}) which have comparable or better results than the iBOWIMG model. 
The ACK model in~\cite{wu2015ask} is essentially the same as the LSTM model in~\cite{DBLP:journals/corr/RenKZ15}, except that it uses image attribute features, the generated image caption and relevant external knowledge from a knowledge base as the input to the LSTM's first time step. 
The DPPnet model in~\cite{noh2015image} tackles VQA by learning a convolutional neural network (CNN) with some parameters predicted from a separate parameter prediction network. Their parameter prediction network uses a Gate Recurrent Unit (GRU) to generate a question representation, and based on this question input, maps the predicted weights to CNN via hashing. 
Neither of these models~\cite{wu2015ask}\cite{noh2015image} contain a spatial attention mechanism, and they both use external data in addition to the VQA dataset~\cite{DBLP:journals/corr/AntolALMBZP15}, e.g. the knowledge base in~\cite{wu2015ask} and the large-scale text corpus used to pre-train the GRU question representation~\cite{noh2015image}. In this paper, we explore a complementary approach of spatial attention to both improve performance and visualize the network's inference process, and obtain improved results without using external data compared to the iBOWIMG model~\cite{zhou2015simple} as well as the ACK model~\cite{wu2015ask} and the DPPnet model~\cite{noh2015image} which use external data. 