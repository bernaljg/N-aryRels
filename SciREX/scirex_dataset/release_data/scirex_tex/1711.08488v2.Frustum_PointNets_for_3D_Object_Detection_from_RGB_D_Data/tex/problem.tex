\label{sec:problem_definition}
Given RGB-D data as input, our goal is to classify and localize objects in 3D space. The depth data, obtained from LiDAR or indoor depth sensors, is represented as a point cloud in RGB camera coordinates. The projection matrix is also known so that we can get a 3D frustum from a 2D image region. Each object is represented by a class (one among $k$ predefined classes) and an \emph{amodal} 3D bounding box. The \emph{amodal} box bounds the complete object even if part of the object is occluded or truncated. The 3D box is parameterized by its size $h,w,l$, center $c_x, c_y, c_z$, and orientation $\theta, \phi, \psi$ relative to a predefined canonical pose for each category. In our implementation, we only consider the heading angle $\theta$ around the up-axis for orientation.