\newcolumntype{L}{>{\arraybackslash}m{10cm}}

\definecolor{dblue}{rgb}{0.0,0.0,0.6}
\definecolor{dred}{rgb}{0.3,0.0,0.0}
\definecolor{die}{rgb}{0.6,0.6,0.0}
\definecolor{dgreen}{rgb}{0.0,0.6,0.0}

%\newcolumntype{L}{>{\arraybackslash}m{10cm}}


\begin{table}[t]
\begin{small}
\begin{center}
 \resizebox{1\linewidth}{!}{
 {
%\begin{tabular}{l}
\begin{tabular}{|L|}
\hline
{\bf Doc: Wikipedia Article for Blade Runner (partially shown)}\\
\vspace{1mm}
\textcolor{dblue}{Blade Runner is a 1982 American neo-noir dystopian science fiction film
 directed by Ridley Scott and starring Harrison Ford, Rutger Hauer, Sean Young, and Edward James Olmos. The screenplay, written by Hampton Fancher and David Peoples, is a modified film adaptation of the 1968 novel ``Do Androids Dream of Electric Sheep?''  by Philip K. Dick. The film depicts a dystopian Los Angeles in November 2019 in which genetically engineered replicants, which are visually indistinguishable from adult humans, are manufactured by the powerful Tyrell Corporation as well as by other ``mega-corporations'' around the world.   %\dots}\\
Their use on Earth is banned and replicants are exclusively used for dangerous, menial, or leisure work on off-world colonies. Replicants who defy the ban and return to Earth are hunted down and ``retired'' by special police operatives known as ``Blade Runners''. \dots}
\\
\hline
%\end{tabular}
%\begin{tabular}{|L|}
{\text{\bf KB entries for Blade Runner (subset)}}\\
\vspace{1mm}
\textcolor{red}{Blade Runner {\em directed\_by} Ridley Scott}\\
\textcolor{red}{Blade Runner {\em written\_by} Philip K. Dick, Hampton Fancher}\\
\textcolor{red}{Blade Runner {\em starred\_actors} Harrison Ford, Sean Young, \dots} \\% Edward James Olmos\\
\textcolor{red}{Blade Runner {\em release\_year} 1982}\\
\textcolor{red}{Blade Runner {\em has\_tags} dystopian, noir, police, androids, \dots}
%dystopia, cult film, police, future, \dots %harrison ford, library, national film registry, philip k. dick, los angeles, ridley scott, androids, noir, visual, 2, rutger hauer, dystopian, edward james olmos, director's cut, sean young, android\\
\\
%\textcolor{red}{\dots} \\
\hline
{\text{\bf IE entries for Blade Runner (subset)}}\\
\vspace{1mm}
\textcolor{die}{Blade Runner, Ridley Scott {\em directed}    dystopian, science fiction, film}\\
\textcolor{die}{Hampton Fancher {\em written}    Blade Runner}\\
%\textcolor{die}{Blade Runner {\em brought}    Philip K. Dick}\\
\textcolor{die}{Blade Runner {\em starred}   Harrison Ford, Rutger Hauer, Sean Young\dots}\\ % Edward James Olmos}\\
\textcolor{die}{Blade Runner {\em labelled}    1982 neo noir}\\
\textcolor{die}{special police, Blade {\em retired} Blade Runner}\\
\textcolor{die}{Blade Runner, special police {\em known} Blade}
\\
\hline
{\bf Questions for Blade Runner (subset)}\\
\vspace{1mm}
\textcolor{dgreen}{Ridley Scott directed which films?}\\
\textcolor{dgreen}{What year was the movie Blade Runner released?}\\
\textcolor{dgreen}{Who is the writer of the film Blade Runner?}\\
   %What films can be described by ridley scott?\\
   %Which films can be described by sean young?\\
   %Which movies are about edward james olmos?\\
   %What movies are about harrison ford?\\
   %Which movies are about noir?\\
   %What films can be described by android?\\
\textcolor{dgreen}{Which films can be described by dystopian?}\\
   %Which movie did Hampton Fancher write?\\
\textcolor{dgreen}{Which movies was Philip K. Dick the writer of?}\\
\textcolor{dgreen}{Can you describe movie Blade Runner in a few words?}
\\
%\textcolor{dgreen}{The movie Blade Runner starred who?}\\
%\textcolor{dgreen}{Who directed the movie Blade Runner?}\\
   %What movies did Harrison Ford star in?\\
%\textcolor{dgreen}{What movies was Sean Young an actor in?}\\
   %What does Edward James Olmos star in?\\
%\dots \\


%%1 what films can be described by ridley scott?  Gladiator, Alien, Prometheus, Blade Runner, American Gangster, Black Hawk Down, Kingdom of Heaven, Robin Hood, Hannibal, Body of Lies, Matchstick Men, The Counselor, A Good Year, G.I. Jane, Legend, Black Rain, White Squall, The Duellists, Someone to Watch Over Me
%%1 what movies can be described with visual?     The Matrix, Avatar, Sin City, 300, Blade Runner, Manhunter, Halloween II
%%1 which films can be described by sean young?   Blade Runner, No Way Out, Fire Birds
%%1 which movies are about edward james olmos?    Blade Runner, Selena, Stand and Deliver, American Me, Zoot Suit
%%1 which movies are about noir?  Pulp Fiction, Sin City, Blade Runner, Drive, L.A. Confidential, Chinatown, Dark City, The Third Man, The Maltese Falcon, Double Indemnity, The Man Who Wasn't There, Brick, Touch of Evil, Following, The Big Sleep, The Killing, Laura, Body Heat, Out of the Past, White Heat, Gilda, The Asphalt Jungle, The Last Seduction, In a Lonely Place, Dark Passage, Stray Dog, D.O.A., Kansas City Confidential
%%1 which movies are about 2?     Forrest Gump, The Sixth Sense, Sin City, The Truman Show, Catch Me If You Can, Blade Runner, American Psycho, Oldboy, There's Something About Mary, Panic Room, Annie Hall, Corpse Bride, 25th Hour, Chicken Run, Rushmore, Miami Vice, Ghost in the Shell, Husbands and Wives, Gridlock'd, Nadja
%%1 what movies are about harrison ford?  Raiders of the Lost Ark, Indiana Jones and the Last Crusade, Blade Runner, Indiana Jones and the Kingdom of the Crystal Skull, The Fugitive, Ender's Game, Air Force One, What Lies Beneath, Patriot Games, The Conversation, Clear and Present Danger, Witness, 42, American Graffiti, Six Days Seven Nights, Morning Glory, Firewall, The Devil's Own, Working Girl, Frantic, Hollywood Homicide, Sabrina, Presumed Innocent, Regarding Henry, The Mosquito Coast, Random Hearts, Extraordinary Measures
%%1 what films can be described by android?       Blade Runner, A.I. Artificial Intelligence
%%1 which movies are about director's cut?        Donnie Darko, Blade Runner, Daredevil
%%1 what movies can be described by rutger hauer? Sin City, Blade Runner, Hobo with a Shotgun, The Hitcher, Ladyhawke, Nighthawks, Blind Fury, Split Second, The Osterman Weekend
%%1 which films can be described by dystopian?    V for Vendetta, Blade Runner, Brazil, Sleep Dealer
%%1 which films can be described by philip k. dick?       Blade Runner, Minority Report, Total Recall, The Adjustment Bureau, Next, Paycheck, A Scanner Darkly, Impostor, Screamers, Radio Free Albemuth
%%1 which movie did Hampton Fancher write?        Blade Runner, The Minus Man, The Mighty Quinn
%%1 which movies was Philip K. Dick the writer of?        Blade Runner, Minority Report, Total Recall, The Adjustment Bureau, Next, Paycheck, A Scanner Darkly, Impostor, Screamers, Radio Free Albemuth
%%1 can you describe movie Blade Runner in a few words?   dystopia, cult film, r, police, future, harrison ford, library, national film registry, philip k. dick, los angeles, ridley scott, androids, noir, visual, 2, rutger hauer, dystopian, edward james olmos, director's cut, sean young, android
%%1 what year was the movie Blade Runner released?        1982
%%1 who is the writer of the film Blade Runner?   Philip K. Dick, Hampton Fancher
%%1 Ridley Scott directed which films?    Gladiator, Alien, Prometheus, Blade Runner, American Gangster, Black Hawk Down, Kingdom of Heaven, Robin Hood, Hannibal, Body of Lies, Matchstick Men, The Counselor, A Good Year, G.I. Jane, Legend, Black Rain, White Squall, The Duellists, Someone to Watch Over Me
%%1 the movie Blade Runner starred who?   Harrison Ford, Sean Young, Rutger Hauer, Edward James Olmos
%%1 who directed the movie Blade Runner?  Ridley Scott
%%1 what movies did Harrison Ford star in?        Raiders of the Lost Ark, Indiana Jones and the Last Crusade, Blade Runner, Indiana Jones and the Kingdom of the Crystal Skull, The Fugitive, Ender's Game, Air Force One, The Expendables 3, Patriot Games, Clear and Present Danger, Witness, 42, Six Days Seven Nights, Firewall, The Devil's Own, Working Girl, Frantic, Hollywood Homicide, Sabrina, Presumed Innocent, Paranoia, Regarding Henry, The Mosquito Coast, Crossing Over, Random Hearts, Extraordinary Measures, Force 10 from Navarone, The Frisco Kid
%%1 what movies was Sean Young an actor in?       Blade Runner, No Way Out, Fatal Instinct, Fire Birds, A Kiss Before Dying, Cousins, Young Doctors in Love, Dr. Jekyll and Ms. Hyde, The Boost
%%1 what does Edward James Olmos star in? Blade Runner, Stand and Deliver, Wolfen, My Family, In the Time of the Butterflies, Triumph of the Spirit, Caught, Zoot Suit, Talent for the Game, The Wonderful Ice Cream Suit
%%


%~~What movies are about open source?   \textcolor{dred}{Revolution OS}\\
%~~Ruggero Raimondi appears in which movies?     \textcolor{dred}{Carmen}\\
%~~What movies did Darren McGavin star in?       \textcolor{dred}{Billy Madison, The Night Stalker, Mrs. Pollifax-Spy}\\
%~~Can you name a film directed by Stuart Ortiz? \textcolor{dred}{Grave Encounters}\\
%~~Who directed the film White Elephant?  \textcolor{dred}{Pablo Trapero}\\
%~~What is the genre of the film Dial M for Murder?   \textcolor{dred}{Thriller, Crime}\\
%~~What language is Whity in?     \textcolor{dred}{German}\\
\hline
\end{tabular}
}}
\caption{
\label{fig:blade}
%{\bf WikiMovies}: Questions and KB, Wikipedia sources.}
{\bf \WikiMovies}: Questions, Doc, KB and IE sources.}
\end{center}
\end{small}
\vspace{-1ex}
\end{table}





%The dataset has around 100,000 question-answer pairs that are split between train, dev and test sets.
%This is much larger than most existing datasets, for example the WikiQA dataset \citep{yang2015wikiqa}
% for which we also conduct experiments in Sec. \ref{sec:wikiqa} has only $\sim$1000 training pairs.
%Being able to separate the problem of severe overfitting from the type of knowledge representation..

\subsection{Knowledge Representations} \label{sec:kr}

We construct three forms of knowledge representation:
(i) Doc: raw Wikipedia documents consisting of the pages of the movies mentioned;
(ii) KB: a classical graph-based KB consisting of entities
and relations created from the Open Movie Database (OMDb) and MovieLens;
and (iii) IE: information extraction performed on the Wikipedia pages to
build a KB in a similar form as (ii).
We take care to construct QA pairs such that they are all potentially answerable
from either the KB from (ii) or the original  Wikipedia documents from (i) to
eliminate data sparsity issues. However, it should
 be noted that the advantage of working from raw documents in real applications
is that data sparsity is less of a concern than for a KB, while on the other hand the KB
has the information already parsed in a form amenable to manipulation by machines.
This dataset can help analyze what  methods we need
to close the gap between all three settings, and in particular what
are the best methods for reading documents when a KB is not available.
A sample of the dataset is shown in Table~\ref{fig:blade}.

\paragraph{Doc}
We selected a set of Wikipedia articles about movies
by identifying a set of movies from OMDb\footnote{\tiny{\url{http://beforethecode.com/projects/omdb/download.aspx}}}
that had an associated article by title match.
%We identified a set of movies from OMDb\footnote{Downloaded from \tiny{\url{http://beforethecode.com/projects/omdb/download.aspx}}.}
%that had an associated Wikipedia article by title match,
We keep the title and the first section (before the contents box) for each article.
This gives $\sim$17k documents (movies) which comprise the set of documents our
models will read from in order to answer questions.

\paragraph{KB}
Our set of movies were also matched to
the MovieLens dataset\footnote{\tiny{\url{http://grouplens.org/datasets/movielens/}}}.
We built a KB using OMDb and MovieLens metadata with entries for each movie and nine different relation types:
director, writer, actor, release year, language, genre, tags, IMDb rating and IMDb votes,
with $\sim$10k related actors, $\sim$6k directors and
$\sim$43k  entities in total.
The KB is stored as triples; see Table~\ref{fig:blade} for examples.
% such as
 %{\small{ {\sc (Young Frankenstein, starred\_actors, Gene Wilder)}}} and
%{\small {\sc (The Little Mermaid, has\_tags, Disney Animation)}}.
IMDb ratings and votes
are originally real-valued but are binned and converted to text
 (``unheard of'', ``unknown'', ``well known'', ``highly watched'', ``famous'').
We finally only retain KB triples where the entities also appear in the Wikipedia
articles\footnote{The dataset also
includes the slightly larger version without this constraint.}
to try to
guarantee that all QA pairs will be equally answerable by either the KB or Wikipedia document
sources.

\paragraph{IE} As an alternative to directly reading documents,
we explore leveraging information extraction  techniques to
transform documents into a KB format.
%Constraining the memories solely to facts identified by an IE system introduces a few tradeoffs.
%Processing each Wikipedia entry into a series of semi-structured facts mimics
%some of the attractive attributes of the KB,
An IE-KB representation has attractive properties
such as more precise and compact expressions of facts
and logical key-value pairings based on subject-verb-object groupings.
This can come at the cost of lower recall due to malformed or completely missing triplets.
%
For IE we use standard open-source software followed by some task-specific
engineering to improve the results.
We first employ coreference resolution via the Stanford NLP Toolkit \citep{manning2014stanford} to reduce ambiguity by replacing pronominal (``he'', ``it'') and nominal (``the film'') references with their representative entities. Next we use the SENNA semantic role labeling tool \citep{senna_collobert} to uncover the grammatical structure of each sentence and pair verbs with their arguments. Each triplet is cleaned of words that are not recognized entities,
and lemmatization is done to collapse different inflections of important task-specific verbs to one form (e.g. stars, starring, star $\rightarrow$ starred).
Finally, we append the movie title to each triple similar to the ``Window + Title''
representation of Sec. \ref{sec:featuremap}, which improved results.


\subsection{Question-Answer Pairs}
%\paragraph{QA Pairs}
%The dataset has more than 100,000 question-answer pairs.
%Being able to separate the problem of severe overfitting from the type of knowledge representation..
%
Within the dataset's more than 100,000 question-answer pairs, we distinguish 13
classes of question corresponding to different kinds of edges in our KB.
They range in scope from specific---such as
{\em actor to movie}:~``What movies did Harrison Ford star in?'' and
{\em movie to actors}:~``Who starred in Blade Runner?''---to more general,
such as {\em tag to movie}:~``Which films can be described by {\em dystopian}?'';
see Table \ref{table:breakdown} for the full list.
For some question there can be multiple correct answers.
%
%The topics correspond to different edges in our KB, and range in scope from specific ({\em movie to actors}  -- ``Who starred in Blade Runner?'') to more general ({\em tag to movie} -- ``Which films can be described by {\em dystopian}?''). For each question type there is a set of possible answers.
%
%as shown in Table \ref{table:breakdown}.
%he topics correspond to different edges in our KB, and range in scope from specific ({\em movie to actors}  -- ``Who starred in Blade Runner?'') to more general ({\em tag to movie} -- ``Which films can be described by {\em dystopian}?'').
%For each question type there is a set of possible answers.
%corresponding to different kinds of edges in our KB:
%{\em actor to movie} (``What movies did Harrison Ford star in?''),
%{\em movie to actors} (``Who starred in Blade Runner?''),
%{\em movie to director}, {\em director to movie},
%{\em movie to writer}, {\em writer to movie},
%{\em movie to tags}, {\em tag to movie},
%{\em movie to year}, {\em movie to genre}, {\em movie to language},
%{\em movie to IMDb rating} and
%{\em movie to IMDb votes}.

Using SimpleQuestions \citep{bordes2015large},
an existing open-domain question answering dataset based on Freebase,
we identified the subset of questions posed by human annotators that covered
our question types.
%We expanded this set to cover all of our KB by substituting the entities
%in those questions to also apply them to other questions.
We created our question set by substituting the entities in those questions
with entities from all of our KB triples.
%
For example, if the original question written by an annotator was
``What movies did Harrison Ford star in?'', we created a pattern
``What movies did [@actor] star in?'', which we substitute for any
other actors in our set, and repeat this for all annotations.
%We removed {\em tag to movie} questions with more than 50 answers,
%and
We split the questions into disjoint training, development and test sets
with $\sim$96k, 10k and 10k examples, respectively.
The same question (even worded differently) cannot appear in
both train and test sets.
Note that this is much larger than most existing datasets;
for example, the {\sc WikiQA} dataset \citep{yang2015wikiqa}
for which we also conduct experiments
in Sec. \ref{sec:wikiqa} has only $\sim$1000 training pairs.
