\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Bias Shift Correction Speeds Up Learning}{}% 2
\BOOKMARK [1][-]{section.3}{Exponential Linear Units \(ELUs\)}{}% 3
\BOOKMARK [1][-]{section.4}{Experiments Using ELUs}{}% 4
\BOOKMARK [2][-]{subsection.4.1}{MNIST}{section.4}% 5
\BOOKMARK [3][-]{subsubsection.4.1.1}{Learning Behavior}{subsection.4.1}% 6
\BOOKMARK [3][-]{subsubsection.4.1.2}{Autoencoder Learning}{subsection.4.1}% 7
\BOOKMARK [2][-]{subsection.4.2}{Comparison of Activation Functions}{section.4}% 8
\BOOKMARK [2][-]{subsection.4.3}{Classification Performance on CIFAR-100 and CIFAR-10}{section.4}% 9
\BOOKMARK [2][-]{subsection.4.4}{ImageNet Challenge Dataset}{section.4}% 10
\BOOKMARK [1][-]{section.5}{Conclusion}{}% 11
\BOOKMARK [1][-]{appendix.A}{Inverse of Block Matrices}{}% 12
\BOOKMARK [1][-]{appendix.B}{Quadratic Form of Mean and Inverse Second Moment}{}% 13
\BOOKMARK [1][-]{appendix.C}{Variance of Mean Activations in ELU and ReLU Networks}{}% 14
