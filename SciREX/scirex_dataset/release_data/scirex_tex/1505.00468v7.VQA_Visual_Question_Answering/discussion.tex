%\vspace{\sectionReduceTop}
\section{Conclusion and Discussion}
%\vspace{\sectionReduceBot}

%We have collected questions for \textcolor{red}{200k} MS COCO images. \aishwarya{Do we still need to keep next line?} However, to increase the diversity of the dataset, we are also considering the addition of images from other sources. The use of more complex and diverse abstract scenes could also open up new application areas.
In conclusion, we introduce the task of Visual Question Answering (VQA). Given an image and an open-ended, natural language question about the image, the task is to provide an accurate natural language answer. We provide a dataset containing over 250K images, 760K questions, and around 10M answers. We demonstrate the wide variety of questions and answers in our dataset, as well as the diverse set of AI capabilities in computer vision, natural language processing, and commonsense reasoning required to answer these questions accurately.

The questions we solicited from our human subjects were open-ended and not task-specific. For some application domains, it would be useful to collect task-specific questions. For instance, questions may be gathered from subjects who are visually impaired \cite{vizwiz}, or the questions could focused on one specific domain (say sports). 
Bigham \etal~\cite{vizwiz} created an application that allows the visually impaired to capture images and ask open-ended questions that are answered by human subjects. Interestingly, these questions can rarely be answered using generic captions. Training on task-specific datasets may help enable practical VQA applications.

We believe VQA has the distinctive advantage of pushing the frontiers on ``AI-complete'' problems, while being amenable to automatic evaluation. Given the recent progress in the community, we believe the time is ripe to take on such an endeavor. 

%\vspace{1pt}
{
%\footnotesize
%\scriptsize
\textbf{Acknowledgements.}
We would like to acknowledge the countless hours of effort provided by the workers on Amazon Mechanical Turk. This work was supported in part by the The Paul G. Allen Family Foundation via an award to D.P., ICTAS at Virginia Tech via awards to D.B. and D.P., Google Faculty Research Awards to D.P. and D.B., the National Science Foundation CAREER award to D.B., the Army Research Office YIP Award to D.B., and a Office of Naval Research grant to D.B.
}
\begin{comment}
%The views and conclusions contained herein are those of the authors and should not beinterpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor.

%We believe VQA sits at a unique sweet spot --  pushing the frontiers on ``AI-complete'' problems, while being amenable to automatic evaluation. Though in some sense being the holy grail of automatic image understanding, and perhaps AI in general, given the recent progress in the community, we believe the time is ripe to take on such an endeavor.


%\devi{Evaluation server, workshop, will continue growing dataset if there is interest. Any other points of discussion? If not, call this conclusion and repeat contributions.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\item  will other researchers be afraid to try approaches you mentioned because other people could be doing the same thing?

%There has been work on making computer vision systems more interpretable by semantically characterizing their failure modes using visual attributes~\cite{semantic_failures}.

%Continued efforts in this direction are being supported by an Army Research Office Young Investigator Award. 
\end{comment}