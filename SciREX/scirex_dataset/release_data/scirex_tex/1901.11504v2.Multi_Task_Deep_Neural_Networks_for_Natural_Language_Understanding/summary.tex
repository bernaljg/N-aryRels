\section{Conclusion}
\label{sec:con}
In this work we proposed a model called MT-DNN to combine multi-task learning and language model pre-training for language representation learning. 
MT-DNN obtains new state-of-the-art results on ten NLU tasks across three popular benchmarks: SNLI, SciTail, and GLUE.
MT-DNN also demonstrates an exceptional generalization capability in domain adaptation experiments. 
%The fewer the training data, the improvement introduced by MT-DNN is larger.

There are many future areas to explore to improve MT-DNN, including a deeper understanding of model structure sharing in MTL, a more effective training method that leverages relatedness among multiple tasks, for both fine-tuning and pre-training \cite{unilm2019}, and ways of incorporating the linguistic structure of text in a more explicit and controllable manner. At last, we also would like to verify whether MT-DNN is resilience against adversarial attacks \cite{breaknli2019acl,talman2018testing,liu2019mt-dnn-kd}.
%how to combine multi-task and language model into a singe training objective. Both of these could be our future direction.

% Neural approaches are now transforming the field of NLP and IR where symbolic approaches have been dominating for decades \cite{gao2018neural}. Language representation learning is core to all neural approaches.  

