\begin{thebibliography}{29}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Bowman et~al.(2015{\natexlab{a}})Bowman, Angeli, Potts, and
  Manning}]{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
  2015{\natexlab{a}}.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 632--642.

\bibitem[{Bowman et~al.(2015{\natexlab{b}})Bowman, Angeli, Potts, and
  Manning}]{snli2015}
Samuel~R. Bowman, Gabor Angeli, Christopher Potts, and Christopher~D. Manning.
  2015{\natexlab{b}}.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}. Association for Computational
  Linguistics.

\bibitem[{Burges et~al.(2005)Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton,
  and Hullender}]{learning-to-rank2005burges}
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole
  Hamilton, and Greg Hullender. 2005.
\newblock Learning to rank using gradient descent.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 89--96. ACM.

\bibitem[{Caruana(1997)}]{caruana1997multitask}
Rich Caruana. 1997.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28(1):41--75.

\bibitem[{Collobert et~al.(2011)Collobert, Weston, Bottou, Karlen, Kavukcuoglu,
  and Kuksa}]{collobert2011natural}
Ronan Collobert, Jason Weston, L{\'e}on Bottou, Michael Karlen, Koray
  Kavukcuoglu, and Pavel Kuksa. 2011.
\newblock Natural language processing (almost) from scratch.
\newblock \emph{Journal of Machine Learning Research}, 12(Aug):2493--2537.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{bert2018}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Dong et~al.(2019)Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and
  Hon}]{unilm2019}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon. 2019.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock \emph{arXiv preprint arXiv:1905.03197}.

\bibitem[{Gao et~al.(2018)Gao, Galley, and Li}]{gao2018neural}
J.~Gao, M.~Galley, and L.~Li. 2018.
\newblock Neural approaches to conversational {AI}.
\newblock \emph{CoRR}, abs/1809.08267.

\bibitem[{Glockner et~al.(2018)Glockner, Shwartz, and
  Goldberg}]{breaknli2019acl}
Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018.
\newblock Breaking nli systems with sentences that require simple lexical
  inferences.
\newblock In \emph{The 56th Annual Meeting of the Association for Computational
  Linguistics (ACL)}, Melbourne, Australia.

\bibitem[{Guo et~al.(2018)Guo, Pasunuru, and Bansal}]{guo2018soft}
Han Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018.
\newblock Soft layer-specific multi-task summarization with entailment and
  question generation.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 687--697.

\bibitem[{Huang et~al.(2013)Huang, He, Gao, Deng, Acero, and
  Heck}]{huang2013dssm}
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li~Deng, Alex Acero, and Larry Heck.
  2013.
\newblock Learning deep structured semantic models for web search using
  clickthrough data.
\newblock In \emph{Proceedings of the 22nd ACM international conference on
  Conference on information \& knowledge management}, pages 2333--2338. ACM.

\bibitem[{Khot et~al.(2018)Khot, Sabharwal, and Clark}]{scitail}
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
\newblock {SciTail}: A textual entailment dataset from science question
  answering.
\newblock In \emph{AAAI}.

\bibitem[{Kim et~al.(2018)Kim, Hong, Kang, and Kwak}]{kim2018semantic}
Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, and Nojun Kwak. 2018.
\newblock Semantic sentence matching with densely-connected recurrent and
  co-attentive information.
\newblock \emph{arXiv preprint arXiv:1805.11360}.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Diederik Kingma and Jimmy Ba. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}.

\bibitem[{Liu et~al.(2018{\natexlab{a}})Liu, Duh, and Gao}]{liu2018san4nli}
Xiaodong Liu, Kevin Duh, and Jianfeng Gao. 2018{\natexlab{a}}.
\newblock Stochastic answer networks for natural language inference.
\newblock \emph{arXiv preprint arXiv:1804.07888}.

\bibitem[{Liu et~al.(2015)Liu, Gao, He, Deng, Duh, and Wang}]{liu2015mtl}
Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li~Deng, Kevin Duh, and Ye-Yi Wang.
  2015.
\newblock Representation learning using multi-task deep neural networks for
  semantic classification and information retrieval.
\newblock In \emph{Proceedings of the 2015 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 912--921.

\bibitem[{Liu et~al.(2019)Liu, He, Chen, and Gao}]{liu2019mt-dnn-kd}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019.
\newblock Improving multi-task deep neural networks via knowledge distillation
  for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1904.09482}.

\bibitem[{Liu et~al.(2018{\natexlab{b}})Liu, Shen, Duh, and Gao}]{liu2018san}
Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2018{\natexlab{b}}.
\newblock Stochastic answer networks for machine reading comprehension.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}. Association for
  Computational Linguistics.

\bibitem[{Luong et~al.(2015)Luong, Le, Sutskever, Vinyals, and
  Kaiser}]{luong2015multi}
Minh-Thang Luong, Quoc~V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.
  2015.
\newblock Multi-task sequence to sequence learning.
\newblock \emph{arXiv preprint arXiv:1511.06114}.

\bibitem[{Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer}]{elmo2018}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer. 2018.
\newblock Deep contextualized word representations.
\newblock \emph{arXiv preprint arXiv:1802.05365}.

\bibitem[{Phang et~al.(2018)Phang, F{\'e}vry, and Bowman}]{phang2018sentence}
Jason Phang, Thibault F{\'e}vry, and Samuel~R Bowman. 2018.
\newblock Sentence encoders on stilts: Supplementary training on intermediate
  labeled-data tasks.
\newblock \emph{arXiv preprint arXiv:1811.01088}.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever}]{gpt2018}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.
\newblock Improving language understanding by generative pre-training.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
\newblock \href {https://aclweb.org/anthology/D16-1264} {Squad: 100,000+
  questions for machine comprehension of text}.
\newblock pages 2383--2392.

\bibitem[{Ruder12 et~al.(2019)Ruder12, Bingel, Augenstein, and
  S{\o}gaard}]{ruder122019latent}
Sebastian Ruder12, Joachim Bingel, Isabelle Augenstein, and Anders S{\o}gaard.
  2019.
\newblock Latent multi-task architecture learning.

\bibitem[{Talman and Chatzikyriakidis(2018)}]{talman2018testing}
Aarne Talman and Stergios Chatzikyriakidis. 2018.
\newblock Testing the generalization power of neural network models across nli
  benchmarks.
\newblock \emph{arXiv preprint arXiv:1810.09774}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2018glue}
Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman. 2018.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}.

\bibitem[{Xu et~al.(2018)Xu, Liu, Shen, Liu, and Gao}]{mt-mrc2018}
Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, and Jianfeng Gao. 2018.
\newblock Multi-task learning for machine reading comprehension.
\newblock \emph{arXiv preprint arXiv:1809.06963}.

\bibitem[{Zhang and Yang(2017)}]{zhang2017survey}
Yu~Zhang and Qiang Yang. 2017.
\newblock A survey on multi-task learning.
\newblock \emph{arXiv preprint arXiv:1707.08114}.

\end{thebibliography}
