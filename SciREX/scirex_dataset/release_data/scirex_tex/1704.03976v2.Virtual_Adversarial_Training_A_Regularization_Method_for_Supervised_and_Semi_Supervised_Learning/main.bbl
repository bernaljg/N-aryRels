\begin{thebibliography}{10}

\bibitem{abadi2016tensorflow}
Mart{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: {Large}-scale machine learning on heterogeneous
  distributed systems.
\newblock {\em arXiv preprint arXiv:1603.04467}, 2016.

\bibitem{abbas2016understanding}
Mudassar Abbas, Jyri Kivinen, and Tapani Raiko.
\newblock Understanding regularization by virtual adversarial training, ladder
  networks and others.
\newblock In {\em Workshop on ICLR}, 2016.

\bibitem{akaike1998information}
Hirotugu Akaike.
\newblock Information theory and an extension of the maximum likelihood
  principle.
\newblock In {\em Selected Papers of Hirotugu Akaike}, pages 199--213.
  Springer, 1998.

\bibitem{arnol2013mathematical}
Vladimir~Igorevich Arnol'd.
\newblock {\em Mathematical methods of classical mechanics}, volume~60.
\newblock Springer Science \& Business Media, 2013.

\bibitem{bachman2014learning}
Philip Bachman, Ouais Alsharif, and Doina Precup.
\newblock Learning with pseudo-ensembles.
\newblock In {\em NIPS}, 2014.

\bibitem{bishop1995training}
Christopher~M Bishop.
\newblock Training with noise is equivalent to {Tikhonov} regularization.
\newblock {\em Neural computation}, 7(1):108--116, 1995.

\bibitem{Bishop:2006}
Christopher~M Bishop.
\newblock {\em Pattern Recognition and Machine Learning}.
\newblock Springer, 2006.

\bibitem{collobert2006large}
Ronan Collobert, Fabian Sinz, Jason Weston, and L{\'e}on Bottou.
\newblock Large scale transductive {SVMs}.
\newblock {\em Journal of Machine Learning Research}, 7(Aug):1687--1712, 2006.

\bibitem{gal2015dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a {Bayesian} approximation: {Representing} model
  uncertainty in deep learning.
\newblock In {\em ICML}, 2016.

\bibitem{glorot2011deep}
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
\newblock Deep sparse rectifier neural networks.
\newblock In {\em AISTATS}, 2011.

\bibitem{golub2000eigenvalue}
Gene~H Golub and Henk~A van~der Vorst.
\newblock Eigenvalue computation in the 20th century.
\newblock {\em Journal of Computational and Applied Mathematics},
  123(1):35--65, 2000.

\bibitem{Goodfellow-et-al-2016}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em NIPS}, 2014.

\bibitem{goodfellow2014explaining}
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em ICLR}, 2015.

\bibitem{grandvalet2004semi}
Yves Grandvalet and Yoshua Bengio.
\newblock Semi-supervised learning by entropy minimization.
\newblock In {\em NIPS}, 2004.

\bibitem{gu2014towards}
Shixiang Gu and Luca Rigazio.
\newblock Towards deep neural network architectures robust to adversarial
  examples.
\newblock In {\em Workshop on ICLR}, 2015.

\bibitem{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em ECCV}, 2016.

\bibitem{huang2016densely}
Gao Huang, Zhuang Liu, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, 2017.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{jarrett2009best}
Kevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato, and Yann LeCun.
\newblock What is the best multi-stage architecture for object recognition?
\newblock In {\em ICCV}, 2009.

\bibitem{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em ICLR}, 2015.

\bibitem{kingma2014semi}
Diederik Kingma, Shakir Mohamed, Danilo~Jimenez Rezende, and Max Welling.
\newblock Semi-supervised learning with deep generative models.
\newblock In {\em NIPS}, 2014.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Technical Report, University of Toronto}, 2009.

\bibitem{laine2016temporal}
Samuli Laine and Timo Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In {\em ICLR}, 2017.

\bibitem{lee2015deeply}
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.
\newblock Deeply-supervised nets.
\newblock In {\em AISTATS}, 2015.

\bibitem{lin2013network}
Min Lin, Qiang Chen, and Shuicheng Yan.
\newblock Network in network.
\newblock In {\em ICLR}, 2014.

\bibitem{maaloe2016auxiliary}
Lars Maal{\o}e, Casper~Kaae S{\o}nderby, S{\o}ren~Kaae S{\o}nderby, and Ole
  Winther.
\newblock Auxiliary deep generative models.
\newblock In {\em ICML}, 2016.

\bibitem{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, and Andrew~Y Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em ICML}, 2013.

\bibitem{maeda2014bayesian}
Shin-ichi Maeda.
\newblock A {Bayesian} encourages dropout.
\newblock {\em arXiv preprint arXiv:1412.7003}, 2014.

\bibitem{miyato2015distributional}
Takeru Miyato, Shin\text{-}ichi Maeda, Masanori Koyama, Ken Nakae, and Shin
  Ishii.
\newblock Distributional smoothing with virtual adversarial training.
\newblock In {\em ICLR}, 2016.

\bibitem{nair2010rectified}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted {Boltzmann} machines.
\newblock In {\em ICML}, 2010.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em Workshop on deep learning and unsupervised feature learning
  on NIPS}, 2011.

\bibitem{rasmus2015semi}
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko.
\newblock Semi-supervised learning with ladder networks.
\newblock In {\em NIPS}, 2015.

\bibitem{reed1992regularization}
Russell Reed, Seho Oh, and RJ~Marks.
\newblock Regularization using jittered training data.
\newblock In {\em IJCNN}. IEEE, 1992.

\bibitem{sajjadi2016regularization}
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
\newblock Regularization with stochastic transformations and perturbations for
  deep semi-supervised learning.
\newblock In {\em NIPS}, 2016.

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training \rm{GANs}.
\newblock In {\em NIPS}, 2016.

\bibitem{springenberg2015unsupervised}
Jost~Tobias Springenberg.
\newblock Unsupervised and semi-supervised learning with categorical generative
  adversarial networks.
\newblock In {\em ICLR}, 2015.

\bibitem{springenberg2014striving}
Jost~Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin
  Riedmiller.
\newblock Striving for simplicity: {The} all convolutional net.
\newblock In {\em Workshop on ICLR}, 2015.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock {Dropout: A simple way to prevent neural networks from overfitting}.
\newblock {\em JMLR}, 15(1), 2014.

\bibitem{srivastava2015highway}
Rupesh~Kumar Srivastava, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Highway networks.
\newblock {\em arXiv preprint arXiv:1505.00387}, 2015.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In {\em ICLR}, 2014.

\bibitem{2016arXiv160502688short}
{Theano Development Team}.
\newblock {Theano: A {Python} framework for fast computation of mathematical
  expressions}.
\newblock {\em arXiv preprint arXiv:1605.02688}, 2016.

\bibitem{tikhonov1977solutions}
Andrej~N Tikhonov and Vasiliy~Y Arsenin.
\newblock {\em Solutions of ill-posed problems}.
\newblock Winston, 1977.

\bibitem{tokui2015chainer}
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton.
\newblock Chainer: a next-generation open source framework for deep learning.
\newblock In {\em Workshop on machine learning systems (LearningSys) on NIPS},
  2015.

\bibitem{wager2013dropout}
Stefan Wager, Sida Wang, and Percy~S Liang.
\newblock Dropout training as adaptive regularization.
\newblock In {\em NIPS}, 2013.

\bibitem{wahba1990spline}
Grace Wahba.
\newblock Spline models for observational data.
\newblock {\em Siam}, 1990.

\bibitem{watanabe2009algebraic}
Sumio Watanabe.
\newblock Algebraic geometry and statistical learning theory.
\newblock {\em Cambridge University Press}, 2009.

\bibitem{zhao2015stacked}
Junbo Zhao, Michael Mathieu, Ross Goroshin, and Yann Lecun.
\newblock Stacked what-where auto-encoders.
\newblock In {\em Workshop on ICLR}, 2016.

\bibitem{zhu2002learning}
Xiaojin Zhu and Zoubin Ghahramani.
\newblock Learning from labeled and unlabeled data with label propagation.
\newblock Technical report, Citeseer, 2002.

\end{thebibliography}
